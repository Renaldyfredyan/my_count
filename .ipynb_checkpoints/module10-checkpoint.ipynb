{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f89ed9b0-5e85-4c70-a7ef-572a44231ac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: baseline\n",
      "Description: Full model with all components\n",
      "Experiment: baseline, Epoch: 1 Train loss: 28.727 Val loss: 9.905 Train MAE: 51.987 Val MAE: 26.513 Epoch time: 242.700 seconds best\n",
      "Experiment: baseline, Epoch: 2 Train loss: 26.730 Val loss: 9.514 Train MAE: 41.406 Val MAE: 26.849 Epoch time: 246.632 seconds \n",
      "Experiment: baseline, Epoch: 3 Train loss: 26.372 Val loss: 9.047 Train MAE: 35.562 Val MAE: 27.564 Epoch time: 245.849 seconds \n",
      "Experiment: baseline, Epoch: 4 Train loss: 23.489 Val loss: 9.016 Train MAE: 27.119 Val MAE: 31.541 Epoch time: 244.549 seconds \n",
      "Experiment: baseline, Epoch: 5 Train loss: 21.715 Val loss: 8.390 Train MAE: 26.079 Val MAE: 21.324 Epoch time: 246.505 seconds best\n",
      "Experiment: baseline, Epoch: 6 Train loss: 21.263 Val loss: 8.643 Train MAE: 26.358 Val MAE: 21.998 Epoch time: 245.934 seconds \n",
      "Experiment: baseline, Epoch: 7 Train loss: 20.550 Val loss: 8.523 Train MAE: 24.432 Val MAE: 38.546 Epoch time: 246.435 seconds \n",
      "Experiment: baseline, Epoch: 8 Train loss: 21.240 Val loss: 8.221 Train MAE: 23.151 Val MAE: 23.177 Epoch time: 241.750 seconds \n",
      "Experiment: baseline, Epoch: 9 Train loss: 19.699 Val loss: 8.116 Train MAE: 23.741 Val MAE: 22.168 Epoch time: 243.251 seconds \n",
      "Experiment: baseline, Epoch: 10 Train loss: 18.648 Val loss: 7.856 Train MAE: 21.562 Val MAE: 24.394 Epoch time: 243.980 seconds \n",
      "Experiment: baseline, Epoch: 11 Train loss: 18.658 Val loss: 7.750 Train MAE: 20.870 Val MAE: 23.249 Epoch time: 252.248 seconds \n",
      "Experiment: baseline, Epoch: 12 Train loss: 19.612 Val loss: 8.128 Train MAE: 22.959 Val MAE: 33.456 Epoch time: 255.586 seconds \n",
      "Experiment: baseline, Epoch: 13 Train loss: 18.562 Val loss: 7.723 Train MAE: 20.818 Val MAE: 26.926 Epoch time: 253.456 seconds \n",
      "Experiment: baseline, Epoch: 14 Train loss: 18.067 Val loss: 7.536 Train MAE: 20.995 Val MAE: 18.538 Epoch time: 251.098 seconds best\n",
      "Experiment: baseline, Epoch: 15 Train loss: 18.486 Val loss: 7.590 Train MAE: 20.219 Val MAE: 21.353 Epoch time: 251.410 seconds \n",
      "Experiment: baseline, Epoch: 16 Train loss: 17.530 Val loss: 8.047 Train MAE: 20.534 Val MAE: 29.595 Epoch time: 250.895 seconds \n",
      "Experiment: baseline, Epoch: 17 Train loss: 18.962 Val loss: 7.714 Train MAE: 21.398 Val MAE: 23.359 Epoch time: 253.809 seconds \n",
      "Experiment: baseline, Epoch: 18 Train loss: 18.544 Val loss: 7.816 Train MAE: 19.551 Val MAE: 26.351 Epoch time: 252.770 seconds \n",
      "Experiment: baseline, Epoch: 19 Train loss: 17.454 Val loss: 7.679 Train MAE: 18.747 Val MAE: 23.865 Epoch time: 247.371 seconds \n",
      "Experiment: baseline, Epoch: 20 Train loss: 17.686 Val loss: 7.545 Train MAE: 19.299 Val MAE: 25.874 Epoch time: 245.149 seconds \n",
      "Experiment: baseline, Epoch: 21 Train loss: 17.743 Val loss: 7.323 Train MAE: 19.072 Val MAE: 27.665 Epoch time: 242.556 seconds \n",
      "Experiment: baseline, Epoch: 22 Train loss: 16.939 Val loss: 7.235 Train MAE: 18.735 Val MAE: 19.291 Epoch time: 244.836 seconds \n",
      "Experiment: baseline, Epoch: 23 Train loss: 16.704 Val loss: 7.205 Train MAE: 18.212 Val MAE: 27.276 Epoch time: 243.921 seconds \n",
      "Experiment: baseline, Epoch: 24 Train loss: 16.959 Val loss: 7.077 Train MAE: 18.448 Val MAE: 19.969 Epoch time: 242.322 seconds \n",
      "Experiment: baseline, Epoch: 25 Train loss: 17.511 Val loss: 7.310 Train MAE: 17.596 Val MAE: 19.454 Epoch time: 244.064 seconds \n",
      "Experiment: baseline, Epoch: 26 Train loss: 15.875 Val loss: 6.824 Train MAE: 16.769 Val MAE: 12.392 Epoch time: 244.004 seconds best\n",
      "Experiment: baseline, Epoch: 27 Train loss: 15.639 Val loss: 7.239 Train MAE: 18.101 Val MAE: 24.533 Epoch time: 244.509 seconds \n",
      "Experiment: baseline, Epoch: 28 Train loss: 17.144 Val loss: 6.930 Train MAE: 17.879 Val MAE: 16.015 Epoch time: 242.940 seconds \n",
      "Experiment: baseline, Epoch: 29 Train loss: 15.966 Val loss: 7.040 Train MAE: 15.925 Val MAE: 18.330 Epoch time: 241.976 seconds \n",
      "Experiment: baseline, Epoch: 30 Train loss: 16.506 Val loss: 6.834 Train MAE: 16.713 Val MAE: 21.064 Epoch time: 243.649 seconds \n",
      "Experiment: baseline, Epoch: 31 Train loss: 16.054 Val loss: 7.310 Train MAE: 16.461 Val MAE: 23.384 Epoch time: 243.476 seconds \n",
      "Experiment: baseline, Epoch: 32 Train loss: 15.030 Val loss: 6.945 Train MAE: 15.134 Val MAE: 21.435 Epoch time: 244.588 seconds \n",
      "Experiment: baseline, Epoch: 33 Train loss: 16.583 Val loss: 6.970 Train MAE: 16.293 Val MAE: 19.204 Epoch time: 242.720 seconds \n",
      "Experiment: baseline, Epoch: 34 Train loss: 15.817 Val loss: 6.892 Train MAE: 16.212 Val MAE: 16.002 Epoch time: 243.886 seconds \n",
      "Experiment: baseline, Epoch: 35 Train loss: 16.088 Val loss: 7.100 Train MAE: 15.422 Val MAE: 20.787 Epoch time: 243.914 seconds \n",
      "Experiment: baseline, Epoch: 36 Train loss: 15.377 Val loss: 6.940 Train MAE: 15.057 Val MAE: 15.050 Epoch time: 245.210 seconds \n",
      "Experiment: baseline, Epoch: 37 Train loss: 15.706 Val loss: 7.272 Train MAE: 15.216 Val MAE: 19.075 Epoch time: 244.604 seconds \n",
      "Experiment: baseline, Epoch: 38 Train loss: 15.575 Val loss: 7.317 Train MAE: 14.998 Val MAE: 19.324 Epoch time: 243.518 seconds \n",
      "Experiment: baseline, Epoch: 39 Train loss: 14.300 Val loss: 7.052 Train MAE: 14.718 Val MAE: 14.897 Epoch time: 243.589 seconds \n",
      "Experiment: baseline, Epoch: 40 Train loss: 15.087 Val loss: 7.137 Train MAE: 15.012 Val MAE: 22.629 Epoch time: 244.353 seconds \n",
      "Experiment: baseline, Epoch: 41 Train loss: 15.097 Val loss: 6.950 Train MAE: 15.468 Val MAE: 21.295 Epoch time: 242.362 seconds \n",
      "Experiment: baseline, Epoch: 42 Train loss: 16.574 Val loss: 7.042 Train MAE: 15.647 Val MAE: 23.784 Epoch time: 243.528 seconds \n",
      "Experiment: baseline, Epoch: 43 Train loss: 15.556 Val loss: 6.823 Train MAE: 16.291 Val MAE: 20.448 Epoch time: 244.169 seconds \n",
      "Experiment: baseline, Epoch: 44 Train loss: 14.609 Val loss: 6.723 Train MAE: 15.297 Val MAE: 14.985 Epoch time: 242.896 seconds \n",
      "Experiment: baseline, Epoch: 45 Train loss: 14.026 Val loss: 7.131 Train MAE: 13.978 Val MAE: 21.805 Epoch time: 241.608 seconds \n",
      "Experiment: baseline, Epoch: 46 Train loss: 15.316 Val loss: 7.020 Train MAE: 14.730 Val MAE: 23.037 Epoch time: 244.620 seconds \n",
      "Experiment: baseline, Epoch: 47 Train loss: 15.111 Val loss: 6.744 Train MAE: 14.503 Val MAE: 15.089 Epoch time: 241.985 seconds \n",
      "Experiment: baseline, Epoch: 48 Train loss: 14.401 Val loss: 6.655 Train MAE: 14.859 Val MAE: 16.283 Epoch time: 243.613 seconds \n",
      "Experiment: baseline, Epoch: 49 Train loss: 14.652 Val loss: 6.878 Train MAE: 13.668 Val MAE: 21.751 Epoch time: 241.393 seconds \n",
      "Experiment: baseline, Epoch: 50 Train loss: 15.668 Val loss: 6.629 Train MAE: 14.988 Val MAE: 20.262 Epoch time: 242.921 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment baseline completed!\n",
      "Best validation MAE: 12.392\n",
      "Test MAE: 16.015\n",
      "Test RMSE: 85.214\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: backbone_frozen\n",
      "Description: Frozen backbone weights\n",
      "Experiment: backbone_frozen, Epoch: 1 Train loss: 27.508 Val loss: 9.901 Train MAE: 47.780 Val MAE: 37.881 Epoch time: 240.812 seconds best\n",
      "Experiment: backbone_frozen, Epoch: 2 Train loss: 26.691 Val loss: 9.607 Train MAE: 38.989 Val MAE: 32.342 Epoch time: 242.040 seconds best\n",
      "Experiment: backbone_frozen, Epoch: 3 Train loss: 24.131 Val loss: 8.876 Train MAE: 30.202 Val MAE: 21.437 Epoch time: 241.960 seconds best\n",
      "Experiment: backbone_frozen, Epoch: 4 Train loss: 23.445 Val loss: 8.693 Train MAE: 30.934 Val MAE: 22.532 Epoch time: 242.705 seconds \n",
      "Experiment: backbone_frozen, Epoch: 5 Train loss: 21.455 Val loss: 8.703 Train MAE: 28.708 Val MAE: 23.471 Epoch time: 243.291 seconds \n",
      "Experiment: backbone_frozen, Epoch: 6 Train loss: 21.267 Val loss: 8.092 Train MAE: 27.238 Val MAE: 23.981 Epoch time: 243.055 seconds \n",
      "Experiment: backbone_frozen, Epoch: 7 Train loss: 19.693 Val loss: 8.737 Train MAE: 25.717 Val MAE: 34.729 Epoch time: 243.112 seconds \n",
      "Experiment: backbone_frozen, Epoch: 8 Train loss: 19.442 Val loss: 7.884 Train MAE: 23.086 Val MAE: 24.259 Epoch time: 242.153 seconds \n",
      "Experiment: backbone_frozen, Epoch: 9 Train loss: 19.980 Val loss: 8.207 Train MAE: 23.047 Val MAE: 29.777 Epoch time: 243.506 seconds \n",
      "Experiment: backbone_frozen, Epoch: 10 Train loss: 19.153 Val loss: 7.669 Train MAE: 23.679 Val MAE: 24.439 Epoch time: 242.415 seconds \n",
      "Experiment: backbone_frozen, Epoch: 11 Train loss: 17.834 Val loss: 7.697 Train MAE: 22.246 Val MAE: 22.940 Epoch time: 243.880 seconds \n",
      "Experiment: backbone_frozen, Epoch: 12 Train loss: 18.849 Val loss: 7.765 Train MAE: 23.370 Val MAE: 19.621 Epoch time: 245.332 seconds best\n",
      "Experiment: backbone_frozen, Epoch: 13 Train loss: 18.255 Val loss: 8.008 Train MAE: 21.912 Val MAE: 22.849 Epoch time: 243.905 seconds \n",
      "Experiment: backbone_frozen, Epoch: 14 Train loss: 18.079 Val loss: 7.470 Train MAE: 21.242 Val MAE: 21.925 Epoch time: 241.958 seconds \n",
      "Experiment: backbone_frozen, Epoch: 15 Train loss: 17.393 Val loss: 7.852 Train MAE: 19.772 Val MAE: 24.019 Epoch time: 241.739 seconds \n",
      "Experiment: backbone_frozen, Epoch: 16 Train loss: 19.172 Val loss: 8.112 Train MAE: 21.717 Val MAE: 24.950 Epoch time: 245.669 seconds \n",
      "Experiment: backbone_frozen, Epoch: 17 Train loss: 17.550 Val loss: 7.589 Train MAE: 19.302 Val MAE: 29.215 Epoch time: 245.375 seconds \n",
      "Experiment: backbone_frozen, Epoch: 18 Train loss: 16.886 Val loss: 7.641 Train MAE: 20.478 Val MAE: 27.890 Epoch time: 245.485 seconds \n",
      "Experiment: backbone_frozen, Epoch: 19 Train loss: 17.557 Val loss: 7.417 Train MAE: 20.385 Val MAE: 30.328 Epoch time: 245.315 seconds \n",
      "Experiment: backbone_frozen, Epoch: 20 Train loss: 17.604 Val loss: 7.409 Train MAE: 18.630 Val MAE: 26.023 Epoch time: 243.342 seconds \n",
      "Experiment: backbone_frozen, Epoch: 21 Train loss: 17.285 Val loss: 7.280 Train MAE: 18.618 Val MAE: 19.021 Epoch time: 242.247 seconds best\n",
      "Experiment: backbone_frozen, Epoch: 22 Train loss: 17.241 Val loss: 7.394 Train MAE: 18.692 Val MAE: 20.541 Epoch time: 246.237 seconds \n",
      "Experiment: backbone_frozen, Epoch: 23 Train loss: 16.464 Val loss: 7.071 Train MAE: 17.644 Val MAE: 21.123 Epoch time: 246.049 seconds \n",
      "Experiment: backbone_frozen, Epoch: 24 Train loss: 17.584 Val loss: 7.067 Train MAE: 19.118 Val MAE: 26.093 Epoch time: 246.878 seconds \n",
      "Experiment: backbone_frozen, Epoch: 25 Train loss: 16.178 Val loss: 7.504 Train MAE: 18.446 Val MAE: 23.595 Epoch time: 242.785 seconds \n",
      "Experiment: backbone_frozen, Epoch: 26 Train loss: 15.861 Val loss: 7.338 Train MAE: 17.715 Val MAE: 19.427 Epoch time: 245.699 seconds \n",
      "Experiment: backbone_frozen, Epoch: 27 Train loss: 16.300 Val loss: 7.230 Train MAE: 18.918 Val MAE: 27.186 Epoch time: 244.266 seconds \n",
      "Experiment: backbone_frozen, Epoch: 28 Train loss: 15.625 Val loss: 7.183 Train MAE: 16.621 Val MAE: 16.513 Epoch time: 243.615 seconds best\n",
      "Experiment: backbone_frozen, Epoch: 29 Train loss: 16.579 Val loss: 7.191 Train MAE: 17.488 Val MAE: 20.299 Epoch time: 246.215 seconds \n",
      "Experiment: backbone_frozen, Epoch: 30 Train loss: 15.689 Val loss: 7.075 Train MAE: 15.955 Val MAE: 19.528 Epoch time: 246.064 seconds \n",
      "Experiment: backbone_frozen, Epoch: 31 Train loss: 15.893 Val loss: 7.422 Train MAE: 17.395 Val MAE: 20.355 Epoch time: 245.445 seconds \n",
      "Experiment: backbone_frozen, Epoch: 32 Train loss: 16.223 Val loss: 7.366 Train MAE: 17.196 Val MAE: 27.379 Epoch time: 246.597 seconds \n",
      "Experiment: backbone_frozen, Epoch: 33 Train loss: 16.094 Val loss: 7.265 Train MAE: 16.781 Val MAE: 24.617 Epoch time: 245.290 seconds \n",
      "Experiment: backbone_frozen, Epoch: 34 Train loss: 15.915 Val loss: 7.633 Train MAE: 17.324 Val MAE: 21.601 Epoch time: 241.918 seconds \n",
      "Experiment: backbone_frozen, Epoch: 35 Train loss: 15.339 Val loss: 7.442 Train MAE: 16.576 Val MAE: 25.751 Epoch time: 242.530 seconds \n",
      "Experiment: backbone_frozen, Epoch: 36 Train loss: 16.109 Val loss: 7.420 Train MAE: 17.052 Val MAE: 19.315 Epoch time: 243.167 seconds \n",
      "Experiment: backbone_frozen, Epoch: 37 Train loss: 16.126 Val loss: 7.262 Train MAE: 17.092 Val MAE: 17.004 Epoch time: 243.988 seconds \n",
      "Experiment: backbone_frozen, Epoch: 38 Train loss: 14.958 Val loss: 7.494 Train MAE: 15.473 Val MAE: 23.214 Epoch time: 243.134 seconds \n",
      "Experiment: backbone_frozen, Epoch: 39 Train loss: 15.916 Val loss: 7.439 Train MAE: 16.600 Val MAE: 23.484 Epoch time: 243.740 seconds \n",
      "Experiment: backbone_frozen, Epoch: 40 Train loss: 14.320 Val loss: 7.171 Train MAE: 14.670 Val MAE: 20.353 Epoch time: 242.098 seconds \n",
      "Experiment: backbone_frozen, Epoch: 41 Train loss: 14.529 Val loss: 7.765 Train MAE: 16.594 Val MAE: 31.925 Epoch time: 242.822 seconds \n",
      "Experiment: backbone_frozen, Epoch: 42 Train loss: 15.154 Val loss: 7.081 Train MAE: 15.735 Val MAE: 21.922 Epoch time: 239.336 seconds \n",
      "Experiment: backbone_frozen, Epoch: 43 Train loss: 14.530 Val loss: 6.892 Train MAE: 15.575 Val MAE: 17.622 Epoch time: 242.353 seconds \n",
      "Experiment: backbone_frozen, Epoch: 44 Train loss: 14.417 Val loss: 7.007 Train MAE: 15.163 Val MAE: 18.093 Epoch time: 245.127 seconds \n",
      "Experiment: backbone_frozen, Epoch: 45 Train loss: 14.417 Val loss: 7.063 Train MAE: 14.706 Val MAE: 16.548 Epoch time: 245.463 seconds \n",
      "Experiment: backbone_frozen, Epoch: 46 Train loss: 14.539 Val loss: 7.009 Train MAE: 15.538 Val MAE: 18.853 Epoch time: 245.523 seconds \n",
      "Experiment: backbone_frozen, Epoch: 47 Train loss: 14.458 Val loss: 6.952 Train MAE: 14.919 Val MAE: 23.895 Epoch time: 246.106 seconds \n",
      "Experiment: backbone_frozen, Epoch: 48 Train loss: 14.897 Val loss: 7.217 Train MAE: 14.888 Val MAE: 22.194 Epoch time: 243.538 seconds \n",
      "Experiment: backbone_frozen, Epoch: 49 Train loss: 14.752 Val loss: 7.294 Train MAE: 15.277 Val MAE: 22.582 Epoch time: 244.760 seconds \n",
      "Experiment: backbone_frozen, Epoch: 50 Train loss: 15.025 Val loss: 7.059 Train MAE: 14.240 Val MAE: 19.365 Epoch time: 244.072 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment backbone_frozen completed!\n",
      "Best validation MAE: 16.513\n",
      "Test MAE: 15.009\n",
      "Test RMSE: 92.208\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: backbone_trainable\n",
      "Description: Trainable backbone weights\n",
      "Experiment: backbone_trainable, Epoch: 1 Train loss: 28.537 Val loss: 10.003 Train MAE: 48.642 Val MAE: 39.784 Epoch time: 316.805 seconds best\n",
      "Experiment: backbone_trainable, Epoch: 2 Train loss: 25.358 Val loss: 9.536 Train MAE: 35.506 Val MAE: 33.882 Epoch time: 317.626 seconds best\n",
      "Experiment: backbone_trainable, Epoch: 3 Train loss: 23.001 Val loss: 8.856 Train MAE: 30.385 Val MAE: 24.377 Epoch time: 315.081 seconds best\n",
      "Experiment: backbone_trainable, Epoch: 4 Train loss: 22.558 Val loss: 8.335 Train MAE: 29.004 Val MAE: 22.634 Epoch time: 312.156 seconds best\n",
      "Experiment: backbone_trainable, Epoch: 5 Train loss: 20.292 Val loss: 8.060 Train MAE: 27.077 Val MAE: 22.499 Epoch time: 314.390 seconds best\n",
      "Experiment: backbone_trainable, Epoch: 6 Train loss: 20.372 Val loss: 8.005 Train MAE: 23.724 Val MAE: 24.462 Epoch time: 318.355 seconds \n",
      "Experiment: backbone_trainable, Epoch: 7 Train loss: 18.421 Val loss: 8.317 Train MAE: 22.111 Val MAE: 28.372 Epoch time: 316.095 seconds \n",
      "Experiment: backbone_trainable, Epoch: 8 Train loss: 19.472 Val loss: 7.974 Train MAE: 22.464 Val MAE: 22.617 Epoch time: 315.944 seconds \n",
      "Experiment: backbone_trainable, Epoch: 9 Train loss: 18.904 Val loss: 7.928 Train MAE: 21.343 Val MAE: 19.411 Epoch time: 315.283 seconds best\n",
      "Experiment: backbone_trainable, Epoch: 10 Train loss: 17.739 Val loss: 7.993 Train MAE: 20.138 Val MAE: 25.909 Epoch time: 310.840 seconds \n",
      "Experiment: backbone_trainable, Epoch: 11 Train loss: 17.323 Val loss: 8.097 Train MAE: 19.424 Val MAE: 22.393 Epoch time: 311.198 seconds \n",
      "Experiment: backbone_trainable, Epoch: 12 Train loss: 18.406 Val loss: 7.723 Train MAE: 21.038 Val MAE: 19.769 Epoch time: 313.239 seconds \n",
      "Experiment: backbone_trainable, Epoch: 13 Train loss: 16.228 Val loss: 7.727 Train MAE: 17.550 Val MAE: 24.733 Epoch time: 313.484 seconds \n",
      "Experiment: backbone_trainable, Epoch: 14 Train loss: 17.952 Val loss: 7.709 Train MAE: 19.008 Val MAE: 17.215 Epoch time: 314.997 seconds best\n",
      "Experiment: backbone_trainable, Epoch: 15 Train loss: 17.102 Val loss: 7.676 Train MAE: 17.259 Val MAE: 20.662 Epoch time: 317.223 seconds \n",
      "Experiment: backbone_trainable, Epoch: 16 Train loss: 15.918 Val loss: 7.686 Train MAE: 17.203 Val MAE: 21.298 Epoch time: 315.615 seconds \n",
      "Experiment: backbone_trainable, Epoch: 17 Train loss: 16.086 Val loss: 7.873 Train MAE: 16.137 Val MAE: 26.876 Epoch time: 314.077 seconds \n",
      "Experiment: backbone_trainable, Epoch: 18 Train loss: 16.085 Val loss: 7.530 Train MAE: 15.439 Val MAE: 26.229 Epoch time: 314.144 seconds \n",
      "Experiment: backbone_trainable, Epoch: 19 Train loss: 16.360 Val loss: 7.135 Train MAE: 19.204 Val MAE: 23.160 Epoch time: 312.458 seconds \n",
      "Experiment: backbone_trainable, Epoch: 20 Train loss: 15.144 Val loss: 7.569 Train MAE: 14.556 Val MAE: 29.151 Epoch time: 312.923 seconds \n",
      "Experiment: backbone_trainable, Epoch: 21 Train loss: 16.445 Val loss: 7.016 Train MAE: 15.728 Val MAE: 19.461 Epoch time: 317.597 seconds \n",
      "Experiment: backbone_trainable, Epoch: 22 Train loss: 15.027 Val loss: 7.181 Train MAE: 15.150 Val MAE: 22.689 Epoch time: 312.795 seconds \n",
      "Experiment: backbone_trainable, Epoch: 23 Train loss: 15.576 Val loss: 7.004 Train MAE: 15.426 Val MAE: 15.539 Epoch time: 315.795 seconds best\n",
      "Experiment: backbone_trainable, Epoch: 24 Train loss: 15.391 Val loss: 7.090 Train MAE: 14.832 Val MAE: 16.392 Epoch time: 311.877 seconds \n",
      "Experiment: backbone_trainable, Epoch: 25 Train loss: 14.558 Val loss: 7.559 Train MAE: 13.462 Val MAE: 18.342 Epoch time: 317.283 seconds \n",
      "Experiment: backbone_trainable, Epoch: 26 Train loss: 14.619 Val loss: 7.532 Train MAE: 15.273 Val MAE: 16.696 Epoch time: 316.314 seconds \n",
      "Experiment: backbone_trainable, Epoch: 27 Train loss: 15.048 Val loss: 7.226 Train MAE: 14.386 Val MAE: 26.220 Epoch time: 316.567 seconds \n",
      "Experiment: backbone_trainable, Epoch: 28 Train loss: 15.672 Val loss: 7.420 Train MAE: 14.299 Val MAE: 24.997 Epoch time: 316.496 seconds \n",
      "Experiment: backbone_trainable, Epoch: 29 Train loss: 14.914 Val loss: 7.211 Train MAE: 14.082 Val MAE: 23.339 Epoch time: 317.139 seconds \n",
      "Experiment: backbone_trainable, Epoch: 30 Train loss: 15.126 Val loss: 7.223 Train MAE: 14.507 Val MAE: 20.748 Epoch time: 318.370 seconds \n",
      "Experiment: backbone_trainable, Epoch: 31 Train loss: 14.312 Val loss: 7.178 Train MAE: 13.318 Val MAE: 19.194 Epoch time: 316.741 seconds \n",
      "Experiment: backbone_trainable, Epoch: 32 Train loss: 14.742 Val loss: 7.508 Train MAE: 13.376 Val MAE: 19.800 Epoch time: 315.635 seconds \n",
      "Experiment: backbone_trainable, Epoch: 33 Train loss: 15.024 Val loss: 7.365 Train MAE: 13.898 Val MAE: 26.438 Epoch time: 316.450 seconds \n",
      "Experiment: backbone_trainable, Epoch: 34 Train loss: 14.556 Val loss: 7.690 Train MAE: 13.796 Val MAE: 21.985 Epoch time: 313.874 seconds \n",
      "Experiment: backbone_trainable, Epoch: 35 Train loss: 14.005 Val loss: 7.629 Train MAE: 12.982 Val MAE: 19.076 Epoch time: 317.607 seconds \n",
      "Experiment: backbone_trainable, Epoch: 36 Train loss: 14.478 Val loss: 7.153 Train MAE: 14.449 Val MAE: 16.691 Epoch time: 316.618 seconds \n",
      "Experiment: backbone_trainable, Epoch: 37 Train loss: 14.419 Val loss: 7.085 Train MAE: 12.152 Val MAE: 20.966 Epoch time: 316.552 seconds \n",
      "Experiment: backbone_trainable, Epoch: 38 Train loss: 13.791 Val loss: 7.053 Train MAE: 12.421 Val MAE: 16.897 Epoch time: 315.362 seconds \n",
      "Experiment: backbone_trainable, Epoch: 39 Train loss: 14.677 Val loss: 7.381 Train MAE: 13.592 Val MAE: 22.678 Epoch time: 316.480 seconds \n",
      "Experiment: backbone_trainable, Epoch: 40 Train loss: 13.456 Val loss: 7.130 Train MAE: 12.381 Val MAE: 16.224 Epoch time: 311.164 seconds \n",
      "Experiment: backbone_trainable, Epoch: 41 Train loss: 13.824 Val loss: 6.759 Train MAE: 13.339 Val MAE: 15.838 Epoch time: 315.546 seconds \n",
      "Experiment: backbone_trainable, Epoch: 42 Train loss: 13.661 Val loss: 7.294 Train MAE: 13.074 Val MAE: 21.380 Epoch time: 309.784 seconds \n",
      "Experiment: backbone_trainable, Epoch: 43 Train loss: 13.236 Val loss: 7.043 Train MAE: 12.522 Val MAE: 16.064 Epoch time: 312.829 seconds \n",
      "Experiment: backbone_trainable, Epoch: 44 Train loss: 13.460 Val loss: 7.006 Train MAE: 12.005 Val MAE: 21.431 Epoch time: 314.718 seconds \n",
      "Experiment: backbone_trainable, Epoch: 45 Train loss: 13.141 Val loss: 6.921 Train MAE: 12.400 Val MAE: 16.395 Epoch time: 314.713 seconds \n",
      "Experiment: backbone_trainable, Epoch: 46 Train loss: 13.609 Val loss: 7.149 Train MAE: 12.149 Val MAE: 19.607 Epoch time: 314.709 seconds \n",
      "Experiment: backbone_trainable, Epoch: 47 Train loss: 13.569 Val loss: 7.029 Train MAE: 12.070 Val MAE: 17.656 Epoch time: 318.681 seconds \n",
      "Experiment: backbone_trainable, Epoch: 48 Train loss: 13.982 Val loss: 7.207 Train MAE: 12.174 Val MAE: 18.302 Epoch time: 317.482 seconds \n",
      "Experiment: backbone_trainable, Epoch: 49 Train loss: 13.780 Val loss: 8.108 Train MAE: 11.898 Val MAE: 27.570 Epoch time: 317.169 seconds \n",
      "Experiment: backbone_trainable, Epoch: 50 Train loss: 13.483 Val loss: 6.698 Train MAE: 13.009 Val MAE: 19.085 Epoch time: 319.453 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment backbone_trainable completed!\n",
      "Best validation MAE: 15.539\n",
      "Test MAE: 13.994\n",
      "Test RMSE: 87.049\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: no_encoder\n",
      "Description: No hybrid encoder layers\n",
      "/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [256, 1344, 1, 1], strides() = [1344, 1, 1344, 1344]\n",
      "bucket_view.sizes() = [256, 1344, 1, 1], strides() = [1344, 1, 1, 1] (Triggered internally at /opt/conda/conda-bld/pytorch_1729647352509/work/torch/csrc/distributed/c10d/reducer.cpp:327.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Experiment: no_encoder, Epoch: 1 Train loss: 28.400 Val loss: 10.053 Train MAE: 49.957 Val MAE: 24.235 Epoch time: 252.017 seconds best\n",
      "Experiment: no_encoder, Epoch: 2 Train loss: 26.937 Val loss: 9.843 Train MAE: 37.766 Val MAE: 28.575 Epoch time: 251.823 seconds \n",
      "Experiment: no_encoder, Epoch: 3 Train loss: 26.067 Val loss: 9.739 Train MAE: 30.092 Val MAE: 26.423 Epoch time: 247.972 seconds \n",
      "Experiment: no_encoder, Epoch: 4 Train loss: 24.834 Val loss: 9.407 Train MAE: 27.755 Val MAE: 21.200 Epoch time: 249.200 seconds best\n",
      "Experiment: no_encoder, Epoch: 5 Train loss: 23.726 Val loss: 9.434 Train MAE: 27.194 Val MAE: 25.182 Epoch time: 249.975 seconds \n",
      "Experiment: no_encoder, Epoch: 6 Train loss: 24.078 Val loss: 8.963 Train MAE: 25.366 Val MAE: 23.503 Epoch time: 249.889 seconds \n",
      "Experiment: no_encoder, Epoch: 7 Train loss: 22.938 Val loss: 9.105 Train MAE: 23.000 Val MAE: 36.617 Epoch time: 250.579 seconds \n",
      "Experiment: no_encoder, Epoch: 8 Train loss: 22.763 Val loss: 9.057 Train MAE: 24.718 Val MAE: 31.344 Epoch time: 249.450 seconds \n",
      "Experiment: no_encoder, Epoch: 9 Train loss: 22.862 Val loss: 8.904 Train MAE: 22.968 Val MAE: 21.963 Epoch time: 248.136 seconds \n",
      "Experiment: no_encoder, Epoch: 10 Train loss: 23.106 Val loss: 8.630 Train MAE: 23.689 Val MAE: 20.602 Epoch time: 247.436 seconds best\n",
      "Experiment: no_encoder, Epoch: 11 Train loss: 20.237 Val loss: 8.798 Train MAE: 23.285 Val MAE: 22.648 Epoch time: 250.672 seconds \n",
      "Experiment: no_encoder, Epoch: 12 Train loss: 22.002 Val loss: 8.781 Train MAE: 23.738 Val MAE: 41.493 Epoch time: 249.799 seconds \n",
      "Experiment: no_encoder, Epoch: 13 Train loss: 21.800 Val loss: 8.402 Train MAE: 22.987 Val MAE: 26.054 Epoch time: 249.196 seconds \n",
      "Experiment: no_encoder, Epoch: 14 Train loss: 20.608 Val loss: 8.499 Train MAE: 20.393 Val MAE: 19.351 Epoch time: 248.064 seconds best\n",
      "Experiment: no_encoder, Epoch: 15 Train loss: 19.990 Val loss: 8.346 Train MAE: 19.713 Val MAE: 19.299 Epoch time: 250.250 seconds best\n",
      "Experiment: no_encoder, Epoch: 16 Train loss: 19.979 Val loss: 8.291 Train MAE: 22.191 Val MAE: 16.082 Epoch time: 248.971 seconds best\n",
      "Experiment: no_encoder, Epoch: 17 Train loss: 21.645 Val loss: 8.531 Train MAE: 19.466 Val MAE: 35.336 Epoch time: 249.735 seconds \n",
      "Experiment: no_encoder, Epoch: 18 Train loss: 20.562 Val loss: 8.182 Train MAE: 20.358 Val MAE: 21.880 Epoch time: 249.182 seconds \n",
      "Experiment: no_encoder, Epoch: 19 Train loss: 20.155 Val loss: 8.168 Train MAE: 20.692 Val MAE: 18.586 Epoch time: 249.301 seconds \n",
      "Experiment: no_encoder, Epoch: 20 Train loss: 19.267 Val loss: 8.089 Train MAE: 19.453 Val MAE: 19.543 Epoch time: 248.541 seconds \n",
      "Experiment: no_encoder, Epoch: 21 Train loss: 19.010 Val loss: 8.124 Train MAE: 19.137 Val MAE: 21.128 Epoch time: 247.487 seconds \n",
      "Experiment: no_encoder, Epoch: 22 Train loss: 18.783 Val loss: 7.983 Train MAE: 19.580 Val MAE: 17.842 Epoch time: 247.114 seconds \n",
      "Experiment: no_encoder, Epoch: 23 Train loss: 20.211 Val loss: 7.860 Train MAE: 19.823 Val MAE: 17.067 Epoch time: 248.448 seconds \n",
      "Experiment: no_encoder, Epoch: 24 Train loss: 18.391 Val loss: 7.896 Train MAE: 18.666 Val MAE: 22.484 Epoch time: 248.862 seconds \n",
      "Experiment: no_encoder, Epoch: 25 Train loss: 18.574 Val loss: 7.669 Train MAE: 19.093 Val MAE: 19.630 Epoch time: 250.526 seconds \n",
      "Experiment: no_encoder, Epoch: 26 Train loss: 18.567 Val loss: 7.957 Train MAE: 18.950 Val MAE: 17.232 Epoch time: 249.685 seconds \n",
      "Experiment: no_encoder, Epoch: 27 Train loss: 17.977 Val loss: 7.931 Train MAE: 18.816 Val MAE: 21.221 Epoch time: 252.709 seconds \n",
      "Experiment: no_encoder, Epoch: 28 Train loss: 19.013 Val loss: 7.895 Train MAE: 19.117 Val MAE: 20.871 Epoch time: 252.837 seconds \n",
      "Experiment: no_encoder, Epoch: 29 Train loss: 17.435 Val loss: 7.924 Train MAE: 18.881 Val MAE: 23.260 Epoch time: 250.929 seconds \n",
      "Experiment: no_encoder, Epoch: 30 Train loss: 18.514 Val loss: 7.921 Train MAE: 18.061 Val MAE: 27.469 Epoch time: 249.759 seconds \n",
      "Experiment: no_encoder, Epoch: 31 Train loss: 17.856 Val loss: 7.786 Train MAE: 17.663 Val MAE: 21.408 Epoch time: 250.666 seconds \n",
      "Experiment: no_encoder, Epoch: 32 Train loss: 18.095 Val loss: 7.692 Train MAE: 17.052 Val MAE: 27.302 Epoch time: 250.443 seconds \n",
      "Experiment: no_encoder, Epoch: 33 Train loss: 18.655 Val loss: 7.360 Train MAE: 18.210 Val MAE: 20.828 Epoch time: 249.718 seconds \n",
      "Experiment: no_encoder, Epoch: 34 Train loss: 17.516 Val loss: 7.499 Train MAE: 17.489 Val MAE: 23.181 Epoch time: 248.190 seconds \n",
      "Experiment: no_encoder, Epoch: 35 Train loss: 17.391 Val loss: 7.662 Train MAE: 18.049 Val MAE: 20.444 Epoch time: 249.150 seconds \n",
      "Experiment: no_encoder, Epoch: 36 Train loss: 16.685 Val loss: 7.312 Train MAE: 16.446 Val MAE: 15.600 Epoch time: 248.967 seconds best\n",
      "Experiment: no_encoder, Epoch: 37 Train loss: 17.837 Val loss: 7.522 Train MAE: 17.855 Val MAE: 20.830 Epoch time: 247.454 seconds \n",
      "Experiment: no_encoder, Epoch: 38 Train loss: 18.168 Val loss: 7.668 Train MAE: 17.246 Val MAE: 20.331 Epoch time: 252.463 seconds \n",
      "Experiment: no_encoder, Epoch: 39 Train loss: 16.681 Val loss: 7.330 Train MAE: 16.156 Val MAE: 18.164 Epoch time: 253.434 seconds \n",
      "Experiment: no_encoder, Epoch: 40 Train loss: 16.435 Val loss: 7.271 Train MAE: 17.549 Val MAE: 20.916 Epoch time: 249.122 seconds \n",
      "Experiment: no_encoder, Epoch: 41 Train loss: 16.782 Val loss: 7.203 Train MAE: 16.066 Val MAE: 15.230 Epoch time: 252.712 seconds best\n",
      "Experiment: no_encoder, Epoch: 42 Train loss: 16.378 Val loss: 7.283 Train MAE: 15.644 Val MAE: 23.239 Epoch time: 252.012 seconds \n",
      "Experiment: no_encoder, Epoch: 43 Train loss: 17.486 Val loss: 7.521 Train MAE: 16.836 Val MAE: 23.318 Epoch time: 252.026 seconds \n",
      "Experiment: no_encoder, Epoch: 44 Train loss: 16.672 Val loss: 6.983 Train MAE: 16.161 Val MAE: 19.011 Epoch time: 251.493 seconds \n",
      "Experiment: no_encoder, Epoch: 45 Train loss: 16.349 Val loss: 7.116 Train MAE: 15.798 Val MAE: 16.061 Epoch time: 250.851 seconds \n",
      "Experiment: no_encoder, Epoch: 46 Train loss: 15.742 Val loss: 7.113 Train MAE: 15.151 Val MAE: 20.083 Epoch time: 251.429 seconds \n",
      "Experiment: no_encoder, Epoch: 47 Train loss: 15.855 Val loss: 7.134 Train MAE: 16.029 Val MAE: 19.914 Epoch time: 253.388 seconds \n",
      "Experiment: no_encoder, Epoch: 48 Train loss: 16.645 Val loss: 6.994 Train MAE: 18.525 Val MAE: 18.061 Epoch time: 250.556 seconds \n",
      "Experiment: no_encoder, Epoch: 49 Train loss: 16.084 Val loss: 7.097 Train MAE: 16.105 Val MAE: 21.766 Epoch time: 249.059 seconds \n",
      "Experiment: no_encoder, Epoch: 50 Train loss: 16.365 Val loss: 6.907 Train MAE: 15.909 Val MAE: 23.026 Epoch time: 252.040 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment no_encoder completed!\n",
      "Best validation MAE: 15.230\n",
      "Test MAE: 17.736\n",
      "Test RMSE: 84.756\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: one_encoder_layer\n",
      "Description: One hybrid encoder layer\n",
      "Experiment: one_encoder_layer, Epoch: 1 Train loss: 27.941 Val loss: 10.188 Train MAE: 49.471 Val MAE: 40.528 Epoch time: 241.773 seconds best\n",
      "Experiment: one_encoder_layer, Epoch: 2 Train loss: 25.608 Val loss: 9.209 Train MAE: 36.718 Val MAE: 23.130 Epoch time: 243.421 seconds best\n",
      "Experiment: one_encoder_layer, Epoch: 3 Train loss: 24.419 Val loss: 9.001 Train MAE: 35.052 Val MAE: 28.249 Epoch time: 246.181 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 4 Train loss: 23.150 Val loss: 8.675 Train MAE: 29.786 Val MAE: 21.419 Epoch time: 246.441 seconds best\n",
      "Experiment: one_encoder_layer, Epoch: 5 Train loss: 20.971 Val loss: 8.326 Train MAE: 27.584 Val MAE: 28.508 Epoch time: 245.844 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 6 Train loss: 20.315 Val loss: 8.534 Train MAE: 26.211 Val MAE: 28.473 Epoch time: 244.808 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 7 Train loss: 20.322 Val loss: 8.474 Train MAE: 24.814 Val MAE: 36.708 Epoch time: 244.605 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 8 Train loss: 20.627 Val loss: 8.120 Train MAE: 24.980 Val MAE: 26.771 Epoch time: 244.774 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 9 Train loss: 19.027 Val loss: 8.322 Train MAE: 23.220 Val MAE: 27.889 Epoch time: 244.463 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 10 Train loss: 19.501 Val loss: 8.138 Train MAE: 22.267 Val MAE: 25.212 Epoch time: 244.070 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 11 Train loss: 19.010 Val loss: 8.092 Train MAE: 22.138 Val MAE: 33.777 Epoch time: 243.165 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 12 Train loss: 18.424 Val loss: 7.902 Train MAE: 22.340 Val MAE: 19.579 Epoch time: 244.719 seconds best\n",
      "Experiment: one_encoder_layer, Epoch: 13 Train loss: 17.894 Val loss: 8.476 Train MAE: 20.257 Val MAE: 35.191 Epoch time: 243.686 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 14 Train loss: 18.107 Val loss: 7.792 Train MAE: 20.355 Val MAE: 22.306 Epoch time: 245.853 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 15 Train loss: 18.228 Val loss: 7.727 Train MAE: 20.612 Val MAE: 33.926 Epoch time: 243.754 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 16 Train loss: 19.831 Val loss: 7.520 Train MAE: 21.635 Val MAE: 18.336 Epoch time: 244.859 seconds best\n",
      "Experiment: one_encoder_layer, Epoch: 17 Train loss: 18.254 Val loss: 7.572 Train MAE: 21.254 Val MAE: 25.143 Epoch time: 246.256 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 18 Train loss: 17.802 Val loss: 7.494 Train MAE: 18.923 Val MAE: 16.238 Epoch time: 244.000 seconds best\n",
      "Experiment: one_encoder_layer, Epoch: 19 Train loss: 17.198 Val loss: 7.290 Train MAE: 21.207 Val MAE: 17.515 Epoch time: 245.692 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 20 Train loss: 17.546 Val loss: 7.412 Train MAE: 19.263 Val MAE: 22.631 Epoch time: 247.000 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 21 Train loss: 16.217 Val loss: 7.741 Train MAE: 18.193 Val MAE: 36.618 Epoch time: 245.580 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 22 Train loss: 16.109 Val loss: 7.466 Train MAE: 18.361 Val MAE: 19.258 Epoch time: 246.542 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 23 Train loss: 16.972 Val loss: 7.546 Train MAE: 18.485 Val MAE: 20.699 Epoch time: 245.697 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 24 Train loss: 16.986 Val loss: 7.209 Train MAE: 18.841 Val MAE: 26.942 Epoch time: 245.888 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 25 Train loss: 15.672 Val loss: 7.526 Train MAE: 17.749 Val MAE: 20.752 Epoch time: 245.301 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 26 Train loss: 15.577 Val loss: 7.223 Train MAE: 16.638 Val MAE: 17.912 Epoch time: 246.497 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 27 Train loss: 16.132 Val loss: 7.317 Train MAE: 17.990 Val MAE: 19.441 Epoch time: 245.571 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 28 Train loss: 16.809 Val loss: 7.149 Train MAE: 16.930 Val MAE: 18.225 Epoch time: 244.024 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 29 Train loss: 15.626 Val loss: 7.184 Train MAE: 16.627 Val MAE: 22.980 Epoch time: 244.143 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 30 Train loss: 15.646 Val loss: 6.998 Train MAE: 16.831 Val MAE: 16.859 Epoch time: 243.704 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 31 Train loss: 15.969 Val loss: 7.263 Train MAE: 17.607 Val MAE: 20.126 Epoch time: 246.592 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 32 Train loss: 16.199 Val loss: 7.100 Train MAE: 17.135 Val MAE: 22.732 Epoch time: 243.706 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 33 Train loss: 16.023 Val loss: 7.205 Train MAE: 17.509 Val MAE: 22.073 Epoch time: 241.812 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 34 Train loss: 15.879 Val loss: 7.113 Train MAE: 16.030 Val MAE: 23.498 Epoch time: 241.762 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 35 Train loss: 15.313 Val loss: 7.267 Train MAE: 15.618 Val MAE: 19.037 Epoch time: 244.403 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 36 Train loss: 15.226 Val loss: 6.986 Train MAE: 15.446 Val MAE: 19.334 Epoch time: 242.582 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 37 Train loss: 15.838 Val loss: 6.838 Train MAE: 16.533 Val MAE: 15.704 Epoch time: 242.096 seconds best\n",
      "Experiment: one_encoder_layer, Epoch: 38 Train loss: 15.968 Val loss: 7.133 Train MAE: 16.140 Val MAE: 31.364 Epoch time: 243.169 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 39 Train loss: 16.344 Val loss: 7.221 Train MAE: 16.390 Val MAE: 24.277 Epoch time: 242.427 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 40 Train loss: 15.856 Val loss: 7.174 Train MAE: 15.568 Val MAE: 24.278 Epoch time: 242.268 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 41 Train loss: 13.862 Val loss: 7.057 Train MAE: 14.432 Val MAE: 21.627 Epoch time: 242.989 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 42 Train loss: 15.488 Val loss: 7.218 Train MAE: 13.860 Val MAE: 25.035 Epoch time: 242.416 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 43 Train loss: 15.069 Val loss: 6.842 Train MAE: 15.489 Val MAE: 16.212 Epoch time: 243.084 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 44 Train loss: 14.925 Val loss: 7.012 Train MAE: 14.190 Val MAE: 19.712 Epoch time: 243.448 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 45 Train loss: 15.235 Val loss: 6.739 Train MAE: 15.217 Val MAE: 16.811 Epoch time: 246.186 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 46 Train loss: 15.034 Val loss: 6.982 Train MAE: 14.744 Val MAE: 19.212 Epoch time: 245.882 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 47 Train loss: 15.191 Val loss: 6.810 Train MAE: 14.453 Val MAE: 24.714 Epoch time: 243.661 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 48 Train loss: 14.450 Val loss: 6.885 Train MAE: 14.200 Val MAE: 24.000 Epoch time: 243.359 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 49 Train loss: 14.859 Val loss: 7.133 Train MAE: 14.747 Val MAE: 20.799 Epoch time: 244.401 seconds \n",
      "Experiment: one_encoder_layer, Epoch: 50 Train loss: 14.762 Val loss: 6.922 Train MAE: 14.224 Val MAE: 20.498 Epoch time: 242.650 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment one_encoder_layer completed!\n",
      "Best validation MAE: 15.704\n",
      "Test MAE: 12.804\n",
      "Test RMSE: 72.909\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: five_encoder_layers\n",
      "Description: Five hybrid encoder layers\n",
      "Experiment: five_encoder_layers, Epoch: 1 Train loss: 29.265 Val loss: 9.804 Train MAE: 50.224 Val MAE: 28.094 Epoch time: 241.397 seconds best\n",
      "Experiment: five_encoder_layers, Epoch: 2 Train loss: 26.436 Val loss: 9.513 Train MAE: 36.404 Val MAE: 33.046 Epoch time: 241.794 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 3 Train loss: 24.942 Val loss: 9.077 Train MAE: 32.519 Val MAE: 24.838 Epoch time: 242.589 seconds best\n",
      "Experiment: five_encoder_layers, Epoch: 4 Train loss: 22.668 Val loss: 8.941 Train MAE: 27.833 Val MAE: 22.709 Epoch time: 242.368 seconds best\n",
      "Experiment: five_encoder_layers, Epoch: 5 Train loss: 20.651 Val loss: 8.432 Train MAE: 25.739 Val MAE: 23.639 Epoch time: 241.182 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 6 Train loss: 22.111 Val loss: 8.516 Train MAE: 26.378 Val MAE: 23.629 Epoch time: 243.918 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 7 Train loss: 20.374 Val loss: 8.478 Train MAE: 26.810 Val MAE: 32.587 Epoch time: 244.217 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 8 Train loss: 21.081 Val loss: 8.380 Train MAE: 23.229 Val MAE: 30.706 Epoch time: 242.578 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 9 Train loss: 20.113 Val loss: 7.855 Train MAE: 23.511 Val MAE: 22.781 Epoch time: 241.719 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 10 Train loss: 20.626 Val loss: 7.800 Train MAE: 23.720 Val MAE: 26.444 Epoch time: 241.921 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 11 Train loss: 19.007 Val loss: 7.782 Train MAE: 23.171 Val MAE: 23.299 Epoch time: 241.904 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 12 Train loss: 19.427 Val loss: 8.114 Train MAE: 23.864 Val MAE: 36.311 Epoch time: 241.442 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 13 Train loss: 18.230 Val loss: 8.430 Train MAE: 21.911 Val MAE: 38.214 Epoch time: 242.904 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 14 Train loss: 17.442 Val loss: 7.342 Train MAE: 20.501 Val MAE: 19.848 Epoch time: 241.914 seconds best\n",
      "Experiment: five_encoder_layers, Epoch: 15 Train loss: 17.581 Val loss: 7.757 Train MAE: 21.410 Val MAE: 32.653 Epoch time: 243.339 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 16 Train loss: 19.095 Val loss: 8.076 Train MAE: 21.831 Val MAE: 22.355 Epoch time: 242.792 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 17 Train loss: 18.981 Val loss: 7.654 Train MAE: 21.320 Val MAE: 28.999 Epoch time: 243.312 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 18 Train loss: 17.394 Val loss: 7.717 Train MAE: 19.353 Val MAE: 34.389 Epoch time: 241.589 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 19 Train loss: 17.253 Val loss: 7.580 Train MAE: 17.350 Val MAE: 22.119 Epoch time: 242.388 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 20 Train loss: 16.866 Val loss: 7.673 Train MAE: 18.031 Val MAE: 23.147 Epoch time: 242.959 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 21 Train loss: 18.110 Val loss: 7.258 Train MAE: 19.010 Val MAE: 22.222 Epoch time: 244.759 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 22 Train loss: 17.167 Val loss: 7.449 Train MAE: 19.806 Val MAE: 18.666 Epoch time: 243.921 seconds best\n",
      "Experiment: five_encoder_layers, Epoch: 23 Train loss: 16.475 Val loss: 7.471 Train MAE: 18.801 Val MAE: 32.028 Epoch time: 242.117 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 24 Train loss: 16.299 Val loss: 7.810 Train MAE: 18.326 Val MAE: 21.580 Epoch time: 242.929 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 25 Train loss: 16.239 Val loss: 7.512 Train MAE: 17.460 Val MAE: 20.185 Epoch time: 241.794 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 26 Train loss: 15.606 Val loss: 7.183 Train MAE: 17.613 Val MAE: 20.928 Epoch time: 242.266 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 27 Train loss: 17.259 Val loss: 7.433 Train MAE: 17.566 Val MAE: 25.687 Epoch time: 241.213 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 28 Train loss: 15.833 Val loss: 7.097 Train MAE: 17.539 Val MAE: 18.553 Epoch time: 245.119 seconds best\n",
      "Experiment: five_encoder_layers, Epoch: 29 Train loss: 15.423 Val loss: 7.503 Train MAE: 17.025 Val MAE: 21.059 Epoch time: 243.143 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 30 Train loss: 15.995 Val loss: 7.162 Train MAE: 17.165 Val MAE: 17.875 Epoch time: 244.796 seconds best\n",
      "Experiment: five_encoder_layers, Epoch: 31 Train loss: 15.843 Val loss: 7.231 Train MAE: 16.350 Val MAE: 27.664 Epoch time: 246.338 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 32 Train loss: 15.800 Val loss: 7.296 Train MAE: 16.339 Val MAE: 24.377 Epoch time: 244.281 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 33 Train loss: 16.219 Val loss: 7.418 Train MAE: 16.623 Val MAE: 27.796 Epoch time: 241.305 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 34 Train loss: 14.966 Val loss: 7.508 Train MAE: 16.635 Val MAE: 19.295 Epoch time: 240.429 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 35 Train loss: 14.857 Val loss: 7.457 Train MAE: 14.851 Val MAE: 20.986 Epoch time: 242.644 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 36 Train loss: 15.796 Val loss: 7.212 Train MAE: 16.688 Val MAE: 23.241 Epoch time: 240.228 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 37 Train loss: 16.498 Val loss: 6.791 Train MAE: 16.105 Val MAE: 14.832 Epoch time: 242.926 seconds best\n",
      "Experiment: five_encoder_layers, Epoch: 38 Train loss: 15.625 Val loss: 6.981 Train MAE: 15.640 Val MAE: 17.823 Epoch time: 240.100 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 39 Train loss: 15.938 Val loss: 7.030 Train MAE: 15.907 Val MAE: 17.984 Epoch time: 242.365 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 40 Train loss: 15.860 Val loss: 6.991 Train MAE: 16.724 Val MAE: 19.501 Epoch time: 242.714 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 41 Train loss: 14.973 Val loss: 7.575 Train MAE: 16.497 Val MAE: 23.618 Epoch time: 242.191 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 42 Train loss: 16.655 Val loss: 7.133 Train MAE: 17.416 Val MAE: 25.603 Epoch time: 241.505 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 43 Train loss: 15.149 Val loss: 7.072 Train MAE: 15.300 Val MAE: 18.151 Epoch time: 241.483 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 44 Train loss: 14.743 Val loss: 7.284 Train MAE: 14.618 Val MAE: 29.798 Epoch time: 240.935 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 45 Train loss: 14.234 Val loss: 6.906 Train MAE: 14.990 Val MAE: 15.741 Epoch time: 241.833 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 46 Train loss: 14.337 Val loss: 6.849 Train MAE: 15.359 Val MAE: 25.594 Epoch time: 245.997 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 47 Train loss: 15.557 Val loss: 7.061 Train MAE: 14.949 Val MAE: 25.506 Epoch time: 245.876 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 48 Train loss: 14.426 Val loss: 7.294 Train MAE: 15.294 Val MAE: 22.133 Epoch time: 244.613 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 49 Train loss: 15.845 Val loss: 7.036 Train MAE: 14.692 Val MAE: 21.223 Epoch time: 245.594 seconds \n",
      "Experiment: five_encoder_layers, Epoch: 50 Train loss: 15.722 Val loss: 6.854 Train MAE: 14.893 Val MAE: 15.299 Epoch time: 241.824 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment five_encoder_layers completed!\n",
      "Best validation MAE: 14.832\n",
      "Test MAE: 12.990\n",
      "Test RMSE: 71.134\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: one_iefl_step\n",
      "Description: One iEFL iterative step\n",
      "Experiment: one_iefl_step, Epoch: 1 Train loss: 28.643 Val loss: 9.971 Train MAE: 52.313 Val MAE: 27.961 Epoch time: 157.856 seconds best\n",
      "Experiment: one_iefl_step, Epoch: 2 Train loss: 26.005 Val loss: 9.529 Train MAE: 36.769 Val MAE: 27.632 Epoch time: 160.163 seconds best\n",
      "Experiment: one_iefl_step, Epoch: 3 Train loss: 24.540 Val loss: 9.359 Train MAE: 31.158 Val MAE: 24.796 Epoch time: 160.161 seconds best\n",
      "Experiment: one_iefl_step, Epoch: 4 Train loss: 21.977 Val loss: 8.990 Train MAE: 28.125 Val MAE: 30.437 Epoch time: 160.535 seconds \n",
      "Experiment: one_iefl_step, Epoch: 5 Train loss: 21.439 Val loss: 8.765 Train MAE: 26.610 Val MAE: 31.956 Epoch time: 163.295 seconds \n",
      "Experiment: one_iefl_step, Epoch: 6 Train loss: 21.575 Val loss: 8.350 Train MAE: 26.759 Val MAE: 28.451 Epoch time: 160.539 seconds \n",
      "Experiment: one_iefl_step, Epoch: 7 Train loss: 20.351 Val loss: 8.727 Train MAE: 25.481 Val MAE: 30.699 Epoch time: 160.107 seconds \n",
      "Experiment: one_iefl_step, Epoch: 8 Train loss: 21.134 Val loss: 8.244 Train MAE: 24.424 Val MAE: 32.591 Epoch time: 161.049 seconds \n",
      "Experiment: one_iefl_step, Epoch: 9 Train loss: 21.229 Val loss: 8.142 Train MAE: 23.260 Val MAE: 23.870 Epoch time: 161.333 seconds best\n",
      "Experiment: one_iefl_step, Epoch: 10 Train loss: 19.492 Val loss: 7.909 Train MAE: 22.793 Val MAE: 25.346 Epoch time: 160.552 seconds \n",
      "Experiment: one_iefl_step, Epoch: 11 Train loss: 17.873 Val loss: 7.598 Train MAE: 22.854 Val MAE: 19.399 Epoch time: 161.151 seconds best\n",
      "Experiment: one_iefl_step, Epoch: 12 Train loss: 19.218 Val loss: 7.688 Train MAE: 21.608 Val MAE: 18.958 Epoch time: 160.424 seconds best\n",
      "Experiment: one_iefl_step, Epoch: 13 Train loss: 19.415 Val loss: 7.938 Train MAE: 22.589 Val MAE: 36.825 Epoch time: 160.334 seconds \n",
      "Experiment: one_iefl_step, Epoch: 14 Train loss: 18.274 Val loss: 7.741 Train MAE: 22.725 Val MAE: 27.890 Epoch time: 163.900 seconds \n",
      "Experiment: one_iefl_step, Epoch: 15 Train loss: 19.738 Val loss: 7.898 Train MAE: 20.389 Val MAE: 33.327 Epoch time: 163.444 seconds \n",
      "Experiment: one_iefl_step, Epoch: 16 Train loss: 18.173 Val loss: 7.557 Train MAE: 21.273 Val MAE: 25.799 Epoch time: 164.236 seconds \n",
      "Experiment: one_iefl_step, Epoch: 17 Train loss: 17.597 Val loss: 7.595 Train MAE: 20.293 Val MAE: 18.154 Epoch time: 159.967 seconds best\n",
      "Experiment: one_iefl_step, Epoch: 18 Train loss: 17.904 Val loss: 7.482 Train MAE: 19.383 Val MAE: 24.763 Epoch time: 160.391 seconds \n",
      "Experiment: one_iefl_step, Epoch: 19 Train loss: 17.902 Val loss: 7.477 Train MAE: 20.075 Val MAE: 23.637 Epoch time: 159.550 seconds \n",
      "Experiment: one_iefl_step, Epoch: 20 Train loss: 18.383 Val loss: 7.535 Train MAE: 18.829 Val MAE: 28.907 Epoch time: 162.848 seconds \n",
      "Experiment: one_iefl_step, Epoch: 21 Train loss: 17.608 Val loss: 7.443 Train MAE: 19.179 Val MAE: 34.503 Epoch time: 160.730 seconds \n",
      "Experiment: one_iefl_step, Epoch: 22 Train loss: 17.217 Val loss: 7.260 Train MAE: 19.051 Val MAE: 16.627 Epoch time: 159.256 seconds best\n",
      "Experiment: one_iefl_step, Epoch: 23 Train loss: 17.655 Val loss: 7.224 Train MAE: 17.821 Val MAE: 21.897 Epoch time: 160.873 seconds \n",
      "Experiment: one_iefl_step, Epoch: 24 Train loss: 16.982 Val loss: 7.310 Train MAE: 18.029 Val MAE: 30.613 Epoch time: 161.125 seconds \n",
      "Experiment: one_iefl_step, Epoch: 25 Train loss: 16.439 Val loss: 7.118 Train MAE: 19.359 Val MAE: 17.928 Epoch time: 161.245 seconds \n",
      "Experiment: one_iefl_step, Epoch: 26 Train loss: 16.553 Val loss: 6.943 Train MAE: 16.742 Val MAE: 16.111 Epoch time: 163.116 seconds best\n",
      "Experiment: one_iefl_step, Epoch: 27 Train loss: 15.760 Val loss: 7.224 Train MAE: 16.707 Val MAE: 25.705 Epoch time: 159.015 seconds \n",
      "Experiment: one_iefl_step, Epoch: 28 Train loss: 16.917 Val loss: 7.033 Train MAE: 17.743 Val MAE: 18.123 Epoch time: 159.646 seconds \n",
      "Experiment: one_iefl_step, Epoch: 29 Train loss: 15.470 Val loss: 6.845 Train MAE: 16.937 Val MAE: 19.128 Epoch time: 163.310 seconds \n",
      "Experiment: one_iefl_step, Epoch: 30 Train loss: 16.726 Val loss: 6.875 Train MAE: 16.958 Val MAE: 18.973 Epoch time: 160.405 seconds \n",
      "Experiment: one_iefl_step, Epoch: 31 Train loss: 16.605 Val loss: 6.991 Train MAE: 16.755 Val MAE: 26.201 Epoch time: 160.149 seconds \n",
      "Experiment: one_iefl_step, Epoch: 32 Train loss: 16.391 Val loss: 7.042 Train MAE: 18.371 Val MAE: 27.973 Epoch time: 160.665 seconds \n",
      "Experiment: one_iefl_step, Epoch: 33 Train loss: 16.631 Val loss: 7.033 Train MAE: 17.188 Val MAE: 22.943 Epoch time: 162.658 seconds \n",
      "Experiment: one_iefl_step, Epoch: 34 Train loss: 15.778 Val loss: 6.863 Train MAE: 16.696 Val MAE: 14.090 Epoch time: 162.686 seconds best\n",
      "Experiment: one_iefl_step, Epoch: 35 Train loss: 16.750 Val loss: 6.877 Train MAE: 15.680 Val MAE: 23.222 Epoch time: 159.214 seconds \n",
      "Experiment: one_iefl_step, Epoch: 36 Train loss: 16.152 Val loss: 6.973 Train MAE: 16.503 Val MAE: 18.189 Epoch time: 160.841 seconds \n",
      "Experiment: one_iefl_step, Epoch: 37 Train loss: 16.485 Val loss: 6.899 Train MAE: 16.372 Val MAE: 22.091 Epoch time: 160.570 seconds \n",
      "Experiment: one_iefl_step, Epoch: 38 Train loss: 15.436 Val loss: 6.660 Train MAE: 16.242 Val MAE: 16.049 Epoch time: 161.348 seconds \n",
      "Experiment: one_iefl_step, Epoch: 39 Train loss: 16.080 Val loss: 6.838 Train MAE: 16.179 Val MAE: 15.937 Epoch time: 160.211 seconds \n",
      "Experiment: one_iefl_step, Epoch: 40 Train loss: 15.072 Val loss: 6.865 Train MAE: 14.806 Val MAE: 20.593 Epoch time: 159.635 seconds \n",
      "Experiment: one_iefl_step, Epoch: 41 Train loss: 14.964 Val loss: 7.021 Train MAE: 15.764 Val MAE: 21.020 Epoch time: 160.531 seconds \n",
      "Experiment: one_iefl_step, Epoch: 42 Train loss: 16.235 Val loss: 6.807 Train MAE: 16.629 Val MAE: 24.694 Epoch time: 160.493 seconds \n",
      "Experiment: one_iefl_step, Epoch: 43 Train loss: 15.985 Val loss: 6.899 Train MAE: 15.595 Val MAE: 29.974 Epoch time: 159.428 seconds \n",
      "Experiment: one_iefl_step, Epoch: 44 Train loss: 15.341 Val loss: 7.115 Train MAE: 14.931 Val MAE: 27.195 Epoch time: 161.802 seconds \n",
      "Experiment: one_iefl_step, Epoch: 45 Train loss: 15.251 Val loss: 6.643 Train MAE: 15.998 Val MAE: 14.176 Epoch time: 160.303 seconds \n",
      "Experiment: one_iefl_step, Epoch: 46 Train loss: 15.496 Val loss: 6.965 Train MAE: 15.816 Val MAE: 18.213 Epoch time: 160.556 seconds \n",
      "Experiment: one_iefl_step, Epoch: 47 Train loss: 15.298 Val loss: 6.901 Train MAE: 14.800 Val MAE: 22.580 Epoch time: 160.700 seconds \n",
      "Experiment: one_iefl_step, Epoch: 48 Train loss: 15.549 Val loss: 6.975 Train MAE: 15.990 Val MAE: 23.036 Epoch time: 160.352 seconds \n",
      "Experiment: one_iefl_step, Epoch: 49 Train loss: 15.005 Val loss: 7.039 Train MAE: 15.165 Val MAE: 16.977 Epoch time: 159.530 seconds \n",
      "Experiment: one_iefl_step, Epoch: 50 Train loss: 15.911 Val loss: 6.607 Train MAE: 15.262 Val MAE: 15.863 Epoch time: 163.271 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment one_iefl_step completed!\n",
      "Best validation MAE: 14.090\n",
      "Test MAE: 15.202\n",
      "Test RMSE: 89.121\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: five_iefl_steps\n",
      "Description: Five iEFL iterative steps\n",
      "Experiment: five_iefl_steps, Epoch: 1 Train loss: 28.579 Val loss: 10.142 Train MAE: 52.611 Val MAE: 44.838 Epoch time: 329.250 seconds best\n",
      "Experiment: five_iefl_steps, Epoch: 2 Train loss: 26.921 Val loss: 9.383 Train MAE: 37.720 Val MAE: 24.527 Epoch time: 327.064 seconds best\n",
      "Experiment: five_iefl_steps, Epoch: 3 Train loss: 24.110 Val loss: 8.867 Train MAE: 32.779 Val MAE: 20.501 Epoch time: 326.771 seconds best\n",
      "Experiment: five_iefl_steps, Epoch: 4 Train loss: 23.310 Val loss: 8.923 Train MAE: 30.046 Val MAE: 27.147 Epoch time: 324.633 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 5 Train loss: 21.354 Val loss: 8.530 Train MAE: 27.479 Val MAE: 25.949 Epoch time: 324.104 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 6 Train loss: 21.552 Val loss: 8.035 Train MAE: 24.617 Val MAE: 24.053 Epoch time: 324.440 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 7 Train loss: 19.734 Val loss: 8.149 Train MAE: 24.065 Val MAE: 20.374 Epoch time: 326.560 seconds best\n",
      "Experiment: five_iefl_steps, Epoch: 8 Train loss: 21.368 Val loss: 7.827 Train MAE: 26.613 Val MAE: 26.421 Epoch time: 322.261 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 9 Train loss: 21.889 Val loss: 8.110 Train MAE: 25.017 Val MAE: 25.680 Epoch time: 322.570 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 10 Train loss: 20.285 Val loss: 7.722 Train MAE: 23.715 Val MAE: 23.092 Epoch time: 325.089 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 11 Train loss: 18.967 Val loss: 7.646 Train MAE: 22.001 Val MAE: 25.606 Epoch time: 323.637 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 12 Train loss: 17.786 Val loss: 7.781 Train MAE: 20.625 Val MAE: 16.781 Epoch time: 323.929 seconds best\n",
      "Experiment: five_iefl_steps, Epoch: 13 Train loss: 18.539 Val loss: 7.567 Train MAE: 22.341 Val MAE: 21.646 Epoch time: 325.108 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 14 Train loss: 18.536 Val loss: 7.721 Train MAE: 21.523 Val MAE: 19.435 Epoch time: 322.965 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 15 Train loss: 17.554 Val loss: 7.860 Train MAE: 21.910 Val MAE: 28.979 Epoch time: 320.593 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 16 Train loss: 18.355 Val loss: 7.581 Train MAE: 22.614 Val MAE: 17.687 Epoch time: 322.747 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 17 Train loss: 17.188 Val loss: 7.627 Train MAE: 19.179 Val MAE: 34.495 Epoch time: 323.829 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 18 Train loss: 18.366 Val loss: 7.697 Train MAE: 21.789 Val MAE: 27.053 Epoch time: 324.571 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 19 Train loss: 15.918 Val loss: 7.359 Train MAE: 18.597 Val MAE: 22.035 Epoch time: 324.664 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 20 Train loss: 17.510 Val loss: 7.812 Train MAE: 20.180 Val MAE: 27.954 Epoch time: 320.166 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 21 Train loss: 17.673 Val loss: 7.569 Train MAE: 18.863 Val MAE: 34.766 Epoch time: 324.529 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 22 Train loss: 17.410 Val loss: 7.214 Train MAE: 19.836 Val MAE: 18.017 Epoch time: 326.206 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 23 Train loss: 16.381 Val loss: 7.278 Train MAE: 18.269 Val MAE: 24.523 Epoch time: 324.634 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 24 Train loss: 16.398 Val loss: 7.034 Train MAE: 18.208 Val MAE: 21.939 Epoch time: 318.928 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 25 Train loss: 15.963 Val loss: 7.310 Train MAE: 18.222 Val MAE: 19.327 Epoch time: 323.152 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 26 Train loss: 16.322 Val loss: 7.146 Train MAE: 17.795 Val MAE: 15.013 Epoch time: 324.248 seconds best\n",
      "Experiment: five_iefl_steps, Epoch: 27 Train loss: 15.545 Val loss: 7.158 Train MAE: 17.338 Val MAE: 25.123 Epoch time: 323.667 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 28 Train loss: 15.403 Val loss: 7.106 Train MAE: 16.880 Val MAE: 20.548 Epoch time: 324.161 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 29 Train loss: 15.954 Val loss: 7.042 Train MAE: 17.451 Val MAE: 17.214 Epoch time: 325.832 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 30 Train loss: 15.940 Val loss: 6.977 Train MAE: 18.117 Val MAE: 17.539 Epoch time: 318.254 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 31 Train loss: 15.708 Val loss: 7.305 Train MAE: 17.490 Val MAE: 18.442 Epoch time: 323.827 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 32 Train loss: 16.185 Val loss: 7.026 Train MAE: 16.891 Val MAE: 17.069 Epoch time: 323.334 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 33 Train loss: 16.089 Val loss: 7.187 Train MAE: 16.910 Val MAE: 19.602 Epoch time: 325.065 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 34 Train loss: 15.985 Val loss: 6.986 Train MAE: 16.430 Val MAE: 22.126 Epoch time: 322.427 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 35 Train loss: 16.096 Val loss: 6.947 Train MAE: 17.302 Val MAE: 14.860 Epoch time: 322.961 seconds best\n",
      "Experiment: five_iefl_steps, Epoch: 36 Train loss: 15.353 Val loss: 6.814 Train MAE: 15.854 Val MAE: 16.632 Epoch time: 327.289 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 37 Train loss: 16.359 Val loss: 6.627 Train MAE: 17.647 Val MAE: 18.518 Epoch time: 325.251 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 38 Train loss: 15.563 Val loss: 6.925 Train MAE: 16.182 Val MAE: 17.935 Epoch time: 324.773 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 39 Train loss: 16.450 Val loss: 6.899 Train MAE: 15.843 Val MAE: 16.066 Epoch time: 325.055 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 40 Train loss: 14.933 Val loss: 7.066 Train MAE: 16.591 Val MAE: 18.978 Epoch time: 322.774 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 41 Train loss: 14.928 Val loss: 6.889 Train MAE: 15.703 Val MAE: 18.051 Epoch time: 324.049 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 42 Train loss: 15.356 Val loss: 6.637 Train MAE: 15.418 Val MAE: 18.717 Epoch time: 323.041 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 43 Train loss: 15.356 Val loss: 6.694 Train MAE: 16.947 Val MAE: 19.297 Epoch time: 322.677 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 44 Train loss: 14.637 Val loss: 6.811 Train MAE: 14.677 Val MAE: 25.149 Epoch time: 322.477 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 45 Train loss: 14.208 Val loss: 7.065 Train MAE: 14.454 Val MAE: 18.342 Epoch time: 325.590 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 46 Train loss: 15.059 Val loss: 6.920 Train MAE: 15.154 Val MAE: 15.255 Epoch time: 321.479 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 47 Train loss: 14.623 Val loss: 6.721 Train MAE: 14.915 Val MAE: 18.746 Epoch time: 323.794 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 48 Train loss: 15.396 Val loss: 6.971 Train MAE: 17.921 Val MAE: 24.555 Epoch time: 327.019 seconds \n",
      "Experiment: five_iefl_steps, Epoch: 49 Train loss: 15.012 Val loss: 6.557 Train MAE: 15.447 Val MAE: 14.841 Epoch time: 322.484 seconds best\n",
      "Experiment: five_iefl_steps, Epoch: 50 Train loss: 15.189 Val loss: 6.694 Train MAE: 15.013 Val MAE: 21.095 Epoch time: 326.163 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment five_iefl_steps completed!\n",
      "Best validation MAE: 14.841\n",
      "Test MAE: 13.294\n",
      "Test RMSE: 71.547\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: small_emb_dim\n",
      "Description: Smaller embedding dimension (128)\n",
      "Experiment: small_emb_dim, Epoch: 1 Train loss: 27.003 Val loss: 10.124 Train MAE: 45.846 Val MAE: 44.719 Epoch time: 215.729 seconds best\n",
      "Experiment: small_emb_dim, Epoch: 2 Train loss: 27.270 Val loss: 9.459 Train MAE: 35.989 Val MAE: 32.906 Epoch time: 218.537 seconds best\n",
      "Experiment: small_emb_dim, Epoch: 3 Train loss: 24.256 Val loss: 9.000 Train MAE: 29.917 Val MAE: 23.492 Epoch time: 214.046 seconds best\n",
      "Experiment: small_emb_dim, Epoch: 4 Train loss: 23.286 Val loss: 8.730 Train MAE: 29.070 Val MAE: 23.965 Epoch time: 215.093 seconds \n",
      "Experiment: small_emb_dim, Epoch: 5 Train loss: 21.228 Val loss: 8.754 Train MAE: 27.027 Val MAE: 34.516 Epoch time: 214.768 seconds \n",
      "Experiment: small_emb_dim, Epoch: 6 Train loss: 21.595 Val loss: 8.221 Train MAE: 25.073 Val MAE: 26.574 Epoch time: 216.714 seconds \n",
      "Experiment: small_emb_dim, Epoch: 7 Train loss: 21.192 Val loss: 8.626 Train MAE: 23.915 Val MAE: 37.808 Epoch time: 216.519 seconds \n",
      "Experiment: small_emb_dim, Epoch: 8 Train loss: 20.624 Val loss: 8.164 Train MAE: 22.888 Val MAE: 20.391 Epoch time: 214.915 seconds best\n",
      "Experiment: small_emb_dim, Epoch: 9 Train loss: 20.074 Val loss: 8.027 Train MAE: 23.099 Val MAE: 21.238 Epoch time: 215.183 seconds \n",
      "Experiment: small_emb_dim, Epoch: 10 Train loss: 19.562 Val loss: 8.018 Train MAE: 20.946 Val MAE: 31.293 Epoch time: 215.059 seconds \n",
      "Experiment: small_emb_dim, Epoch: 11 Train loss: 19.923 Val loss: 7.987 Train MAE: 23.171 Val MAE: 20.080 Epoch time: 217.233 seconds best\n",
      "Experiment: small_emb_dim, Epoch: 12 Train loss: 18.791 Val loss: 7.819 Train MAE: 21.878 Val MAE: 18.084 Epoch time: 215.349 seconds best\n",
      "Experiment: small_emb_dim, Epoch: 13 Train loss: 18.098 Val loss: 7.854 Train MAE: 20.516 Val MAE: 24.053 Epoch time: 216.187 seconds \n",
      "Experiment: small_emb_dim, Epoch: 14 Train loss: 18.634 Val loss: 7.863 Train MAE: 20.546 Val MAE: 28.786 Epoch time: 215.537 seconds \n",
      "Experiment: small_emb_dim, Epoch: 15 Train loss: 19.401 Val loss: 7.814 Train MAE: 21.094 Val MAE: 28.884 Epoch time: 215.449 seconds \n",
      "Experiment: small_emb_dim, Epoch: 16 Train loss: 18.694 Val loss: 7.916 Train MAE: 22.393 Val MAE: 30.703 Epoch time: 219.913 seconds \n",
      "Experiment: small_emb_dim, Epoch: 17 Train loss: 18.712 Val loss: 7.705 Train MAE: 20.018 Val MAE: 27.259 Epoch time: 220.318 seconds \n",
      "Experiment: small_emb_dim, Epoch: 18 Train loss: 17.226 Val loss: 7.535 Train MAE: 19.400 Val MAE: 22.176 Epoch time: 216.753 seconds \n",
      "Experiment: small_emb_dim, Epoch: 19 Train loss: 18.271 Val loss: 7.534 Train MAE: 19.268 Val MAE: 24.752 Epoch time: 215.656 seconds \n",
      "Experiment: small_emb_dim, Epoch: 20 Train loss: 17.791 Val loss: 8.329 Train MAE: 18.985 Val MAE: 33.400 Epoch time: 216.686 seconds \n",
      "Experiment: small_emb_dim, Epoch: 21 Train loss: 18.696 Val loss: 7.447 Train MAE: 19.802 Val MAE: 23.259 Epoch time: 215.616 seconds \n",
      "Experiment: small_emb_dim, Epoch: 22 Train loss: 17.351 Val loss: 7.348 Train MAE: 20.147 Val MAE: 18.432 Epoch time: 215.850 seconds \n",
      "Experiment: small_emb_dim, Epoch: 23 Train loss: 17.591 Val loss: 7.379 Train MAE: 18.602 Val MAE: 17.286 Epoch time: 216.689 seconds best\n",
      "Experiment: small_emb_dim, Epoch: 24 Train loss: 16.981 Val loss: 7.311 Train MAE: 17.455 Val MAE: 22.728 Epoch time: 217.602 seconds \n",
      "Experiment: small_emb_dim, Epoch: 25 Train loss: 17.569 Val loss: 7.703 Train MAE: 18.267 Val MAE: 24.758 Epoch time: 213.650 seconds \n",
      "Experiment: small_emb_dim, Epoch: 26 Train loss: 16.623 Val loss: 7.349 Train MAE: 17.830 Val MAE: 21.095 Epoch time: 215.732 seconds \n",
      "Experiment: small_emb_dim, Epoch: 27 Train loss: 17.063 Val loss: 7.403 Train MAE: 18.695 Val MAE: 27.418 Epoch time: 219.293 seconds \n",
      "Experiment: small_emb_dim, Epoch: 28 Train loss: 17.021 Val loss: 7.445 Train MAE: 18.378 Val MAE: 21.685 Epoch time: 221.035 seconds \n",
      "Experiment: small_emb_dim, Epoch: 29 Train loss: 16.115 Val loss: 7.452 Train MAE: 17.034 Val MAE: 31.235 Epoch time: 220.128 seconds \n",
      "Experiment: small_emb_dim, Epoch: 30 Train loss: 17.719 Val loss: 7.247 Train MAE: 18.218 Val MAE: 26.278 Epoch time: 219.442 seconds \n",
      "Experiment: small_emb_dim, Epoch: 31 Train loss: 15.674 Val loss: 7.309 Train MAE: 16.707 Val MAE: 24.707 Epoch time: 219.915 seconds \n",
      "Experiment: small_emb_dim, Epoch: 32 Train loss: 16.523 Val loss: 7.233 Train MAE: 16.290 Val MAE: 23.173 Epoch time: 218.109 seconds \n",
      "Experiment: small_emb_dim, Epoch: 33 Train loss: 17.398 Val loss: 6.887 Train MAE: 16.541 Val MAE: 16.670 Epoch time: 219.677 seconds best\n",
      "Experiment: small_emb_dim, Epoch: 34 Train loss: 16.032 Val loss: 7.459 Train MAE: 16.523 Val MAE: 30.515 Epoch time: 219.775 seconds \n",
      "Experiment: small_emb_dim, Epoch: 35 Train loss: 16.073 Val loss: 7.187 Train MAE: 17.473 Val MAE: 24.264 Epoch time: 218.900 seconds \n",
      "Experiment: small_emb_dim, Epoch: 36 Train loss: 15.094 Val loss: 7.200 Train MAE: 15.727 Val MAE: 17.251 Epoch time: 220.921 seconds \n",
      "Experiment: small_emb_dim, Epoch: 37 Train loss: 16.358 Val loss: 7.352 Train MAE: 17.569 Val MAE: 18.445 Epoch time: 218.859 seconds \n",
      "Experiment: small_emb_dim, Epoch: 38 Train loss: 15.062 Val loss: 7.486 Train MAE: 16.273 Val MAE: 21.497 Epoch time: 218.874 seconds \n",
      "Experiment: small_emb_dim, Epoch: 39 Train loss: 16.184 Val loss: 7.404 Train MAE: 15.711 Val MAE: 20.527 Epoch time: 217.318 seconds \n",
      "Experiment: small_emb_dim, Epoch: 40 Train loss: 14.709 Val loss: 7.107 Train MAE: 15.832 Val MAE: 17.611 Epoch time: 215.189 seconds \n",
      "Experiment: small_emb_dim, Epoch: 41 Train loss: 14.490 Val loss: 6.968 Train MAE: 15.150 Val MAE: 19.917 Epoch time: 218.094 seconds \n",
      "Experiment: small_emb_dim, Epoch: 42 Train loss: 15.810 Val loss: 7.188 Train MAE: 15.339 Val MAE: 25.630 Epoch time: 219.042 seconds \n",
      "Experiment: small_emb_dim, Epoch: 43 Train loss: 15.997 Val loss: 7.565 Train MAE: 15.830 Val MAE: 27.713 Epoch time: 214.608 seconds \n",
      "Experiment: small_emb_dim, Epoch: 44 Train loss: 15.042 Val loss: 7.177 Train MAE: 15.214 Val MAE: 20.890 Epoch time: 215.862 seconds \n",
      "Experiment: small_emb_dim, Epoch: 45 Train loss: 15.260 Val loss: 7.139 Train MAE: 14.542 Val MAE: 22.938 Epoch time: 213.380 seconds \n",
      "Experiment: small_emb_dim, Epoch: 46 Train loss: 15.085 Val loss: 7.037 Train MAE: 14.301 Val MAE: 19.108 Epoch time: 216.281 seconds \n",
      "Experiment: small_emb_dim, Epoch: 47 Train loss: 15.673 Val loss: 7.098 Train MAE: 14.732 Val MAE: 26.670 Epoch time: 213.666 seconds \n",
      "Experiment: small_emb_dim, Epoch: 48 Train loss: 15.895 Val loss: 7.315 Train MAE: 14.805 Val MAE: 24.141 Epoch time: 214.596 seconds \n",
      "Experiment: small_emb_dim, Epoch: 49 Train loss: 14.630 Val loss: 7.032 Train MAE: 15.179 Val MAE: 18.802 Epoch time: 216.130 seconds \n",
      "Experiment: small_emb_dim, Epoch: 50 Train loss: 15.933 Val loss: 7.002 Train MAE: 14.746 Val MAE: 18.300 Epoch time: 214.174 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment small_emb_dim completed!\n",
      "Best validation MAE: 16.670\n",
      "Test MAE: 16.485\n",
      "Test RMSE: 87.521\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: large_emb_dim\n",
      "Description: Larger embedding dimension (512)\n",
      "Experiment: large_emb_dim, Epoch: 1 Train loss: 29.853 Val loss: 10.161 Train MAE: 54.983 Val MAE: 33.859 Epoch time: 310.807 seconds best\n",
      "Experiment: large_emb_dim, Epoch: 2 Train loss: 26.755 Val loss: 9.560 Train MAE: 41.893 Val MAE: 25.945 Epoch time: 315.428 seconds best\n",
      "Experiment: large_emb_dim, Epoch: 3 Train loss: 25.302 Val loss: 9.815 Train MAE: 35.113 Val MAE: 47.737 Epoch time: 313.750 seconds \n",
      "Experiment: large_emb_dim, Epoch: 4 Train loss: 23.930 Val loss: 9.094 Train MAE: 30.202 Val MAE: 31.213 Epoch time: 313.154 seconds \n",
      "Experiment: large_emb_dim, Epoch: 5 Train loss: 22.241 Val loss: 8.586 Train MAE: 28.991 Val MAE: 20.565 Epoch time: 312.196 seconds best\n",
      "Experiment: large_emb_dim, Epoch: 6 Train loss: 21.882 Val loss: 9.712 Train MAE: 27.132 Val MAE: 28.572 Epoch time: 311.455 seconds \n",
      "Experiment: large_emb_dim, Epoch: 7 Train loss: 22.996 Val loss: 8.475 Train MAE: 27.048 Val MAE: 26.623 Epoch time: 310.329 seconds \n",
      "Experiment: large_emb_dim, Epoch: 8 Train loss: 21.028 Val loss: 8.422 Train MAE: 24.928 Val MAE: 18.404 Epoch time: 311.824 seconds best\n",
      "Experiment: large_emb_dim, Epoch: 9 Train loss: 21.569 Val loss: 8.151 Train MAE: 25.411 Val MAE: 22.353 Epoch time: 309.373 seconds \n",
      "Experiment: large_emb_dim, Epoch: 10 Train loss: 20.031 Val loss: 8.045 Train MAE: 23.751 Val MAE: 30.182 Epoch time: 310.767 seconds \n",
      "Experiment: large_emb_dim, Epoch: 11 Train loss: 18.880 Val loss: 7.962 Train MAE: 21.808 Val MAE: 22.974 Epoch time: 313.196 seconds \n",
      "Experiment: large_emb_dim, Epoch: 12 Train loss: 18.089 Val loss: 7.862 Train MAE: 23.128 Val MAE: 18.476 Epoch time: 312.915 seconds \n",
      "Experiment: large_emb_dim, Epoch: 13 Train loss: 18.711 Val loss: 7.931 Train MAE: 22.203 Val MAE: 26.303 Epoch time: 313.802 seconds \n",
      "Experiment: large_emb_dim, Epoch: 14 Train loss: 18.536 Val loss: 7.636 Train MAE: 21.267 Val MAE: 23.714 Epoch time: 311.406 seconds \n",
      "Experiment: large_emb_dim, Epoch: 15 Train loss: 19.031 Val loss: 7.837 Train MAE: 22.578 Val MAE: 32.667 Epoch time: 308.360 seconds \n",
      "Experiment: large_emb_dim, Epoch: 16 Train loss: 18.771 Val loss: 7.612 Train MAE: 20.108 Val MAE: 15.907 Epoch time: 312.740 seconds best\n",
      "Experiment: large_emb_dim, Epoch: 17 Train loss: 18.673 Val loss: 7.599 Train MAE: 20.884 Val MAE: 16.371 Epoch time: 312.200 seconds \n",
      "Experiment: large_emb_dim, Epoch: 18 Train loss: 17.634 Val loss: 7.613 Train MAE: 21.428 Val MAE: 16.192 Epoch time: 314.206 seconds \n",
      "Experiment: large_emb_dim, Epoch: 19 Train loss: 17.737 Val loss: 7.744 Train MAE: 19.988 Val MAE: 22.250 Epoch time: 312.290 seconds \n",
      "Experiment: large_emb_dim, Epoch: 20 Train loss: 17.039 Val loss: 7.750 Train MAE: 19.497 Val MAE: 37.356 Epoch time: 311.200 seconds \n",
      "Experiment: large_emb_dim, Epoch: 21 Train loss: 18.095 Val loss: 7.189 Train MAE: 19.779 Val MAE: 18.916 Epoch time: 309.753 seconds \n",
      "Experiment: large_emb_dim, Epoch: 22 Train loss: 17.111 Val loss: 7.382 Train MAE: 18.526 Val MAE: 20.300 Epoch time: 311.934 seconds \n",
      "Experiment: large_emb_dim, Epoch: 23 Train loss: 17.557 Val loss: 7.155 Train MAE: 18.764 Val MAE: 21.554 Epoch time: 313.485 seconds \n",
      "Experiment: large_emb_dim, Epoch: 24 Train loss: 16.657 Val loss: 7.190 Train MAE: 20.436 Val MAE: 26.754 Epoch time: 314.999 seconds \n",
      "Experiment: large_emb_dim, Epoch: 25 Train loss: 16.641 Val loss: 7.323 Train MAE: 18.756 Val MAE: 17.181 Epoch time: 312.741 seconds \n",
      "Experiment: large_emb_dim, Epoch: 26 Train loss: 16.249 Val loss: 7.275 Train MAE: 18.336 Val MAE: 21.103 Epoch time: 311.452 seconds \n",
      "Experiment: large_emb_dim, Epoch: 27 Train loss: 16.477 Val loss: 6.889 Train MAE: 18.064 Val MAE: 18.672 Epoch time: 313.514 seconds \n",
      "Experiment: large_emb_dim, Epoch: 28 Train loss: 15.892 Val loss: 7.472 Train MAE: 17.152 Val MAE: 19.728 Epoch time: 315.987 seconds \n",
      "Experiment: large_emb_dim, Epoch: 29 Train loss: 15.646 Val loss: 6.894 Train MAE: 17.650 Val MAE: 22.800 Epoch time: 319.071 seconds \n",
      "Experiment: large_emb_dim, Epoch: 30 Train loss: 16.327 Val loss: 7.338 Train MAE: 18.132 Val MAE: 18.604 Epoch time: 316.688 seconds \n",
      "Experiment: large_emb_dim, Epoch: 31 Train loss: 16.134 Val loss: 7.025 Train MAE: 16.775 Val MAE: 18.843 Epoch time: 314.346 seconds \n",
      "Experiment: large_emb_dim, Epoch: 32 Train loss: 16.242 Val loss: 7.246 Train MAE: 17.471 Val MAE: 15.749 Epoch time: 313.262 seconds best\n",
      "Experiment: large_emb_dim, Epoch: 33 Train loss: 15.878 Val loss: 6.907 Train MAE: 16.712 Val MAE: 17.245 Epoch time: 317.187 seconds \n",
      "Experiment: large_emb_dim, Epoch: 34 Train loss: 16.471 Val loss: 7.405 Train MAE: 17.783 Val MAE: 25.333 Epoch time: 318.366 seconds \n",
      "Experiment: large_emb_dim, Epoch: 35 Train loss: 16.116 Val loss: 7.114 Train MAE: 16.518 Val MAE: 17.708 Epoch time: 316.805 seconds \n",
      "Experiment: large_emb_dim, Epoch: 36 Train loss: 15.858 Val loss: 7.008 Train MAE: 16.744 Val MAE: 14.901 Epoch time: 317.277 seconds best\n",
      "Experiment: large_emb_dim, Epoch: 37 Train loss: 16.430 Val loss: 7.061 Train MAE: 15.905 Val MAE: 15.452 Epoch time: 315.794 seconds \n",
      "Experiment: large_emb_dim, Epoch: 38 Train loss: 16.189 Val loss: 6.773 Train MAE: 17.010 Val MAE: 14.995 Epoch time: 317.586 seconds \n",
      "Experiment: large_emb_dim, Epoch: 39 Train loss: 16.003 Val loss: 6.829 Train MAE: 16.029 Val MAE: 19.486 Epoch time: 321.712 seconds \n",
      "Experiment: large_emb_dim, Epoch: 40 Train loss: 15.526 Val loss: 7.165 Train MAE: 16.332 Val MAE: 23.349 Epoch time: 322.831 seconds \n",
      "Experiment: large_emb_dim, Epoch: 41 Train loss: 15.301 Val loss: 7.375 Train MAE: 15.955 Val MAE: 18.690 Epoch time: 322.952 seconds \n",
      "Experiment: large_emb_dim, Epoch: 42 Train loss: 15.604 Val loss: 7.142 Train MAE: 16.144 Val MAE: 20.485 Epoch time: 320.802 seconds \n",
      "Experiment: large_emb_dim, Epoch: 43 Train loss: 14.799 Val loss: 6.939 Train MAE: 16.146 Val MAE: 22.247 Epoch time: 318.110 seconds \n",
      "Experiment: large_emb_dim, Epoch: 44 Train loss: 14.794 Val loss: 6.779 Train MAE: 15.321 Val MAE: 20.716 Epoch time: 315.468 seconds \n",
      "Experiment: large_emb_dim, Epoch: 45 Train loss: 14.471 Val loss: 6.793 Train MAE: 15.874 Val MAE: 17.333 Epoch time: 317.740 seconds \n",
      "Experiment: large_emb_dim, Epoch: 46 Train loss: 15.517 Val loss: 6.737 Train MAE: 15.786 Val MAE: 20.190 Epoch time: 317.136 seconds \n",
      "Experiment: large_emb_dim, Epoch: 47 Train loss: 15.150 Val loss: 6.790 Train MAE: 15.371 Val MAE: 20.270 Epoch time: 317.673 seconds \n",
      "Experiment: large_emb_dim, Epoch: 48 Train loss: 15.000 Val loss: 7.312 Train MAE: 14.603 Val MAE: 19.340 Epoch time: 319.168 seconds \n",
      "Experiment: large_emb_dim, Epoch: 49 Train loss: 15.179 Val loss: 7.081 Train MAE: 15.183 Val MAE: 20.504 Epoch time: 316.770 seconds \n",
      "Experiment: large_emb_dim, Epoch: 50 Train loss: 14.776 Val loss: 6.751 Train MAE: 15.532 Val MAE: 21.599 Epoch time: 318.617 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment large_emb_dim completed!\n",
      "Best validation MAE: 14.901\n",
      "Test MAE: 14.057\n",
      "Test RMSE: 85.206\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: few_heads\n",
      "Description: Fewer attention heads (4)\n",
      "Experiment: few_heads, Epoch: 1 Train loss: 28.949 Val loss: 9.700 Train MAE: 47.884 Val MAE: 29.278 Epoch time: 248.372 seconds best\n",
      "Experiment: few_heads, Epoch: 2 Train loss: 25.883 Val loss: 9.335 Train MAE: 37.793 Val MAE: 31.251 Epoch time: 250.396 seconds \n",
      "Experiment: few_heads, Epoch: 3 Train loss: 24.374 Val loss: 8.992 Train MAE: 33.133 Val MAE: 26.704 Epoch time: 252.263 seconds best\n",
      "Experiment: few_heads, Epoch: 4 Train loss: 22.092 Val loss: 8.804 Train MAE: 29.857 Val MAE: 22.365 Epoch time: 251.393 seconds best\n",
      "Experiment: few_heads, Epoch: 5 Train loss: 20.534 Val loss: 8.425 Train MAE: 26.631 Val MAE: 25.546 Epoch time: 249.346 seconds \n",
      "Experiment: few_heads, Epoch: 6 Train loss: 20.900 Val loss: 8.350 Train MAE: 24.975 Val MAE: 31.332 Epoch time: 247.685 seconds \n",
      "Experiment: few_heads, Epoch: 7 Train loss: 19.658 Val loss: 8.661 Train MAE: 26.167 Val MAE: 30.099 Epoch time: 248.742 seconds \n",
      "Experiment: few_heads, Epoch: 8 Train loss: 21.273 Val loss: 8.382 Train MAE: 25.052 Val MAE: 24.453 Epoch time: 250.571 seconds \n",
      "Experiment: few_heads, Epoch: 9 Train loss: 19.112 Val loss: 8.103 Train MAE: 21.689 Val MAE: 27.868 Epoch time: 250.060 seconds \n",
      "Experiment: few_heads, Epoch: 10 Train loss: 18.802 Val loss: 8.186 Train MAE: 22.647 Val MAE: 23.203 Epoch time: 249.277 seconds \n",
      "Experiment: few_heads, Epoch: 11 Train loss: 18.576 Val loss: 7.871 Train MAE: 20.948 Val MAE: 18.830 Epoch time: 249.349 seconds best\n",
      "Experiment: few_heads, Epoch: 12 Train loss: 18.604 Val loss: 7.848 Train MAE: 20.311 Val MAE: 21.293 Epoch time: 249.729 seconds \n",
      "Experiment: few_heads, Epoch: 13 Train loss: 19.055 Val loss: 8.178 Train MAE: 20.692 Val MAE: 37.175 Epoch time: 247.778 seconds \n",
      "Experiment: few_heads, Epoch: 14 Train loss: 17.718 Val loss: 7.631 Train MAE: 21.535 Val MAE: 18.205 Epoch time: 248.733 seconds best\n",
      "Experiment: few_heads, Epoch: 15 Train loss: 18.548 Val loss: 7.507 Train MAE: 20.489 Val MAE: 23.690 Epoch time: 249.950 seconds \n",
      "Experiment: few_heads, Epoch: 16 Train loss: 17.461 Val loss: 7.862 Train MAE: 20.444 Val MAE: 22.999 Epoch time: 250.121 seconds \n",
      "Experiment: few_heads, Epoch: 17 Train loss: 17.778 Val loss: 7.747 Train MAE: 20.289 Val MAE: 24.134 Epoch time: 252.074 seconds \n",
      "Experiment: few_heads, Epoch: 18 Train loss: 17.172 Val loss: 7.493 Train MAE: 19.202 Val MAE: 21.284 Epoch time: 249.370 seconds \n",
      "Experiment: few_heads, Epoch: 19 Train loss: 18.698 Val loss: 7.588 Train MAE: 20.589 Val MAE: 23.701 Epoch time: 247.494 seconds \n",
      "Experiment: few_heads, Epoch: 20 Train loss: 17.195 Val loss: 7.342 Train MAE: 19.168 Val MAE: 19.353 Epoch time: 249.615 seconds \n",
      "Experiment: few_heads, Epoch: 21 Train loss: 17.675 Val loss: 7.413 Train MAE: 18.297 Val MAE: 26.785 Epoch time: 250.612 seconds \n",
      "Experiment: few_heads, Epoch: 22 Train loss: 16.528 Val loss: 7.563 Train MAE: 18.148 Val MAE: 23.232 Epoch time: 248.579 seconds \n",
      "Experiment: few_heads, Epoch: 23 Train loss: 17.890 Val loss: 7.316 Train MAE: 16.892 Val MAE: 32.620 Epoch time: 248.046 seconds \n",
      "Experiment: few_heads, Epoch: 24 Train loss: 16.480 Val loss: 7.144 Train MAE: 18.372 Val MAE: 21.577 Epoch time: 248.774 seconds \n",
      "Experiment: few_heads, Epoch: 25 Train loss: 16.027 Val loss: 7.112 Train MAE: 17.495 Val MAE: 15.369 Epoch time: 248.955 seconds best\n",
      "Experiment: few_heads, Epoch: 26 Train loss: 16.197 Val loss: 7.363 Train MAE: 16.840 Val MAE: 17.394 Epoch time: 248.354 seconds \n",
      "Experiment: few_heads, Epoch: 27 Train loss: 16.068 Val loss: 7.198 Train MAE: 17.325 Val MAE: 19.747 Epoch time: 249.562 seconds \n",
      "Experiment: few_heads, Epoch: 28 Train loss: 15.459 Val loss: 7.097 Train MAE: 15.938 Val MAE: 17.775 Epoch time: 266.521 seconds \n",
      "Experiment: few_heads, Epoch: 29 Train loss: 15.235 Val loss: 6.982 Train MAE: 16.481 Val MAE: 22.651 Epoch time: 277.700 seconds \n",
      "Experiment: few_heads, Epoch: 30 Train loss: 15.648 Val loss: 7.305 Train MAE: 17.161 Val MAE: 19.027 Epoch time: 283.952 seconds \n",
      "Experiment: few_heads, Epoch: 31 Train loss: 16.094 Val loss: 6.963 Train MAE: 16.998 Val MAE: 15.986 Epoch time: 282.791 seconds \n",
      "Experiment: few_heads, Epoch: 32 Train loss: 15.995 Val loss: 7.140 Train MAE: 16.614 Val MAE: 22.741 Epoch time: 285.011 seconds \n",
      "Experiment: few_heads, Epoch: 33 Train loss: 15.684 Val loss: 6.977 Train MAE: 16.793 Val MAE: 16.165 Epoch time: 280.545 seconds \n",
      "Experiment: few_heads, Epoch: 34 Train loss: 15.429 Val loss: 6.837 Train MAE: 17.226 Val MAE: 22.091 Epoch time: 276.261 seconds \n",
      "Experiment: few_heads, Epoch: 35 Train loss: 16.051 Val loss: 7.544 Train MAE: 16.296 Val MAE: 24.133 Epoch time: 279.920 seconds \n",
      "Experiment: few_heads, Epoch: 36 Train loss: 15.158 Val loss: 6.983 Train MAE: 14.964 Val MAE: 16.197 Epoch time: 279.756 seconds \n",
      "Experiment: few_heads, Epoch: 37 Train loss: 15.884 Val loss: 7.118 Train MAE: 16.793 Val MAE: 18.824 Epoch time: 275.906 seconds \n",
      "Experiment: few_heads, Epoch: 38 Train loss: 15.489 Val loss: 6.855 Train MAE: 15.729 Val MAE: 15.706 Epoch time: 280.710 seconds \n",
      "Experiment: few_heads, Epoch: 39 Train loss: 14.628 Val loss: 7.288 Train MAE: 16.148 Val MAE: 19.951 Epoch time: 281.135 seconds \n",
      "Experiment: few_heads, Epoch: 40 Train loss: 14.801 Val loss: 7.113 Train MAE: 14.403 Val MAE: 18.231 Epoch time: 284.195 seconds \n",
      "Experiment: few_heads, Epoch: 41 Train loss: 15.590 Val loss: 6.757 Train MAE: 15.010 Val MAE: 14.208 Epoch time: 282.753 seconds best\n",
      "Experiment: few_heads, Epoch: 42 Train loss: 15.885 Val loss: 6.902 Train MAE: 15.564 Val MAE: 18.162 Epoch time: 284.341 seconds \n",
      "Experiment: few_heads, Epoch: 43 Train loss: 15.028 Val loss: 6.970 Train MAE: 15.239 Val MAE: 27.144 Epoch time: 281.141 seconds \n",
      "Experiment: few_heads, Epoch: 44 Train loss: 15.231 Val loss: 7.087 Train MAE: 14.839 Val MAE: 15.297 Epoch time: 283.268 seconds \n",
      "Experiment: few_heads, Epoch: 45 Train loss: 13.852 Val loss: 6.721 Train MAE: 14.622 Val MAE: 14.894 Epoch time: 285.713 seconds \n",
      "Experiment: few_heads, Epoch: 46 Train loss: 14.931 Val loss: 7.002 Train MAE: 15.422 Val MAE: 24.111 Epoch time: 280.610 seconds \n",
      "Experiment: few_heads, Epoch: 47 Train loss: 15.189 Val loss: 6.970 Train MAE: 15.332 Val MAE: 27.395 Epoch time: 278.656 seconds \n",
      "Experiment: few_heads, Epoch: 48 Train loss: 14.873 Val loss: 6.864 Train MAE: 15.826 Val MAE: 20.049 Epoch time: 273.817 seconds \n",
      "Experiment: few_heads, Epoch: 49 Train loss: 14.280 Val loss: 6.855 Train MAE: 14.296 Val MAE: 16.865 Epoch time: 269.684 seconds \n",
      "Experiment: few_heads, Epoch: 50 Train loss: 15.125 Val loss: 6.611 Train MAE: 14.438 Val MAE: 14.342 Epoch time: 240.841 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment few_heads completed!\n",
      "Best validation MAE: 14.208\n",
      "Test MAE: 14.345\n",
      "Test RMSE: 68.689\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: many_heads\n",
      "Description: More attention heads (16)\n",
      "Experiment: many_heads, Epoch: 1 Train loss: 28.599 Val loss: 10.340 Train MAE: 53.219 Val MAE: 41.251 Epoch time: 240.984 seconds best\n",
      "Experiment: many_heads, Epoch: 2 Train loss: 26.847 Val loss: 9.830 Train MAE: 43.019 Val MAE: 25.803 Epoch time: 240.128 seconds best\n",
      "Experiment: many_heads, Epoch: 3 Train loss: 25.413 Val loss: 9.111 Train MAE: 34.990 Val MAE: 31.927 Epoch time: 242.813 seconds \n",
      "Experiment: many_heads, Epoch: 4 Train loss: 23.506 Val loss: 8.624 Train MAE: 30.959 Val MAE: 22.102 Epoch time: 242.852 seconds best\n",
      "Experiment: many_heads, Epoch: 5 Train loss: 21.951 Val loss: 8.773 Train MAE: 29.655 Val MAE: 19.107 Epoch time: 244.188 seconds best\n",
      "Experiment: many_heads, Epoch: 6 Train loss: 22.488 Val loss: 8.345 Train MAE: 29.064 Val MAE: 18.763 Epoch time: 242.846 seconds best\n",
      "Experiment: many_heads, Epoch: 7 Train loss: 20.668 Val loss: 8.612 Train MAE: 26.709 Val MAE: 35.640 Epoch time: 241.468 seconds \n",
      "Experiment: many_heads, Epoch: 8 Train loss: 21.029 Val loss: 7.955 Train MAE: 24.194 Val MAE: 23.602 Epoch time: 244.083 seconds \n",
      "Experiment: many_heads, Epoch: 9 Train loss: 19.806 Val loss: 8.116 Train MAE: 25.927 Val MAE: 20.700 Epoch time: 243.860 seconds \n",
      "Experiment: many_heads, Epoch: 10 Train loss: 20.124 Val loss: 8.077 Train MAE: 25.143 Val MAE: 26.615 Epoch time: 242.149 seconds \n",
      "Experiment: many_heads, Epoch: 11 Train loss: 19.328 Val loss: 7.802 Train MAE: 23.212 Val MAE: 24.119 Epoch time: 241.269 seconds \n",
      "Experiment: many_heads, Epoch: 12 Train loss: 19.655 Val loss: 7.759 Train MAE: 22.963 Val MAE: 18.503 Epoch time: 242.690 seconds best\n",
      "Experiment: many_heads, Epoch: 13 Train loss: 18.062 Val loss: 8.147 Train MAE: 22.313 Val MAE: 31.183 Epoch time: 243.227 seconds \n",
      "Experiment: many_heads, Epoch: 14 Train loss: 18.842 Val loss: 7.890 Train MAE: 23.261 Val MAE: 23.997 Epoch time: 245.302 seconds \n",
      "Experiment: many_heads, Epoch: 15 Train loss: 19.075 Val loss: 7.416 Train MAE: 20.699 Val MAE: 15.910 Epoch time: 243.293 seconds best\n",
      "Experiment: many_heads, Epoch: 16 Train loss: 19.671 Val loss: 7.760 Train MAE: 23.970 Val MAE: 26.090 Epoch time: 243.800 seconds \n",
      "Experiment: many_heads, Epoch: 17 Train loss: 18.121 Val loss: 8.019 Train MAE: 19.986 Val MAE: 23.693 Epoch time: 242.335 seconds \n",
      "Experiment: many_heads, Epoch: 18 Train loss: 17.032 Val loss: 7.604 Train MAE: 19.501 Val MAE: 22.889 Epoch time: 240.796 seconds \n",
      "Experiment: many_heads, Epoch: 19 Train loss: 17.608 Val loss: 8.104 Train MAE: 19.761 Val MAE: 34.306 Epoch time: 244.608 seconds \n",
      "Experiment: many_heads, Epoch: 20 Train loss: 17.493 Val loss: 8.268 Train MAE: 19.511 Val MAE: 36.689 Epoch time: 244.019 seconds \n",
      "Experiment: many_heads, Epoch: 21 Train loss: 17.332 Val loss: 7.698 Train MAE: 19.140 Val MAE: 33.905 Epoch time: 242.526 seconds \n",
      "Experiment: many_heads, Epoch: 22 Train loss: 17.352 Val loss: 7.884 Train MAE: 19.521 Val MAE: 21.957 Epoch time: 244.561 seconds \n",
      "Experiment: many_heads, Epoch: 23 Train loss: 17.341 Val loss: 7.504 Train MAE: 18.237 Val MAE: 24.577 Epoch time: 243.891 seconds \n",
      "Experiment: many_heads, Epoch: 24 Train loss: 15.536 Val loss: 7.806 Train MAE: 18.279 Val MAE: 28.739 Epoch time: 240.931 seconds \n",
      "Experiment: many_heads, Epoch: 25 Train loss: 16.835 Val loss: 7.888 Train MAE: 18.460 Val MAE: 21.674 Epoch time: 243.099 seconds \n",
      "Experiment: many_heads, Epoch: 26 Train loss: 16.648 Val loss: 7.468 Train MAE: 18.177 Val MAE: 19.183 Epoch time: 242.497 seconds \n",
      "Experiment: many_heads, Epoch: 27 Train loss: 15.783 Val loss: 7.677 Train MAE: 19.444 Val MAE: 25.286 Epoch time: 242.652 seconds \n",
      "Experiment: many_heads, Epoch: 28 Train loss: 16.451 Val loss: 7.537 Train MAE: 16.673 Val MAE: 22.404 Epoch time: 239.217 seconds \n",
      "Experiment: many_heads, Epoch: 29 Train loss: 16.706 Val loss: 7.485 Train MAE: 17.202 Val MAE: 21.329 Epoch time: 242.252 seconds \n",
      "Experiment: many_heads, Epoch: 30 Train loss: 16.296 Val loss: 7.422 Train MAE: 15.760 Val MAE: 25.019 Epoch time: 242.815 seconds \n",
      "Experiment: many_heads, Epoch: 31 Train loss: 16.289 Val loss: 7.236 Train MAE: 18.359 Val MAE: 18.663 Epoch time: 242.433 seconds \n",
      "Experiment: many_heads, Epoch: 32 Train loss: 15.914 Val loss: 7.402 Train MAE: 17.431 Val MAE: 22.112 Epoch time: 241.866 seconds \n",
      "Experiment: many_heads, Epoch: 33 Train loss: 15.811 Val loss: 7.187 Train MAE: 16.477 Val MAE: 18.795 Epoch time: 241.982 seconds \n",
      "Experiment: many_heads, Epoch: 34 Train loss: 15.659 Val loss: 7.387 Train MAE: 16.829 Val MAE: 27.896 Epoch time: 241.880 seconds \n",
      "Experiment: many_heads, Epoch: 35 Train loss: 15.894 Val loss: 7.398 Train MAE: 16.734 Val MAE: 18.517 Epoch time: 243.387 seconds \n",
      "Experiment: many_heads, Epoch: 36 Train loss: 15.722 Val loss: 7.406 Train MAE: 17.200 Val MAE: 20.634 Epoch time: 243.139 seconds \n",
      "Experiment: many_heads, Epoch: 37 Train loss: 16.399 Val loss: 7.187 Train MAE: 16.652 Val MAE: 19.909 Epoch time: 243.228 seconds \n",
      "Experiment: many_heads, Epoch: 38 Train loss: 15.978 Val loss: 7.371 Train MAE: 16.699 Val MAE: 20.623 Epoch time: 243.406 seconds \n",
      "Experiment: many_heads, Epoch: 39 Train loss: 15.716 Val loss: 7.385 Train MAE: 15.660 Val MAE: 21.605 Epoch time: 241.185 seconds \n",
      "Experiment: many_heads, Epoch: 40 Train loss: 14.606 Val loss: 7.096 Train MAE: 15.010 Val MAE: 19.809 Epoch time: 241.196 seconds \n",
      "Experiment: many_heads, Epoch: 41 Train loss: 15.315 Val loss: 7.490 Train MAE: 15.144 Val MAE: 23.127 Epoch time: 243.270 seconds \n",
      "Experiment: many_heads, Epoch: 42 Train loss: 15.767 Val loss: 7.447 Train MAE: 15.761 Val MAE: 34.656 Epoch time: 240.352 seconds \n",
      "Experiment: many_heads, Epoch: 43 Train loss: 14.632 Val loss: 7.132 Train MAE: 15.938 Val MAE: 20.268 Epoch time: 242.427 seconds \n",
      "Experiment: many_heads, Epoch: 44 Train loss: 14.420 Val loss: 7.626 Train MAE: 14.257 Val MAE: 24.010 Epoch time: 244.141 seconds \n",
      "Experiment: many_heads, Epoch: 45 Train loss: 15.743 Val loss: 7.427 Train MAE: 17.413 Val MAE: 18.539 Epoch time: 242.705 seconds \n",
      "Experiment: many_heads, Epoch: 46 Train loss: 15.443 Val loss: 7.446 Train MAE: 17.655 Val MAE: 29.052 Epoch time: 240.553 seconds \n",
      "Experiment: many_heads, Epoch: 47 Train loss: 13.950 Val loss: 6.846 Train MAE: 14.411 Val MAE: 15.265 Epoch time: 241.449 seconds best\n",
      "Experiment: many_heads, Epoch: 48 Train loss: 14.958 Val loss: 7.375 Train MAE: 16.239 Val MAE: 23.191 Epoch time: 240.389 seconds \n",
      "Experiment: many_heads, Epoch: 49 Train loss: 14.583 Val loss: 7.338 Train MAE: 14.997 Val MAE: 20.067 Epoch time: 242.549 seconds \n",
      "Experiment: many_heads, Epoch: 50 Train loss: 14.881 Val loss: 7.051 Train MAE: 14.570 Val MAE: 16.878 Epoch time: 245.255 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment many_heads completed!\n",
      "Best validation MAE: 15.265\n",
      "Test MAE: 14.078\n",
      "Test RMSE: 89.114\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: small_kernel\n",
      "Description: Smaller kernel dimension (1x1)\n",
      "Experiment: small_kernel, Epoch: 1 Train loss: 27.723 Val loss: 9.056 Train MAE: 50.556 Val MAE: 21.509 Epoch time: 239.593 seconds best\n",
      "Experiment: small_kernel, Epoch: 2 Train loss: 25.090 Val loss: 9.828 Train MAE: 38.452 Val MAE: 37.540 Epoch time: 238.203 seconds \n",
      "Experiment: small_kernel, Epoch: 3 Train loss: 22.983 Val loss: 8.693 Train MAE: 33.077 Val MAE: 24.609 Epoch time: 240.026 seconds \n",
      "Experiment: small_kernel, Epoch: 4 Train loss: 21.110 Val loss: 8.067 Train MAE: 30.981 Val MAE: 18.179 Epoch time: 237.699 seconds best\n",
      "Experiment: small_kernel, Epoch: 5 Train loss: 20.277 Val loss: 7.958 Train MAE: 27.820 Val MAE: 19.944 Epoch time: 239.624 seconds \n",
      "Experiment: small_kernel, Epoch: 6 Train loss: 20.971 Val loss: 8.049 Train MAE: 27.287 Val MAE: 23.609 Epoch time: 239.570 seconds \n",
      "Experiment: small_kernel, Epoch: 7 Train loss: 19.420 Val loss: 8.587 Train MAE: 27.462 Val MAE: 30.156 Epoch time: 237.833 seconds \n",
      "Experiment: small_kernel, Epoch: 8 Train loss: 20.523 Val loss: 8.272 Train MAE: 24.668 Val MAE: 26.226 Epoch time: 239.906 seconds \n",
      "Experiment: small_kernel, Epoch: 9 Train loss: 19.250 Val loss: 7.564 Train MAE: 22.700 Val MAE: 17.787 Epoch time: 238.201 seconds best\n",
      "Experiment: small_kernel, Epoch: 10 Train loss: 18.321 Val loss: 7.285 Train MAE: 22.798 Val MAE: 20.709 Epoch time: 238.858 seconds \n",
      "Experiment: small_kernel, Epoch: 11 Train loss: 19.667 Val loss: 7.778 Train MAE: 23.120 Val MAE: 21.252 Epoch time: 239.372 seconds \n",
      "Experiment: small_kernel, Epoch: 12 Train loss: 17.768 Val loss: 7.322 Train MAE: 20.627 Val MAE: 21.474 Epoch time: 236.834 seconds \n",
      "Experiment: small_kernel, Epoch: 13 Train loss: 18.467 Val loss: 7.312 Train MAE: 21.481 Val MAE: 24.465 Epoch time: 238.107 seconds \n",
      "Experiment: small_kernel, Epoch: 14 Train loss: 16.312 Val loss: 7.786 Train MAE: 19.677 Val MAE: 28.984 Epoch time: 239.305 seconds \n",
      "Experiment: small_kernel, Epoch: 15 Train loss: 17.147 Val loss: 7.418 Train MAE: 19.825 Val MAE: 26.245 Epoch time: 237.801 seconds \n",
      "Experiment: small_kernel, Epoch: 16 Train loss: 17.223 Val loss: 7.249 Train MAE: 19.911 Val MAE: 16.975 Epoch time: 243.047 seconds best\n",
      "Experiment: small_kernel, Epoch: 17 Train loss: 17.337 Val loss: 6.977 Train MAE: 18.930 Val MAE: 22.868 Epoch time: 240.152 seconds \n",
      "Experiment: small_kernel, Epoch: 18 Train loss: 16.731 Val loss: 7.213 Train MAE: 19.934 Val MAE: 20.289 Epoch time: 243.457 seconds \n",
      "Experiment: small_kernel, Epoch: 19 Train loss: 15.919 Val loss: 7.010 Train MAE: 17.497 Val MAE: 16.859 Epoch time: 243.305 seconds best\n",
      "Experiment: small_kernel, Epoch: 20 Train loss: 16.144 Val loss: 7.080 Train MAE: 18.851 Val MAE: 18.373 Epoch time: 240.558 seconds \n",
      "Experiment: small_kernel, Epoch: 21 Train loss: 16.076 Val loss: 7.116 Train MAE: 17.772 Val MAE: 20.798 Epoch time: 238.019 seconds \n",
      "Experiment: small_kernel, Epoch: 22 Train loss: 15.958 Val loss: 7.067 Train MAE: 19.969 Val MAE: 22.273 Epoch time: 240.688 seconds \n",
      "Experiment: small_kernel, Epoch: 23 Train loss: 16.891 Val loss: 6.765 Train MAE: 17.587 Val MAE: 17.264 Epoch time: 239.782 seconds \n",
      "Experiment: small_kernel, Epoch: 24 Train loss: 15.366 Val loss: 6.990 Train MAE: 16.906 Val MAE: 20.835 Epoch time: 244.072 seconds \n",
      "Experiment: small_kernel, Epoch: 25 Train loss: 16.323 Val loss: 7.036 Train MAE: 17.911 Val MAE: 19.091 Epoch time: 243.141 seconds \n",
      "Experiment: small_kernel, Epoch: 26 Train loss: 15.000 Val loss: 7.265 Train MAE: 17.057 Val MAE: 18.081 Epoch time: 239.462 seconds \n",
      "Experiment: small_kernel, Epoch: 27 Train loss: 15.566 Val loss: 6.701 Train MAE: 17.623 Val MAE: 17.852 Epoch time: 242.952 seconds \n",
      "Experiment: small_kernel, Epoch: 28 Train loss: 15.439 Val loss: 6.653 Train MAE: 16.747 Val MAE: 18.252 Epoch time: 242.194 seconds \n",
      "Experiment: small_kernel, Epoch: 29 Train loss: 15.217 Val loss: 6.562 Train MAE: 15.288 Val MAE: 22.420 Epoch time: 240.440 seconds \n",
      "Experiment: small_kernel, Epoch: 30 Train loss: 16.165 Val loss: 6.617 Train MAE: 16.708 Val MAE: 22.941 Epoch time: 239.908 seconds \n",
      "Experiment: small_kernel, Epoch: 31 Train loss: 15.455 Val loss: 6.789 Train MAE: 17.363 Val MAE: 16.657 Epoch time: 239.273 seconds best\n",
      "Experiment: small_kernel, Epoch: 32 Train loss: 14.819 Val loss: 6.547 Train MAE: 16.006 Val MAE: 16.588 Epoch time: 239.231 seconds best\n",
      "Experiment: small_kernel, Epoch: 33 Train loss: 15.575 Val loss: 6.504 Train MAE: 16.089 Val MAE: 16.224 Epoch time: 240.896 seconds best\n",
      "Experiment: small_kernel, Epoch: 34 Train loss: 16.186 Val loss: 6.704 Train MAE: 16.416 Val MAE: 16.782 Epoch time: 242.566 seconds \n",
      "Experiment: small_kernel, Epoch: 35 Train loss: 15.951 Val loss: 6.427 Train MAE: 18.118 Val MAE: 14.432 Epoch time: 238.503 seconds best\n",
      "Experiment: small_kernel, Epoch: 36 Train loss: 14.914 Val loss: 6.690 Train MAE: 16.444 Val MAE: 16.956 Epoch time: 237.223 seconds \n",
      "Experiment: small_kernel, Epoch: 37 Train loss: 15.051 Val loss: 6.572 Train MAE: 15.521 Val MAE: 20.613 Epoch time: 239.255 seconds \n",
      "Experiment: small_kernel, Epoch: 38 Train loss: 14.671 Val loss: 6.440 Train MAE: 15.054 Val MAE: 15.360 Epoch time: 237.428 seconds \n",
      "Experiment: small_kernel, Epoch: 39 Train loss: 15.161 Val loss: 6.583 Train MAE: 16.686 Val MAE: 20.081 Epoch time: 241.904 seconds \n",
      "Experiment: small_kernel, Epoch: 40 Train loss: 14.325 Val loss: 6.869 Train MAE: 14.382 Val MAE: 19.074 Epoch time: 240.795 seconds \n",
      "Experiment: small_kernel, Epoch: 41 Train loss: 15.015 Val loss: 6.469 Train MAE: 15.820 Val MAE: 16.367 Epoch time: 241.378 seconds \n",
      "Experiment: small_kernel, Epoch: 42 Train loss: 14.930 Val loss: 6.653 Train MAE: 14.589 Val MAE: 15.927 Epoch time: 240.786 seconds \n",
      "Experiment: small_kernel, Epoch: 43 Train loss: 14.577 Val loss: 6.513 Train MAE: 15.274 Val MAE: 22.678 Epoch time: 239.716 seconds \n",
      "Experiment: small_kernel, Epoch: 44 Train loss: 14.936 Val loss: 6.494 Train MAE: 14.985 Val MAE: 23.157 Epoch time: 239.916 seconds \n",
      "Experiment: small_kernel, Epoch: 45 Train loss: 13.811 Val loss: 6.395 Train MAE: 14.453 Val MAE: 14.566 Epoch time: 239.307 seconds \n",
      "Experiment: small_kernel, Epoch: 46 Train loss: 14.755 Val loss: 7.015 Train MAE: 15.719 Val MAE: 19.497 Epoch time: 241.006 seconds \n",
      "Experiment: small_kernel, Epoch: 47 Train loss: 15.254 Val loss: 6.668 Train MAE: 15.722 Val MAE: 15.701 Epoch time: 241.271 seconds \n",
      "Experiment: small_kernel, Epoch: 48 Train loss: 14.231 Val loss: 6.580 Train MAE: 16.392 Val MAE: 18.570 Epoch time: 241.230 seconds \n",
      "Experiment: small_kernel, Epoch: 49 Train loss: 15.345 Val loss: 6.845 Train MAE: 16.741 Val MAE: 21.880 Epoch time: 241.271 seconds \n",
      "Experiment: small_kernel, Epoch: 50 Train loss: 15.039 Val loss: 6.538 Train MAE: 14.858 Val MAE: 15.713 Epoch time: 243.438 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment small_kernel completed!\n",
      "Best validation MAE: 14.432\n",
      "Test MAE: 14.293\n",
      "Test RMSE: 78.917\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: large_kernel\n",
      "Description: Larger kernel dimension (5x5)\n",
      "Experiment: large_kernel, Epoch: 1 Train loss: 29.727 Val loss: 10.305 Train MAE: 53.593 Val MAE: 26.363 Epoch time: 250.942 seconds best\n",
      "Experiment: large_kernel, Epoch: 2 Train loss: 27.054 Val loss: 9.818 Train MAE: 38.553 Val MAE: 28.904 Epoch time: 248.169 seconds \n",
      "Experiment: large_kernel, Epoch: 3 Train loss: 26.428 Val loss: 9.394 Train MAE: 34.753 Val MAE: 24.743 Epoch time: 246.745 seconds best\n",
      "Experiment: large_kernel, Epoch: 4 Train loss: 23.637 Val loss: 9.132 Train MAE: 32.592 Val MAE: 31.767 Epoch time: 246.513 seconds \n",
      "Experiment: large_kernel, Epoch: 5 Train loss: 22.850 Val loss: 9.325 Train MAE: 31.817 Val MAE: 23.708 Epoch time: 246.091 seconds best\n",
      "Experiment: large_kernel, Epoch: 6 Train loss: 22.790 Val loss: 8.574 Train MAE: 28.141 Val MAE: 21.452 Epoch time: 248.045 seconds best\n",
      "Experiment: large_kernel, Epoch: 7 Train loss: 21.352 Val loss: 8.739 Train MAE: 26.832 Val MAE: 32.784 Epoch time: 247.838 seconds \n",
      "Experiment: large_kernel, Epoch: 8 Train loss: 20.881 Val loss: 8.850 Train MAE: 24.536 Val MAE: 36.332 Epoch time: 247.802 seconds \n",
      "Experiment: large_kernel, Epoch: 9 Train loss: 19.976 Val loss: 8.237 Train MAE: 24.406 Val MAE: 29.458 Epoch time: 248.476 seconds \n",
      "Experiment: large_kernel, Epoch: 10 Train loss: 20.023 Val loss: 8.120 Train MAE: 24.099 Val MAE: 19.089 Epoch time: 247.183 seconds best\n",
      "Experiment: large_kernel, Epoch: 11 Train loss: 19.558 Val loss: 8.207 Train MAE: 25.832 Val MAE: 25.709 Epoch time: 246.811 seconds \n",
      "Experiment: large_kernel, Epoch: 12 Train loss: 19.417 Val loss: 8.226 Train MAE: 22.268 Val MAE: 31.589 Epoch time: 247.245 seconds \n",
      "Experiment: large_kernel, Epoch: 13 Train loss: 20.132 Val loss: 8.442 Train MAE: 25.061 Val MAE: 35.372 Epoch time: 248.323 seconds \n",
      "Experiment: large_kernel, Epoch: 14 Train loss: 18.884 Val loss: 8.155 Train MAE: 22.211 Val MAE: 23.005 Epoch time: 248.028 seconds \n",
      "Experiment: large_kernel, Epoch: 15 Train loss: 18.238 Val loss: 7.973 Train MAE: 20.721 Val MAE: 36.868 Epoch time: 251.904 seconds \n",
      "Experiment: large_kernel, Epoch: 16 Train loss: 18.960 Val loss: 7.558 Train MAE: 22.075 Val MAE: 24.287 Epoch time: 251.301 seconds \n",
      "Experiment: large_kernel, Epoch: 17 Train loss: 18.385 Val loss: 7.873 Train MAE: 20.028 Val MAE: 24.448 Epoch time: 249.588 seconds \n",
      "Experiment: large_kernel, Epoch: 18 Train loss: 18.854 Val loss: 7.831 Train MAE: 22.255 Val MAE: 27.481 Epoch time: 247.780 seconds \n",
      "Experiment: large_kernel, Epoch: 19 Train loss: 17.979 Val loss: 7.737 Train MAE: 20.834 Val MAE: 22.449 Epoch time: 248.830 seconds \n",
      "Experiment: large_kernel, Epoch: 20 Train loss: 16.922 Val loss: 7.920 Train MAE: 18.801 Val MAE: 27.999 Epoch time: 248.287 seconds \n",
      "Experiment: large_kernel, Epoch: 21 Train loss: 17.785 Val loss: 7.654 Train MAE: 19.571 Val MAE: 29.883 Epoch time: 252.165 seconds \n",
      "Experiment: large_kernel, Epoch: 22 Train loss: 17.238 Val loss: 7.520 Train MAE: 18.533 Val MAE: 17.175 Epoch time: 251.023 seconds best\n",
      "Experiment: large_kernel, Epoch: 23 Train loss: 16.855 Val loss: 7.618 Train MAE: 18.857 Val MAE: 22.429 Epoch time: 252.114 seconds \n",
      "Experiment: large_kernel, Epoch: 24 Train loss: 17.263 Val loss: 7.758 Train MAE: 18.405 Val MAE: 24.189 Epoch time: 249.614 seconds \n",
      "Experiment: large_kernel, Epoch: 25 Train loss: 17.062 Val loss: 7.577 Train MAE: 17.600 Val MAE: 30.211 Epoch time: 248.212 seconds \n",
      "Experiment: large_kernel, Epoch: 26 Train loss: 16.033 Val loss: 7.419 Train MAE: 16.836 Val MAE: 24.345 Epoch time: 248.294 seconds \n",
      "Experiment: large_kernel, Epoch: 27 Train loss: 16.393 Val loss: 7.585 Train MAE: 18.834 Val MAE: 26.757 Epoch time: 247.916 seconds \n",
      "Experiment: large_kernel, Epoch: 28 Train loss: 15.970 Val loss: 7.716 Train MAE: 17.264 Val MAE: 28.301 Epoch time: 247.291 seconds \n",
      "Experiment: large_kernel, Epoch: 29 Train loss: 16.757 Val loss: 7.459 Train MAE: 18.616 Val MAE: 19.178 Epoch time: 245.943 seconds \n",
      "Experiment: large_kernel, Epoch: 30 Train loss: 17.104 Val loss: 7.495 Train MAE: 18.284 Val MAE: 32.449 Epoch time: 246.447 seconds \n",
      "Experiment: large_kernel, Epoch: 31 Train loss: 16.276 Val loss: 7.205 Train MAE: 17.467 Val MAE: 26.707 Epoch time: 246.790 seconds \n",
      "Experiment: large_kernel, Epoch: 32 Train loss: 15.768 Val loss: 7.501 Train MAE: 17.815 Val MAE: 28.565 Epoch time: 246.092 seconds \n",
      "Experiment: large_kernel, Epoch: 33 Train loss: 17.059 Val loss: 7.109 Train MAE: 17.199 Val MAE: 24.874 Epoch time: 248.049 seconds \n",
      "Experiment: large_kernel, Epoch: 34 Train loss: 16.372 Val loss: 7.425 Train MAE: 17.308 Val MAE: 26.420 Epoch time: 244.777 seconds \n",
      "Experiment: large_kernel, Epoch: 35 Train loss: 15.835 Val loss: 7.462 Train MAE: 16.925 Val MAE: 22.248 Epoch time: 247.423 seconds \n",
      "Experiment: large_kernel, Epoch: 36 Train loss: 16.040 Val loss: 7.165 Train MAE: 15.374 Val MAE: 19.881 Epoch time: 247.731 seconds \n",
      "Experiment: large_kernel, Epoch: 37 Train loss: 16.652 Val loss: 7.063 Train MAE: 16.487 Val MAE: 17.036 Epoch time: 250.383 seconds best\n",
      "Experiment: large_kernel, Epoch: 38 Train loss: 16.368 Val loss: 7.282 Train MAE: 17.680 Val MAE: 17.721 Epoch time: 247.896 seconds \n",
      "Experiment: large_kernel, Epoch: 39 Train loss: 15.238 Val loss: 7.146 Train MAE: 16.050 Val MAE: 19.380 Epoch time: 247.465 seconds \n",
      "Experiment: large_kernel, Epoch: 40 Train loss: 16.481 Val loss: 6.997 Train MAE: 16.773 Val MAE: 17.315 Epoch time: 248.440 seconds \n",
      "Experiment: large_kernel, Epoch: 41 Train loss: 15.019 Val loss: 7.492 Train MAE: 15.290 Val MAE: 32.098 Epoch time: 247.337 seconds \n",
      "Experiment: large_kernel, Epoch: 42 Train loss: 17.100 Val loss: 6.961 Train MAE: 16.948 Val MAE: 18.496 Epoch time: 247.235 seconds \n",
      "Experiment: large_kernel, Epoch: 43 Train loss: 15.212 Val loss: 7.083 Train MAE: 15.433 Val MAE: 28.528 Epoch time: 249.651 seconds \n",
      "Experiment: large_kernel, Epoch: 44 Train loss: 15.273 Val loss: 6.969 Train MAE: 16.122 Val MAE: 21.178 Epoch time: 247.909 seconds \n",
      "Experiment: large_kernel, Epoch: 45 Train loss: 14.691 Val loss: 6.869 Train MAE: 16.300 Val MAE: 16.255 Epoch time: 250.741 seconds best\n",
      "Experiment: large_kernel, Epoch: 46 Train loss: 14.472 Val loss: 7.628 Train MAE: 15.540 Val MAE: 29.203 Epoch time: 250.047 seconds \n",
      "Experiment: large_kernel, Epoch: 47 Train loss: 16.524 Val loss: 6.851 Train MAE: 16.543 Val MAE: 17.114 Epoch time: 246.874 seconds \n",
      "Experiment: large_kernel, Epoch: 48 Train loss: 15.008 Val loss: 7.026 Train MAE: 16.916 Val MAE: 28.585 Epoch time: 246.576 seconds \n",
      "Experiment: large_kernel, Epoch: 49 Train loss: 14.600 Val loss: 7.081 Train MAE: 14.792 Val MAE: 20.378 Epoch time: 247.022 seconds \n",
      "Experiment: large_kernel, Epoch: 50 Train loss: 15.875 Val loss: 6.848 Train MAE: 14.287 Val MAE: 17.806 Epoch time: 246.783 seconds \n",
      "Loading best model for test evaluation...\n",
      "Experiment large_kernel completed!\n",
      "Best validation MAE: 16.255\n",
      "Test MAE: 14.900\n",
      "Test RMSE: 76.418\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Starting experiment: small_reduction\n",
      "Description: Smaller feature reduction (4)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: Traceback (most recent call last):\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module10/run-ablation-study.py\", line 188, in <module>\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     main()\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module10/run-ablation-study.py\", line 167, in main\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     study.run_experiment(\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module10/ablation_study_framework.py\", line 192, in run_experiment\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     out, aux_out = model(img, bboxes)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1643, in forward\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     else self._run_ddp_forward(*inputs, **kwargs)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1459, in _run_ddp_forward\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module10/engine_modified.py\", line 144, in forward\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     predicted_dmaps = self.aux_heads[i](response_maps)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module10/regression_head.py\", line 47, in forward\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     return self.regressor(x)\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1931, in __getattr__\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]:     raise AttributeError(\n",
      "[rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: [rank0]: AttributeError: 'DensityMapRegressor' object has no attribute 'regressor'\n",
      "[rank0]:[W316 16:36:48.358050374 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "E0316 16:36:50.594000 271685 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 271694) of binary: /opt/miniconda/envs/Rey2/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda/envs/Rey2/bin/torchrun\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('torch==2.5.1', 'console_scripts', 'torchrun')())\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "module10/run-ablation-study.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-03-16_16:36:50\n",
      "  host      : viplab-G481-H81-00\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 271694)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 torchrun --nproc_per_node=1 module10/run-ablation-study.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091b65da-30c0-4401-9494-f1d215a51bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ABLATION STUDY SUMMARY\n",
      "================================================================================\n",
      "Experiment                     Description                    Test MAE   Test RMSE  Rel. Imp.%\n",
      "--------------------------------------------------------------------------------\n",
      "one_encoder_layer              One hybrid encoder layer       12.804     72.909     20.05     \n",
      "five_encoder_layers            Five hybrid encoder layers     12.990     71.134     18.89     \n",
      "five_iefl_steps                Five iEFL iterative steps      13.294     71.547     16.99     \n",
      "backbone_trainable             Trainable backbone weights     13.994     87.049     12.62     \n",
      "large_emb_dim                  Larger embedding dimension (51 14.057     85.206     12.23     \n",
      "many_heads                     More attention heads (16)      14.078     89.114     12.10     \n",
      "small_kernel                   Smaller kernel dimension (1x1) 14.293     78.917     10.75     \n",
      "few_heads                      Fewer attention heads (4)      14.345     68.689     10.43     \n",
      "large_kernel                   Larger kernel dimension (5x5)  14.900     76.418     6.97      \n",
      "backbone_frozen                Frozen backbone weights        15.009     92.208     6.29      \n",
      "one_iefl_step                  One iEFL iterative step        15.202     89.121     5.08      \n",
      "baseline                       Full model with all components 16.015     85.214     N/A       \n",
      "small_emb_dim                  Smaller embedding dimension (1 16.485     87.521     -2.93     \n",
      "no_encoder                     No hybrid encoder layers       17.736     84.756     -10.75    \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Tentukan path ke direktori hasil ablation study\n",
    "results_dir = \"/home/renaldy_fredyan/PhDResearch/ELS/checkpoints/ablation_results\"  # Sesuaikan dengan path Anda\n",
    "\n",
    "# Kumpulkan semua hasil yang tersedia\n",
    "all_results = {}\n",
    "baseline_mae = None\n",
    "\n",
    "# Cari semua file results.json di subdirektori\n",
    "for result_file in glob.glob(os.path.join(results_dir, \"*/results.json\")):\n",
    "    experiment_name = os.path.basename(os.path.dirname(result_file))\n",
    "    \n",
    "    with open(result_file, 'r') as f:\n",
    "        result_data = json.load(f)\n",
    "        all_results[experiment_name] = result_data\n",
    "        \n",
    "        # Simpan baseline MAE jika ini eksperimen baseline\n",
    "        if experiment_name == \"baseline\":\n",
    "            baseline_mae = result_data[\"test_mae\"]\n",
    "\n",
    "# Hitung relative improvement jika baseline ditemukan\n",
    "if baseline_mae is not None:\n",
    "    for name, result in all_results.items():\n",
    "        if name != \"baseline\":\n",
    "            rel_improvement = ((baseline_mae - result[\"test_mae\"]) / baseline_mae) * 100\n",
    "            all_results[name][\"relative_improvement\"] = rel_improvement\n",
    "else:\n",
    "    print(\"Warning: Baseline experiment not found. Cannot calculate relative improvements.\")\n",
    "\n",
    "# Buat ringkasan\n",
    "summary = {\n",
    "    \"experiments\": all_results,\n",
    "    \"baseline\": all_results.get(\"baseline\")\n",
    "}\n",
    "\n",
    "# Simpan ringkasan\n",
    "summary_path = os.path.join(results_dir, \"manual_ablation_summary.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# Tampilkan tabel ringkasan\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ABLATION STUDY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Experiment':<30} {'Description':<30} {'Test MAE':<10} {'Test RMSE':<10} {'Rel. Imp.%':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for name, result in sorted(all_results.items(), key=lambda x: x[1][\"test_mae\"]):\n",
    "    rel_imp = result.get(\"relative_improvement\", \"N/A\")\n",
    "    rel_imp_str = f\"{rel_imp:.2f}\" if isinstance(rel_imp, float) else rel_imp\n",
    "    \n",
    "    print(f\"{name:<30} {result['description'][:30]:<30} {result['test_mae']:<10.3f} {result['test_rmse']:<10.3f} {rel_imp_str:<10}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c33c4d-9010-4b7d-baf5-c475fd723dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7cb35ed-4df2-474a-9780-41c66ba29a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0314 14:52:30.842000 265890 site-packages/torch/distributed/run.py:793] \n",
      "W0314 14:52:30.842000 265890 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0314 14:52:30.842000 265890 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0314 14:52:30.842000 265890 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Rank 0: Tensor awal = tensor([0.], device='cuda:0')\n",
      "viplab-G481-H81-00:265906:265906 [0] NCCL INFO Bootstrap : Using eno1:140.118.125.208<0>\n",
      "viplab-G481-H81-00:265906:265906 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\n",
      "viplab-G481-H81-00:265906:265906 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so\n",
      "viplab-G481-H81-00:265906:265906 [0] NCCL INFO NET/Plugin: Using internal network plugin.\n",
      "viplab-G481-H81-00:265906:265906 [0] NCCL INFO cudaDriverVersion 12060\n",
      "NCCL version 2.21.5+cuda12.1\n",
      "Rank 3: Tensor awal = tensor([3.], device='cuda:3')\n",
      "viplab-G481-H81-00:265909:265909 [3] NCCL INFO cudaDriverVersion 12060\n",
      "viplab-G481-H81-00:265909:265909 [3] NCCL INFO Bootstrap : Using eno1:140.118.125.208<0>\n",
      "viplab-G481-H81-00:265909:265909 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\n",
      "viplab-G481-H81-00:265909:265909 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so\n",
      "viplab-G481-H81-00:265909:265909 [3] NCCL INFO NET/Plugin: Using internal network plugin.\n",
      "Rank 1: Tensor awal = tensor([1.], device='cuda:1')\n",
      "Rank 2: Tensor awal = tensor([2.], device='cuda:2')\n",
      "viplab-G481-H81-00:265907:265907 [1] NCCL INFO cudaDriverVersion 12060\n",
      "viplab-G481-H81-00:265908:265908 [2] NCCL INFO cudaDriverVersion 12060\n",
      "viplab-G481-H81-00:265907:265907 [1] NCCL INFO Bootstrap : Using eno1:140.118.125.208<0>\n",
      "viplab-G481-H81-00:265908:265908 [2] NCCL INFO Bootstrap : Using eno1:140.118.125.208<0>\n",
      "viplab-G481-H81-00:265907:265907 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\n",
      "viplab-G481-H81-00:265908:265908 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)\n",
      "viplab-G481-H81-00:265907:265907 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so\n",
      "viplab-G481-H81-00:265908:265908 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so\n",
      "viplab-G481-H81-00:265907:265907 [1] NCCL INFO NET/Plugin: Using internal network plugin.\n",
      "viplab-G481-H81-00:265908:265908 [2] NCCL INFO NET/Plugin: Using internal network plugin.\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO NET/Socket : Using [0]eno1:140.118.125.208<0> [1]ham0:25.5.174.203<0>\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Using non-device net plugin version 0\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Using network Socket\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO NET/Socket : Using [0]eno1:140.118.125.208<0> [1]ham0:25.5.174.203<0>\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Using non-device net plugin version 0\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Using network Socket\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO NET/Socket : Using [0]eno1:140.118.125.208<0> [1]ham0:25.5.174.203<0>\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Using non-device net plugin version 0\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Using network Socket\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO NET/Socket : Using [0]eno1:140.118.125.208<0> [1]ham0:25.5.174.203<0>\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Using non-device net plugin version 0\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Using network Socket\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO ncclCommInitRank comm 0x9b63a70 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId b1000 commId 0xc9c048b04979e3de - Init START\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO ncclCommInitRank comm 0xcb141c0 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId db000 commId 0xc9c048b04979e3de - Init START\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO ncclCommInitRank comm 0xdcf48e0 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId da000 commId 0xc9c048b04979e3de - Init START\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO ncclCommInitRank comm 0xd650b30 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId b2000 commId 0xc9c048b04979e3de - Init START\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Setting affinity for GPU 4 to ff00ff00\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO NVLS multicast support is not available on dev 0\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Setting affinity for GPU 7 to ff00ff00\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO NVLS multicast support is not available on dev 3\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Setting affinity for GPU 6 to ff00ff00\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO P2P is disabled between connected GPUs 5 and 4. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO P2P is disabled between connected GPUs 4 and 5. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO P2P is disabled between connected GPUs 7 and 6. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO P2P is disabled between connected GPUs 6 and 7. You can repress this message with NCCL_IGNORE_DISABLED_P2P=1.\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Setting affinity for GPU 5 to ff00ff00\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO NVLS multicast support is not available on dev 2\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO NVLS multicast support is not available on dev 1\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO comm 0xdcf48e0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO comm 0x9b63a70 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO comm 0xd650b30 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO comm 0xcb141c0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Channel 00/02 :    0   1   2   3\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Channel 01/02 :    0   1   2   3\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO P2P Chunksize set to 131072\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO P2P Chunksize set to 131072\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO P2P Chunksize set to 131072\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Channel 00 : 2[6] -> 3[7] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Channel 00 : 0[4] -> 1[5] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Channel 01 : 2[6] -> 3[7] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Channel 01 : 0[4] -> 1[5] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Channel 00 : 3[7] -> 0[4] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Channel 00 : 1[5] -> 2[6] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Channel 01 : 3[7] -> 0[4] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Channel 01 : 1[5] -> 2[6] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Connected all rings\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Connected all rings\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Connected all rings\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Connected all rings\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Channel 00 : 3[7] -> 2[6] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Channel 01 : 3[7] -> 2[6] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Channel 00 : 2[6] -> 1[5] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Channel 00 : 1[5] -> 0[4] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Channel 01 : 2[6] -> 1[5] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Channel 01 : 1[5] -> 0[4] via SHM/direct/direct\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO Connected all trees\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO Connected all trees\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO Connected all trees\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO Connected all trees\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.\n",
      "viplab-G481-H81-00:265906:265961 [0] NCCL INFO ncclCommInitRank comm 0x9b63a70 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId b1000 commId 0xc9c048b04979e3de - Init COMPLETE\n",
      "viplab-G481-H81-00:265908:265963 [2] NCCL INFO ncclCommInitRank comm 0xdcf48e0 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId da000 commId 0xc9c048b04979e3de - Init COMPLETE\n",
      "viplab-G481-H81-00:265909:265962 [3] NCCL INFO ncclCommInitRank comm 0xcb141c0 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId db000 commId 0xc9c048b04979e3de - Init COMPLETE\n",
      "viplab-G481-H81-00:265907:265964 [1] NCCL INFO ncclCommInitRank comm 0xd650b30 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId b2000 commId 0xc9c048b04979e3de - Init COMPLETE\n",
      "Rank 3: Tensor setelah all_reduce = tensor([6.], device='cuda:3'), Seharusnya = 6\n",
      "viplab-G481-H81-00:265909:265968 [3] NCCL INFO [Service thread] Connection closed by localRank 3\n",
      "Rank 1: Tensor setelah all_reduce = tensor([6.], device='cuda:1'), Seharusnya = 6\n",
      "Rank 2: Tensor setelah all_reduce = tensor([6.], device='cuda:2'), Seharusnya = 6\n",
      "viplab-G481-H81-00:265907:265967 [1] NCCL INFO [Service thread] Connection closed by localRank 1\n",
      "viplab-G481-H81-00:265908:265965 [2] NCCL INFO [Service thread] Connection closed by localRank 2\n",
      "Rank 0: Tensor setelah all_reduce = tensor([6.], device='cuda:0'), Seharusnya = 6\n",
      "viplab-G481-H81-00:265906:265966 [0] NCCL INFO [Service thread] Connection closed by localRank 0\n",
      "viplab-G481-H81-00:265909:265977 [3] NCCL INFO comm 0xcb141c0 rank 3 nranks 4 cudaDev 3 busId db000 - Abort COMPLETE\n",
      "viplab-G481-H81-00:265908:265979 [2] NCCL INFO comm 0xdcf48e0 rank 2 nranks 4 cudaDev 2 busId da000 - Abort COMPLETE\n",
      "viplab-G481-H81-00:265907:265978 [1] NCCL INFO comm 0xd650b30 rank 1 nranks 4 cudaDev 1 busId b2000 - Abort COMPLETE\n",
      "viplab-G481-H81-00:265906:265980 [0] NCCL INFO comm 0x9b63a70 rank 0 nranks 4 cudaDev 0 busId b1000 - Abort COMPLETE\n"
     ]
    }
   ],
   "source": [
    "!NCCL_DEBUG=INFO CUDA_VISIBLE_DEVICES=4,5,6,7 torchrun --nproc_per_node=4 module10/test_nccl.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
