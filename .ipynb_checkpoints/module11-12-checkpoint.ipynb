{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca5ac1f-d3b6-47a8-b454-9b824cd916b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING BACKBONE\n",
      "==================================================\n",
      "Using device: cuda\n",
      "Backbone structure:\n",
      "  patch_embed.proj: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  patch_embed.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.downsample: Identity()\n",
      "  layers_0.blocks.0.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.0.drop_path1: Identity()\n",
      "  layers_0.blocks.0.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.mlp.norm: Identity()\n",
      "  layers_0.blocks.0.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.drop_path2: Identity()\n",
      "  layers_0.blocks.1.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.1.drop_path1: DropPath(drop_prob=0.004)\n",
      "  layers_0.blocks.1.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.mlp.norm: Identity()\n",
      "  layers_0.blocks.1.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.drop_path2: DropPath(drop_prob=0.004)\n",
      "  layers_1.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  layers_1.blocks.0.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.0.drop_path1: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.0.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.mlp.norm: Identity()\n",
      "  layers_1.blocks.0.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.drop_path2: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.1.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.1.drop_path1: DropPath(drop_prob=0.013)\n",
      "  layers_1.blocks.1.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.mlp.norm: Identity()\n",
      "  layers_1.blocks.1.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.drop_path2: DropPath(drop_prob=0.013)\n",
      "  layers_2.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  layers_2.blocks.0.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.0.drop_path1: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.0.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.mlp.norm: Identity()\n",
      "  layers_2.blocks.0.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.drop_path2: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.1.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.1.drop_path1: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.1.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.mlp.norm: Identity()\n",
      "  layers_2.blocks.1.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.drop_path2: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.2.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.2.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.2.drop_path1: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.2.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.2.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.2.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.mlp.norm: Identity()\n",
      "  layers_2.blocks.2.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.drop_path2: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.3.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.3.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.3.drop_path1: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.3.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.3.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.3.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.mlp.norm: Identity()\n",
      "  layers_2.blocks.3.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.drop_path2: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.4.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.4.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.4.drop_path1: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.4.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.4.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.4.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.mlp.norm: Identity()\n",
      "  layers_2.blocks.4.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.drop_path2: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.5.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.5.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.5.drop_path1: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.5.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.5.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.5.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.mlp.norm: Identity()\n",
      "  layers_2.blocks.5.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.drop_path2: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.6.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.6.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.6.drop_path1: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.6.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.6.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.6.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.mlp.norm: Identity()\n",
      "  layers_2.blocks.6.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.drop_path2: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.7.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.7.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.7.drop_path1: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.7.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.7.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.7.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.mlp.norm: Identity()\n",
      "  layers_2.blocks.7.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.drop_path2: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.8.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.8.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.8.drop_path1: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.8.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.8.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.8.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.mlp.norm: Identity()\n",
      "  layers_2.blocks.8.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.drop_path2: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.9.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.9.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.9.drop_path1: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.9.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.9.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.9.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.mlp.norm: Identity()\n",
      "  layers_2.blocks.9.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.drop_path2: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.10.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.10.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.10.drop_path1: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.10.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.10.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.10.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.mlp.norm: Identity()\n",
      "  layers_2.blocks.10.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.drop_path2: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.11.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.11.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.11.drop_path1: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.11.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.11.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.11.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.mlp.norm: Identity()\n",
      "  layers_2.blocks.11.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.drop_path2: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.12.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.12.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.12.drop_path1: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.12.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.12.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.12.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.mlp.norm: Identity()\n",
      "  layers_2.blocks.12.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.drop_path2: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.13.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.13.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.13.drop_path1: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.13.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.13.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.13.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.mlp.norm: Identity()\n",
      "  layers_2.blocks.13.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.drop_path2: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.14.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.14.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.14.drop_path1: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.14.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.14.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.14.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.mlp.norm: Identity()\n",
      "  layers_2.blocks.14.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.drop_path2: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.15.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.15.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.15.drop_path1: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.15.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.15.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.15.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.mlp.norm: Identity()\n",
      "  layers_2.blocks.15.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.drop_path2: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.16.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.16.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.16.drop_path1: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.16.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.16.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.16.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.mlp.norm: Identity()\n",
      "  layers_2.blocks.16.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.drop_path2: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.17.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.17.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.17.drop_path1: DropPath(drop_prob=0.091)\n",
      "  layers_2.blocks.17.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.17.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.17.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.mlp.norm: Identity()\n",
      "  layers_2.blocks.17.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.drop_path2: DropPath(drop_prob=0.091)\n",
      "  layers_3.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  layers_3.blocks.0.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.0.drop_path1: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.0.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.mlp.norm: Identity()\n",
      "  layers_3.blocks.0.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.drop_path2: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.1.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.1.drop_path1: DropPath(drop_prob=0.100)\n",
      "  layers_3.blocks.1.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.mlp.norm: Identity()\n",
      "  layers_3.blocks.1.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.drop_path2: DropPath(drop_prob=0.100)\n",
      "Creating new parameter mapping from GroundingDINO...\n",
      "config.json: 100%|█████████████████████████| 1.74k/1.74k [00:00<00:00, 2.79MB/s]\n",
      "model.safetensors: 100%|██████████████████████| 933M/933M [00:08<00:00, 111MB/s]\n",
      "/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "\n",
      "GroundingDINO Backbone Structure:\n",
      "  embeddings.patch_embeddings.projection: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  embeddings.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  embeddings.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.layernorm_before: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.drop_path: Identity()\n",
      "  encoder.layers.0.blocks.0.layernorm_after: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.0.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  encoder.layers.0.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.0.blocks.0.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.layernorm_before: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.drop_path: SwinDropPath(p=0.004347826354205608)\n",
      "  encoder.layers.0.blocks.1.layernorm_after: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.1.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  encoder.layers.0.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.0.blocks.1.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  encoder.layers.0.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.layernorm_before: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.query: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.key: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.value: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.0.attention.output.dense: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.0.drop_path: SwinDropPath(p=0.008695652708411217)\n",
      "  encoder.layers.1.blocks.0.layernorm_after: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.intermediate.dense: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  encoder.layers.1.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.1.blocks.0.output.dense: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.layernorm_before: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.query: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.key: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.value: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.attention.output.dense: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.drop_path: SwinDropPath(p=0.013043479062616825)\n",
      "  encoder.layers.1.blocks.1.layernorm_after: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.1.intermediate.dense: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  encoder.layers.1.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.1.blocks.1.output.dense: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  encoder.layers.1.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.0.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.0.drop_path: SwinDropPath(p=0.017391305416822433)\n",
      "  encoder.layers.2.blocks.0.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.0.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.drop_path: SwinDropPath(p=0.021739132702350616)\n",
      "  encoder.layers.2.blocks.1.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.1.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.1.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.drop_path: SwinDropPath(p=0.02608695812523365)\n",
      "  encoder.layers.2.blocks.2.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.2.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.2.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.2.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.drop_path: SwinDropPath(p=0.030434783548116684)\n",
      "  encoder.layers.2.blocks.3.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.3.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.3.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.3.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.drop_path: SwinDropPath(p=0.03478261083364487)\n",
      "  encoder.layers.2.blocks.4.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.4.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.4.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.4.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.drop_path: SwinDropPath(p=0.03913043811917305)\n",
      "  encoder.layers.2.blocks.5.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.5.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.5.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.5.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.drop_path: SwinDropPath(p=0.04347826540470123)\n",
      "  encoder.layers.2.blocks.6.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.6.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.6.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.6.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.drop_path: SwinDropPath(p=0.04782608896493912)\n",
      "  encoder.layers.2.blocks.7.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.7.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.7.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.7.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.drop_path: SwinDropPath(p=0.052173912525177)\n",
      "  encoder.layers.2.blocks.8.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.8.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.8.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.8.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.drop_path: SwinDropPath(p=0.056521736085414886)\n",
      "  encoder.layers.2.blocks.9.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.9.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.9.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.9.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.drop_path: SwinDropPath(p=0.06086956337094307)\n",
      "  encoder.layers.2.blocks.10.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.10.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.10.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.10.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.drop_path: SwinDropPath(p=0.06521739065647125)\n",
      "  encoder.layers.2.blocks.11.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.11.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.11.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.11.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.drop_path: SwinDropPath(p=0.06956521421670914)\n",
      "  encoder.layers.2.blocks.12.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.12.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.12.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.12.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.drop_path: SwinDropPath(p=0.07391304522752762)\n",
      "  encoder.layers.2.blocks.13.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.13.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.13.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.13.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.drop_path: SwinDropPath(p=0.0782608687877655)\n",
      "  encoder.layers.2.blocks.14.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.14.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.14.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.14.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.drop_path: SwinDropPath(p=0.08260869979858398)\n",
      "  encoder.layers.2.blocks.15.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.15.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.15.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.15.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.drop_path: SwinDropPath(p=0.08695652335882187)\n",
      "  encoder.layers.2.blocks.16.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.16.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.16.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.16.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.drop_path: SwinDropPath(p=0.09130434691905975)\n",
      "  encoder.layers.2.blocks.17.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.17.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.17.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.17.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  encoder.layers.2.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.layernorm_before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.query: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.key: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.value: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.0.attention.output.dense: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.0.drop_path: SwinDropPath(p=0.09565217792987823)\n",
      "  encoder.layers.3.blocks.0.layernorm_after: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.intermediate.dense: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  encoder.layers.3.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.3.blocks.0.output.dense: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.layernorm_before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.query: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.key: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.value: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.attention.output.dense: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.drop_path: SwinDropPath(p=0.10000000149011612)\n",
      "  encoder.layers.3.blocks.1.layernorm_after: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.1.intermediate.dense: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  encoder.layers.3.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.3.blocks.1.output.dense: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  hidden_states_norms.stage2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  hidden_states_norms.stage3: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  hidden_states_norms.stage4: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "Parameter stats sebelum mapping:\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "\n",
      "Sample TIMM keys:\n",
      "  patch_embed.proj.weight: torch.Size([128, 3, 4, 4])\n",
      "  patch_embed.proj.bias: torch.Size([128])\n",
      "  patch_embed.norm.weight: torch.Size([128])\n",
      "  patch_embed.norm.bias: torch.Size([128])\n",
      "  layers_0.blocks.0.norm1.weight: torch.Size([128])\n",
      "  layers_0.blocks.0.norm1.bias: torch.Size([128])\n",
      "  layers_0.blocks.0.attn.relative_position_bias_table: torch.Size([169, 4])\n",
      "  layers_0.blocks.0.attn.qkv.weight: torch.Size([384, 128])\n",
      "  layers_0.blocks.0.attn.qkv.bias: torch.Size([384])\n",
      "  layers_0.blocks.0.attn.proj.weight: torch.Size([128, 128])\n",
      "\n",
      "Sample GroundingDINO keys:\n",
      "  embeddings.patch_embeddings.projection.weight: torch.Size([128, 3, 4, 4])\n",
      "  embeddings.patch_embeddings.projection.bias: torch.Size([128])\n",
      "  embeddings.norm.weight: torch.Size([128])\n",
      "  embeddings.norm.bias: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.layernorm_before.weight: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.layernorm_before.bias: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.attention.self.relative_position_bias_table: torch.Size([529, 4])\n",
      "  encoder.layers.0.blocks.0.attention.self.relative_position_index: torch.Size([144, 144])\n",
      "  encoder.layers.0.blocks.0.attention.self.query.weight: torch.Size([128, 128])\n",
      "  encoder.layers.0.blocks.0.attention.self.query.bias: torch.Size([128])\n",
      "Mapped 4/325 parameters successfully\n",
      "QKV parameters: 0/15 potential matches found\n",
      "Saved mapped parameters to ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "\n",
      "Parameter stats setelah mapping:\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "Backbone parameters frozen (requires_grad=False)\n",
      "Backbone in EVAL mode\n",
      "Input tensor shape: torch.Size([2, 3, 512, 512])\n",
      "\n",
      "Testing forward_multiscale...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -5.0436, max: 4.9420, mean: 0.0010, std: 1.0011\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4240, max: 2.3352, mean: -0.0044, std: 0.4798\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -5.6362, max: 5.6722, mean: 0.0345, std: 1.2204\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -6.0518, max: 5.0640, mean: 0.0133, std: 1.1655\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "\n",
      "Testing forward_concatenated...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -5.0436, max: 4.9420, mean: 0.0010, std: 1.0011\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4240, max: 2.3352, mean: -0.0044, std: 0.4798\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -5.6362, max: 5.6722, mean: 0.0345, std: 1.2204\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -6.0518, max: 5.0640, mean: 0.0133, std: 1.1655\n",
      "Permuting feature dimensions from BHWC to BCHW\n",
      "Target size after reduction: (64, 64)\n",
      "Concatenated feature shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "\n",
      "==================================================\n",
      "BACKBONE TEST COMPLETE\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "TESTING BACKBONE\n",
      "==================================================\n",
      "Using device: cuda\n",
      "Backbone structure:\n",
      "  patch_embed.proj: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  patch_embed.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.downsample: Identity()\n",
      "  layers_0.blocks.0.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.0.drop_path1: Identity()\n",
      "  layers_0.blocks.0.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.mlp.norm: Identity()\n",
      "  layers_0.blocks.0.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.drop_path2: Identity()\n",
      "  layers_0.blocks.1.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.1.drop_path1: DropPath(drop_prob=0.004)\n",
      "  layers_0.blocks.1.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.mlp.norm: Identity()\n",
      "  layers_0.blocks.1.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.drop_path2: DropPath(drop_prob=0.004)\n",
      "  layers_1.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  layers_1.blocks.0.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.0.drop_path1: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.0.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.mlp.norm: Identity()\n",
      "  layers_1.blocks.0.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.drop_path2: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.1.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.1.drop_path1: DropPath(drop_prob=0.013)\n",
      "  layers_1.blocks.1.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.mlp.norm: Identity()\n",
      "  layers_1.blocks.1.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.drop_path2: DropPath(drop_prob=0.013)\n",
      "  layers_2.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  layers_2.blocks.0.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.0.drop_path1: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.0.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.mlp.norm: Identity()\n",
      "  layers_2.blocks.0.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.drop_path2: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.1.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.1.drop_path1: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.1.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.mlp.norm: Identity()\n",
      "  layers_2.blocks.1.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.drop_path2: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.2.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.2.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.2.drop_path1: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.2.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.2.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.2.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.mlp.norm: Identity()\n",
      "  layers_2.blocks.2.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.drop_path2: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.3.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.3.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.3.drop_path1: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.3.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.3.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.3.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.mlp.norm: Identity()\n",
      "  layers_2.blocks.3.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.drop_path2: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.4.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.4.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.4.drop_path1: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.4.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.4.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.4.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.mlp.norm: Identity()\n",
      "  layers_2.blocks.4.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.drop_path2: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.5.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.5.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.5.drop_path1: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.5.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.5.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.5.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.mlp.norm: Identity()\n",
      "  layers_2.blocks.5.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.drop_path2: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.6.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.6.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.6.drop_path1: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.6.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.6.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.6.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.mlp.norm: Identity()\n",
      "  layers_2.blocks.6.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.drop_path2: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.7.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.7.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.7.drop_path1: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.7.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.7.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.7.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.mlp.norm: Identity()\n",
      "  layers_2.blocks.7.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.drop_path2: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.8.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.8.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.8.drop_path1: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.8.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.8.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.8.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.mlp.norm: Identity()\n",
      "  layers_2.blocks.8.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.drop_path2: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.9.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.9.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.9.drop_path1: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.9.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.9.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.9.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.mlp.norm: Identity()\n",
      "  layers_2.blocks.9.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.drop_path2: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.10.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.10.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.10.drop_path1: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.10.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.10.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.10.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.mlp.norm: Identity()\n",
      "  layers_2.blocks.10.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.drop_path2: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.11.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.11.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.11.drop_path1: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.11.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.11.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.11.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.mlp.norm: Identity()\n",
      "  layers_2.blocks.11.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.drop_path2: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.12.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.12.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.12.drop_path1: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.12.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.12.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.12.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.mlp.norm: Identity()\n",
      "  layers_2.blocks.12.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.drop_path2: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.13.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.13.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.13.drop_path1: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.13.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.13.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.13.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.mlp.norm: Identity()\n",
      "  layers_2.blocks.13.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.drop_path2: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.14.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.14.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.14.drop_path1: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.14.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.14.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.14.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.mlp.norm: Identity()\n",
      "  layers_2.blocks.14.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.drop_path2: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.15.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.15.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.15.drop_path1: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.15.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.15.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.15.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.mlp.norm: Identity()\n",
      "  layers_2.blocks.15.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.drop_path2: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.16.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.16.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.16.drop_path1: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.16.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.16.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.16.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.mlp.norm: Identity()\n",
      "  layers_2.blocks.16.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.drop_path2: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.17.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.17.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.17.drop_path1: DropPath(drop_prob=0.091)\n",
      "  layers_2.blocks.17.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.17.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.17.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.mlp.norm: Identity()\n",
      "  layers_2.blocks.17.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.drop_path2: DropPath(drop_prob=0.091)\n",
      "  layers_3.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  layers_3.blocks.0.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.0.drop_path1: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.0.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.mlp.norm: Identity()\n",
      "  layers_3.blocks.0.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.drop_path2: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.1.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.1.drop_path1: DropPath(drop_prob=0.100)\n",
      "  layers_3.blocks.1.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.mlp.norm: Identity()\n",
      "  layers_3.blocks.1.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.drop_path2: DropPath(drop_prob=0.100)\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "Backbone parameters set to trainable (requires_grad=True)\n",
      "Backbone in TRAIN mode\n",
      "Input tensor shape: torch.Size([2, 3, 512, 512])\n",
      "\n",
      "Testing forward_multiscale...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -4.7221, max: 5.1338, mean: -0.0016, std: 1.0001\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4599, max: 2.5013, mean: -0.0051, std: 0.4809\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -6.0789, max: 5.6200, mean: 0.0361, std: 1.2340\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -6.2115, max: 5.5309, mean: 0.0098, std: 1.0917\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "\n",
      "Testing forward_concatenated...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -4.7221, max: 5.1338, mean: -0.0016, std: 1.0001\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4599, max: 2.5013, mean: -0.0051, std: 0.4809\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -6.8098, max: 5.6830, mean: 0.0376, std: 1.2516\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -7.9927, max: 6.2326, mean: 0.0210, std: 1.1489\n",
      "Permuting feature dimensions from BHWC to BCHW\n",
      "Target size after reduction: (64, 64)\n",
      "Concatenated feature shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "\n",
      "Testing backward pass...\n",
      "Dummy loss: 0.021941233426332474\n",
      "Top 5 gradients by norm:\n",
      "  backbone.layers_3.downsample.reduction.weight: 0.426721\n",
      "  backbone.layers_2.downsample.reduction.weight: 0.291985\n",
      "  backbone.layers_1.downsample.reduction.weight: 0.251706\n",
      "  backbone.layers_3.blocks.1.mlp.fc2.weight: 0.251678\n",
      "  backbone.layers_2.blocks.0.mlp.fc2.weight: 0.193036\n",
      "Optimizer step completed\n",
      "\n",
      "==================================================\n",
      "BACKBONE TEST COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!python module11/debug_backbone.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2807378f-4ac3-41f5-a7ef-fe939c251bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n"
     ]
    }
   ],
   "source": [
    "!python module11/backbone.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4a8c6-c85e-47c3-9dfc-be6de51e3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python module11/test_debuge.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581ae1d-0472-400e-8783-e7583a955ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765387d9-4965-4694-9f07-34b6b582298a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0319 08:45:02.714000 3931795 site-packages/torch/distributed/run.py:793] \n",
      "W0319 08:45:02.714000 3931795 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0319 08:45:02.714000 3931795 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0319 08:45:02.714000 3931795 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "1\n",
      "0\n",
      "Epoch: 1 Train loss: 24.334 Aux train loss: 11.608 Val loss: 6.931 Aux val loss: 4.191 Train MAE: 106.167 Val MAE: 59.544 Epoch time: 196.604 seconds best\n",
      "Epoch: 2 Train loss: 17.531 Aux train loss: 10.581 Val loss: 6.830 Aux val loss: 4.096 Train MAE: 67.637 Val MAE: 47.228 Epoch time: 198.214 seconds best\n",
      "Epoch: 3 Train loss: 16.988 Aux train loss: 10.208 Val loss: 7.071 Aux val loss: 4.210 Train MAE: 66.847 Val MAE: 63.252 Epoch time: 200.054 seconds \n",
      "Epoch: 4 Train loss: 17.422 Aux train loss: 10.450 Val loss: 6.706 Aux val loss: 4.027 Train MAE: 62.983 Val MAE: 47.326 Epoch time: 200.964 seconds \n",
      "Epoch: 5 Train loss: 17.647 Aux train loss: 10.549 Val loss: 6.803 Aux val loss: 4.089 Train MAE: 63.155 Val MAE: 53.198 Epoch time: 200.626 seconds \n",
      "Epoch: 6 Train loss: 17.098 Aux train loss: 10.282 Val loss: 6.698 Aux val loss: 4.028 Train MAE: 59.646 Val MAE: 47.485 Epoch time: 200.662 seconds \n",
      "Epoch: 7 Train loss: 16.880 Aux train loss: 10.052 Val loss: 6.697 Aux val loss: 4.018 Train MAE: 62.652 Val MAE: 46.162 Epoch time: 199.800 seconds best\n",
      "Epoch: 8 Train loss: 16.398 Aux train loss: 9.793 Val loss: 6.744 Aux val loss: 3.968 Train MAE: 56.856 Val MAE: 48.598 Epoch time: 200.856 seconds \n",
      "Epoch: 9 Train loss: 16.967 Aux train loss: 10.116 Val loss: 6.691 Aux val loss: 4.024 Train MAE: 58.786 Val MAE: 59.173 Epoch time: 200.647 seconds \n",
      "Epoch: 10 Train loss: 16.643 Aux train loss: 9.934 Val loss: 6.573 Aux val loss: 3.944 Train MAE: 54.683 Val MAE: 47.166 Epoch time: 200.627 seconds \n",
      "Epoch: 11 Train loss: 16.270 Aux train loss: 9.738 Val loss: 6.570 Aux val loss: 3.912 Train MAE: 54.801 Val MAE: 41.713 Epoch time: 201.204 seconds best\n",
      "Epoch: 12 Train loss: 16.682 Aux train loss: 9.990 Val loss: 6.424 Aux val loss: 3.837 Train MAE: 54.337 Val MAE: 44.248 Epoch time: 200.590 seconds \n",
      "Epoch: 13 Train loss: 15.825 Aux train loss: 9.483 Val loss: 6.387 Aux val loss: 3.843 Train MAE: 50.311 Val MAE: 35.571 Epoch time: 201.078 seconds best\n",
      "Epoch: 14 Train loss: 16.557 Aux train loss: 9.970 Val loss: 6.441 Aux val loss: 3.866 Train MAE: 50.874 Val MAE: 44.334 Epoch time: 200.718 seconds \n",
      "Epoch: 15 Train loss: 15.426 Aux train loss: 9.246 Val loss: 6.380 Aux val loss: 3.855 Train MAE: 48.942 Val MAE: 36.915 Epoch time: 201.033 seconds \n",
      "Epoch: 16 Train loss: 15.835 Aux train loss: 9.489 Val loss: 6.343 Aux val loss: 3.786 Train MAE: 49.715 Val MAE: 36.931 Epoch time: 200.573 seconds \n",
      "Epoch: 17 Train loss: 15.992 Aux train loss: 9.589 Val loss: 6.643 Aux val loss: 3.915 Train MAE: 50.933 Val MAE: 63.641 Epoch time: 201.127 seconds \n",
      "Epoch: 18 Train loss: 16.290 Aux train loss: 9.749 Val loss: 6.253 Aux val loss: 3.763 Train MAE: 47.304 Val MAE: 35.875 Epoch time: 201.066 seconds \n",
      "Epoch: 19 Train loss: 14.808 Aux train loss: 8.880 Val loss: 6.261 Aux val loss: 3.810 Train MAE: 45.392 Val MAE: 45.408 Epoch time: 200.526 seconds \n",
      "Epoch: 20 Train loss: 16.104 Aux train loss: 9.723 Val loss: 6.240 Aux val loss: 3.731 Train MAE: 47.862 Val MAE: 36.427 Epoch time: 200.333 seconds \n",
      "Epoch: 21 Train loss: 15.200 Aux train loss: 9.114 Val loss: 6.226 Aux val loss: 3.712 Train MAE: 46.154 Val MAE: 35.439 Epoch time: 200.309 seconds best\n",
      "Epoch: 22 Train loss: 14.801 Aux train loss: 8.872 Val loss: 6.336 Aux val loss: 3.708 Train MAE: 42.813 Val MAE: 50.749 Epoch time: 199.602 seconds \n",
      "Epoch: 23 Train loss: 15.417 Aux train loss: 9.278 Val loss: 6.720 Aux val loss: 4.071 Train MAE: 48.123 Val MAE: 66.030 Epoch time: 200.144 seconds \n",
      "Epoch: 24 Train loss: 15.389 Aux train loss: 9.246 Val loss: 6.167 Aux val loss: 3.716 Train MAE: 42.072 Val MAE: 33.036 Epoch time: 198.748 seconds best\n",
      "Epoch: 25 Train loss: 14.847 Aux train loss: 8.919 Val loss: 6.156 Aux val loss: 3.684 Train MAE: 43.735 Val MAE: 33.825 Epoch time: 200.555 seconds \n",
      "Epoch: 26 Train loss: 14.926 Aux train loss: 8.985 Val loss: 6.469 Aux val loss: 3.920 Train MAE: 43.080 Val MAE: 55.130 Epoch time: 199.889 seconds \n",
      "Epoch: 27 Train loss: 15.633 Aux train loss: 9.633 Val loss: 6.067 Aux val loss: 3.624 Train MAE: 42.165 Val MAE: 34.037 Epoch time: 199.252 seconds \n",
      "Epoch: 28 Train loss: 14.900 Aux train loss: 8.981 Val loss: 6.172 Aux val loss: 3.703 Train MAE: 42.472 Val MAE: 37.402 Epoch time: 199.805 seconds \n",
      "Epoch: 29 Train loss: 15.002 Aux train loss: 9.036 Val loss: 6.071 Aux val loss: 3.673 Train MAE: 40.729 Val MAE: 34.128 Epoch time: 199.395 seconds \n",
      "Epoch: 30 Train loss: 13.633 Aux train loss: 8.214 Val loss: 5.999 Aux val loss: 3.600 Train MAE: 37.266 Val MAE: 30.851 Epoch time: 199.424 seconds best\n",
      "Epoch: 31 Train loss: 15.005 Aux train loss: 8.852 Val loss: 5.981 Aux val loss: 3.606 Train MAE: 42.086 Val MAE: 32.327 Epoch time: 199.171 seconds \n",
      "Epoch: 32 Train loss: 14.615 Aux train loss: 9.004 Val loss: 6.003 Aux val loss: 3.635 Train MAE: 43.087 Val MAE: 34.450 Epoch time: 198.997 seconds \n",
      "Epoch: 33 Train loss: 14.880 Aux train loss: 9.013 Val loss: 5.921 Aux val loss: 3.558 Train MAE: 41.712 Val MAE: 30.206 Epoch time: 199.534 seconds best\n",
      "Epoch: 34 Train loss: 14.049 Aux train loss: 8.491 Val loss: 5.960 Aux val loss: 3.600 Train MAE: 40.618 Val MAE: 33.383 Epoch time: 198.397 seconds \n",
      "Epoch: 35 Train loss: 15.223 Aux train loss: 9.228 Val loss: 6.017 Aux val loss: 3.609 Train MAE: 43.164 Val MAE: 34.843 Epoch time: 199.209 seconds \n",
      "Epoch: 36 Train loss: 14.532 Aux train loss: 8.781 Val loss: 5.870 Aux val loss: 3.519 Train MAE: 40.332 Val MAE: 38.802 Epoch time: 199.714 seconds \n",
      "Epoch: 37 Train loss: 14.349 Aux train loss: 8.690 Val loss: 6.016 Aux val loss: 3.668 Train MAE: 39.154 Val MAE: 52.733 Epoch time: 199.703 seconds \n",
      "Epoch: 38 Train loss: 14.702 Aux train loss: 8.832 Val loss: 5.958 Aux val loss: 3.626 Train MAE: 40.649 Val MAE: 44.623 Epoch time: 199.581 seconds \n",
      "Epoch: 39 Train loss: 14.975 Aux train loss: 9.098 Val loss: 5.791 Aux val loss: 3.511 Train MAE: 39.171 Val MAE: 31.928 Epoch time: 199.648 seconds \n",
      "Epoch: 40 Train loss: 14.195 Aux train loss: 8.609 Val loss: 5.902 Aux val loss: 3.552 Train MAE: 41.015 Val MAE: 41.811 Epoch time: 200.442 seconds \n",
      "Epoch: 41 Train loss: 14.132 Aux train loss: 8.641 Val loss: 5.762 Aux val loss: 3.503 Train MAE: 40.053 Val MAE: 31.427 Epoch time: 200.163 seconds \n",
      "Epoch: 42 Train loss: 14.761 Aux train loss: 8.968 Val loss: 6.029 Aux val loss: 3.628 Train MAE: 40.482 Val MAE: 52.071 Epoch time: 199.795 seconds \n",
      "Epoch: 43 Train loss: 14.131 Aux train loss: 8.444 Val loss: 5.661 Aux val loss: 3.449 Train MAE: 37.519 Val MAE: 30.150 Epoch time: 199.409 seconds best\n",
      "Epoch: 44 Train loss: 14.128 Aux train loss: 8.526 Val loss: 5.758 Aux val loss: 3.503 Train MAE: 38.680 Val MAE: 32.110 Epoch time: 199.470 seconds \n",
      "Epoch: 45 Train loss: 13.922 Aux train loss: 8.506 Val loss: 5.630 Aux val loss: 3.463 Train MAE: 39.207 Val MAE: 30.654 Epoch time: 199.506 seconds \n",
      "Epoch: 46 Train loss: 13.000 Aux train loss: 7.961 Val loss: 5.664 Aux val loss: 3.499 Train MAE: 38.757 Val MAE: 32.389 Epoch time: 198.856 seconds \n",
      "Epoch: 47 Train loss: 13.892 Aux train loss: 8.466 Val loss: 5.625 Aux val loss: 3.410 Train MAE: 34.731 Val MAE: 30.069 Epoch time: 199.135 seconds best\n",
      "Epoch: 48 Train loss: 14.127 Aux train loss: 8.545 Val loss: 5.725 Aux val loss: 3.436 Train MAE: 36.388 Val MAE: 27.197 Epoch time: 199.655 seconds best\n",
      "Epoch: 49 Train loss: 13.101 Aux train loss: 7.984 Val loss: 5.686 Aux val loss: 3.513 Train MAE: 33.958 Val MAE: 41.456 Epoch time: 199.370 seconds \n",
      "Epoch: 50 Train loss: 13.259 Aux train loss: 8.083 Val loss: 5.583 Aux val loss: 3.346 Train MAE: 35.465 Val MAE: 31.442 Epoch time: 199.072 seconds \n",
      "Epoch: 51 Train loss: 13.333 Aux train loss: 8.080 Val loss: 5.484 Aux val loss: 3.340 Train MAE: 34.315 Val MAE: 30.044 Epoch time: 199.104 seconds \n",
      "Epoch: 52 Train loss: 14.041 Aux train loss: 8.609 Val loss: 5.625 Aux val loss: 3.408 Train MAE: 37.315 Val MAE: 37.093 Epoch time: 199.705 seconds \n",
      "Epoch: 53 Train loss: 13.603 Aux train loss: 8.316 Val loss: 5.433 Aux val loss: 3.299 Train MAE: 35.889 Val MAE: 31.307 Epoch time: 199.066 seconds \n",
      "Epoch: 54 Train loss: 12.878 Aux train loss: 7.816 Val loss: 5.547 Aux val loss: 3.370 Train MAE: 37.442 Val MAE: 30.982 Epoch time: 199.209 seconds \n",
      "Epoch: 55 Train loss: 13.513 Aux train loss: 8.247 Val loss: 5.551 Aux val loss: 3.422 Train MAE: 36.157 Val MAE: 32.228 Epoch time: 199.793 seconds \n",
      "Epoch: 56 Train loss: 13.221 Aux train loss: 8.055 Val loss: 5.542 Aux val loss: 3.371 Train MAE: 34.957 Val MAE: 31.717 Epoch time: 198.843 seconds \n",
      "Epoch: 57 Train loss: 12.896 Aux train loss: 7.869 Val loss: 5.465 Aux val loss: 3.358 Train MAE: 33.108 Val MAE: 31.204 Epoch time: 199.181 seconds \n",
      "Epoch: 58 Train loss: 13.695 Aux train loss: 8.369 Val loss: 5.597 Aux val loss: 3.378 Train MAE: 36.451 Val MAE: 38.962 Epoch time: 199.189 seconds \n",
      "Epoch: 59 Train loss: 13.275 Aux train loss: 8.075 Val loss: 5.492 Aux val loss: 3.379 Train MAE: 37.219 Val MAE: 30.514 Epoch time: 199.559 seconds \n",
      "Epoch: 60 Train loss: 13.297 Aux train loss: 8.089 Val loss: 5.485 Aux val loss: 3.320 Train MAE: 33.714 Val MAE: 28.741 Epoch time: 199.095 seconds \n",
      "Epoch: 61 Train loss: 12.459 Aux train loss: 7.600 Val loss: 5.536 Aux val loss: 3.362 Train MAE: 33.484 Val MAE: 31.737 Epoch time: 199.192 seconds \n",
      "Epoch: 62 Train loss: 12.261 Aux train loss: 7.453 Val loss: 5.460 Aux val loss: 3.309 Train MAE: 34.577 Val MAE: 27.300 Epoch time: 199.626 seconds \n",
      "Epoch: 63 Train loss: 13.559 Aux train loss: 8.370 Val loss: 5.458 Aux val loss: 3.318 Train MAE: 37.573 Val MAE: 37.480 Epoch time: 198.933 seconds \n",
      "Epoch: 64 Train loss: 12.816 Aux train loss: 7.859 Val loss: 5.581 Aux val loss: 3.397 Train MAE: 33.803 Val MAE: 29.449 Epoch time: 200.390 seconds \n",
      "Epoch: 65 Train loss: 13.032 Aux train loss: 7.947 Val loss: 5.456 Aux val loss: 3.369 Train MAE: 33.689 Val MAE: 27.350 Epoch time: 199.193 seconds \n",
      "Epoch: 66 Train loss: 13.284 Aux train loss: 8.110 Val loss: 5.451 Aux val loss: 3.312 Train MAE: 35.199 Val MAE: 27.947 Epoch time: 198.637 seconds \n",
      "Epoch: 67 Train loss: 12.879 Aux train loss: 7.868 Val loss: 5.421 Aux val loss: 3.276 Train MAE: 36.238 Val MAE: 32.061 Epoch time: 198.730 seconds \n",
      "Epoch: 68 Train loss: 12.810 Aux train loss: 7.754 Val loss: 5.506 Aux val loss: 3.333 Train MAE: 36.598 Val MAE: 41.440 Epoch time: 199.521 seconds \n",
      "Epoch: 69 Train loss: 13.254 Aux train loss: 8.057 Val loss: 5.344 Aux val loss: 3.270 Train MAE: 33.175 Val MAE: 29.809 Epoch time: 197.974 seconds \n",
      "Epoch: 70 Train loss: 12.642 Aux train loss: 7.707 Val loss: 5.422 Aux val loss: 3.265 Train MAE: 32.988 Val MAE: 41.623 Epoch time: 199.804 seconds \n",
      "Epoch: 71 Train loss: 12.397 Aux train loss: 7.643 Val loss: 5.331 Aux val loss: 3.252 Train MAE: 32.604 Val MAE: 27.497 Epoch time: 199.745 seconds \n",
      "Epoch: 72 Train loss: 12.470 Aux train loss: 7.636 Val loss: 5.308 Aux val loss: 3.222 Train MAE: 32.275 Val MAE: 31.145 Epoch time: 200.856 seconds \n",
      "Epoch: 73 Train loss: 12.864 Aux train loss: 7.857 Val loss: 5.327 Aux val loss: 3.264 Train MAE: 34.380 Val MAE: 27.918 Epoch time: 201.379 seconds \n",
      "Epoch: 74 Train loss: 12.908 Aux train loss: 7.902 Val loss: 5.587 Aux val loss: 3.430 Train MAE: 31.679 Val MAE: 44.479 Epoch time: 201.799 seconds \n",
      "Epoch: 75 Train loss: 12.497 Aux train loss: 7.672 Val loss: 5.282 Aux val loss: 3.210 Train MAE: 33.969 Val MAE: 29.999 Epoch time: 201.356 seconds \n",
      "Epoch: 76 Train loss: 11.991 Aux train loss: 7.332 Val loss: 5.307 Aux val loss: 3.237 Train MAE: 30.805 Val MAE: 36.012 Epoch time: 202.429 seconds \n",
      "Epoch: 77 Train loss: 12.349 Aux train loss: 7.614 Val loss: 5.320 Aux val loss: 3.254 Train MAE: 31.567 Val MAE: 28.342 Epoch time: 202.379 seconds \n",
      "Epoch: 78 Train loss: 11.715 Aux train loss: 7.168 Val loss: 5.203 Aux val loss: 3.224 Train MAE: 30.786 Val MAE: 29.122 Epoch time: 202.738 seconds \n",
      "Epoch: 79 Train loss: 13.131 Aux train loss: 8.012 Val loss: 5.338 Aux val loss: 3.255 Train MAE: 36.633 Val MAE: 27.614 Epoch time: 201.660 seconds \n",
      "Epoch: 80 Train loss: 11.982 Aux train loss: 7.326 Val loss: 5.381 Aux val loss: 3.263 Train MAE: 31.965 Val MAE: 29.147 Epoch time: 201.253 seconds \n",
      "Epoch: 81 Train loss: 13.404 Aux train loss: 8.179 Val loss: 5.378 Aux val loss: 3.245 Train MAE: 32.167 Val MAE: 40.842 Epoch time: 201.260 seconds \n",
      "Epoch: 82 Train loss: 12.014 Aux train loss: 7.393 Val loss: 5.344 Aux val loss: 3.277 Train MAE: 31.111 Val MAE: 31.587 Epoch time: 201.961 seconds \n",
      "Epoch: 83 Train loss: 12.026 Aux train loss: 7.398 Val loss: 5.490 Aux val loss: 3.377 Train MAE: 30.914 Val MAE: 38.215 Epoch time: 200.565 seconds \n",
      "Epoch: 84 Train loss: 12.121 Aux train loss: 7.410 Val loss: 5.194 Aux val loss: 3.192 Train MAE: 31.582 Val MAE: 29.989 Epoch time: 201.177 seconds \n",
      "Epoch: 85 Train loss: 12.245 Aux train loss: 7.478 Val loss: 5.198 Aux val loss: 3.192 Train MAE: 29.497 Val MAE: 29.004 Epoch time: 201.138 seconds \n",
      "Epoch: 86 Train loss: 11.987 Aux train loss: 7.388 Val loss: 5.234 Aux val loss: 3.217 Train MAE: 29.903 Val MAE: 24.771 Epoch time: 200.559 seconds best\n",
      "Epoch: 87 Train loss: 11.998 Aux train loss: 7.370 Val loss: 5.316 Aux val loss: 3.199 Train MAE: 29.938 Val MAE: 32.665 Epoch time: 201.257 seconds \n",
      "Epoch: 88 Train loss: 12.541 Aux train loss: 7.659 Val loss: 5.179 Aux val loss: 3.151 Train MAE: 30.621 Val MAE: 31.182 Epoch time: 201.939 seconds \n",
      "Epoch: 89 Train loss: 12.043 Aux train loss: 7.365 Val loss: 5.215 Aux val loss: 3.197 Train MAE: 30.718 Val MAE: 27.406 Epoch time: 201.076 seconds \n",
      "Epoch: 90 Train loss: 12.358 Aux train loss: 7.527 Val loss: 5.187 Aux val loss: 3.156 Train MAE: 30.326 Val MAE: 32.712 Epoch time: 201.123 seconds \n",
      "Epoch: 91 Train loss: 12.257 Aux train loss: 7.417 Val loss: 5.300 Aux val loss: 3.174 Train MAE: 30.023 Val MAE: 36.334 Epoch time: 200.560 seconds \n",
      "Epoch: 92 Train loss: 12.674 Aux train loss: 7.809 Val loss: 5.263 Aux val loss: 3.205 Train MAE: 31.521 Val MAE: 28.993 Epoch time: 200.316 seconds \n",
      "Epoch: 93 Train loss: 11.920 Aux train loss: 7.312 Val loss: 5.165 Aux val loss: 3.187 Train MAE: 29.917 Val MAE: 23.644 Epoch time: 201.480 seconds best\n",
      "Epoch: 94 Train loss: 12.290 Aux train loss: 297.628 Val loss: 5.140 Aux val loss: 3.140 Train MAE: 30.884 Val MAE: 25.226 Epoch time: 202.145 seconds \n",
      "Epoch: 95 Train loss: 12.123 Aux train loss: 7.418 Val loss: 5.329 Aux val loss: 3.221 Train MAE: 29.251 Val MAE: 36.969 Epoch time: 201.330 seconds \n",
      "Epoch: 96 Train loss: 12.666 Aux train loss: 7.804 Val loss: 5.162 Aux val loss: 3.182 Train MAE: 30.064 Val MAE: 28.959 Epoch time: 201.227 seconds \n",
      "Epoch: 97 Train loss: 11.583 Aux train loss: 7.108 Val loss: 5.147 Aux val loss: 3.122 Train MAE: 27.222 Val MAE: 25.963 Epoch time: 200.961 seconds \n",
      "Epoch: 98 Train loss: 12.254 Aux train loss: 7.509 Val loss: 5.109 Aux val loss: 3.137 Train MAE: 28.068 Val MAE: 24.986 Epoch time: 202.839 seconds \n",
      "Epoch: 99 Train loss: 11.984 Aux train loss: 7.360 Val loss: 5.118 Aux val loss: 3.136 Train MAE: 29.370 Val MAE: 25.146 Epoch time: 200.829 seconds \n",
      "Epoch: 100 Train loss: 12.192 Aux train loss: 7.599 Val loss: 5.372 Aux val loss: 3.282 Train MAE: 28.696 Val MAE: 30.728 Epoch time: 201.575 seconds \n",
      "Epoch: 101 Train loss: 12.500 Aux train loss: 7.300 Val loss: 5.318 Aux val loss: 3.236 Train MAE: 31.290 Val MAE: 34.864 Epoch time: 201.290 seconds \n",
      "Epoch: 102 Train loss: 12.220 Aux train loss: 7.481 Val loss: 5.112 Aux val loss: 3.120 Train MAE: 30.354 Val MAE: 24.128 Epoch time: 200.790 seconds \n",
      "Epoch: 103 Train loss: 12.016 Aux train loss: 7.424 Val loss: 5.155 Aux val loss: 3.152 Train MAE: 30.575 Val MAE: 29.525 Epoch time: 202.036 seconds \n",
      "Epoch: 104 Train loss: 11.532 Aux train loss: 7.084 Val loss: 5.071 Aux val loss: 3.092 Train MAE: 25.372 Val MAE: 23.844 Epoch time: 202.420 seconds \n",
      "Epoch: 105 Train loss: 11.278 Aux train loss: 6.919 Val loss: 5.247 Aux val loss: 3.284 Train MAE: 27.254 Val MAE: 39.698 Epoch time: 203.084 seconds \n",
      "Epoch: 106 Train loss: 11.874 Aux train loss: 7.289 Val loss: 5.136 Aux val loss: 3.167 Train MAE: 28.611 Val MAE: 28.505 Epoch time: 201.737 seconds \n",
      "Epoch: 107 Train loss: 10.995 Aux train loss: 6.761 Val loss: 5.095 Aux val loss: 3.103 Train MAE: 27.823 Val MAE: 24.756 Epoch time: 201.195 seconds \n",
      "Epoch: 108 Train loss: 11.399 Aux train loss: 7.024 Val loss: 5.104 Aux val loss: 3.118 Train MAE: 29.045 Val MAE: 23.726 Epoch time: 201.741 seconds \n",
      "Epoch: 109 Train loss: 11.262 Aux train loss: 6.875 Val loss: 5.255 Aux val loss: 3.157 Train MAE: 27.484 Val MAE: 32.181 Epoch time: 202.324 seconds \n",
      "Epoch: 110 Train loss: 11.520 Aux train loss: 7.062 Val loss: 5.042 Aux val loss: 3.090 Train MAE: 28.608 Val MAE: 27.852 Epoch time: 202.749 seconds \n",
      "Epoch: 111 Train loss: 11.621 Aux train loss: 7.154 Val loss: 5.094 Aux val loss: 3.168 Train MAE: 29.587 Val MAE: 30.100 Epoch time: 201.541 seconds \n",
      "Epoch: 112 Train loss: 11.403 Aux train loss: 6.951 Val loss: 5.111 Aux val loss: 3.134 Train MAE: 27.087 Val MAE: 24.310 Epoch time: 200.527 seconds \n",
      "Epoch: 113 Train loss: 11.846 Aux train loss: 7.264 Val loss: 5.010 Aux val loss: 3.075 Train MAE: 28.736 Val MAE: 30.730 Epoch time: 201.620 seconds \n",
      "Epoch: 114 Train loss: 12.123 Aux train loss: 7.324 Val loss: 5.204 Aux val loss: 3.141 Train MAE: 28.839 Val MAE: 36.926 Epoch time: 201.633 seconds \n",
      "Epoch: 115 Train loss: 11.168 Aux train loss: 6.875 Val loss: 4.989 Aux val loss: 3.076 Train MAE: 27.007 Val MAE: 26.199 Epoch time: 199.989 seconds \n",
      "Epoch: 116 Train loss: 11.164 Aux train loss: 6.724 Val loss: 5.169 Aux val loss: 3.102 Train MAE: 27.530 Val MAE: 40.251 Epoch time: 200.397 seconds \n",
      "Epoch: 117 Train loss: 11.945 Aux train loss: 7.346 Val loss: 5.091 Aux val loss: 3.117 Train MAE: 29.134 Val MAE: 27.152 Epoch time: 200.578 seconds \n",
      "Epoch: 118 Train loss: 11.456 Aux train loss: 7.061 Val loss: 5.068 Aux val loss: 3.122 Train MAE: 27.402 Val MAE: 29.856 Epoch time: 200.479 seconds \n",
      "Epoch: 119 Train loss: 11.083 Aux train loss: 6.912 Val loss: 5.020 Aux val loss: 3.084 Train MAE: 27.066 Val MAE: 26.911 Epoch time: 200.901 seconds \n",
      "Epoch: 120 Train loss: 12.266 Aux train loss: 7.541 Val loss: 5.103 Aux val loss: 3.083 Train MAE: 28.694 Val MAE: 33.459 Epoch time: 200.562 seconds \n",
      "Epoch: 121 Train loss: 11.439 Aux train loss: 7.046 Val loss: 5.109 Aux val loss: 3.149 Train MAE: 26.797 Val MAE: 31.881 Epoch time: 200.486 seconds \n",
      "Epoch: 122 Train loss: 11.812 Aux train loss: 7.287 Val loss: 5.178 Aux val loss: 3.138 Train MAE: 27.716 Val MAE: 35.025 Epoch time: 199.938 seconds \n",
      "Epoch: 123 Train loss: 11.730 Aux train loss: 7.251 Val loss: 5.124 Aux val loss: 3.159 Train MAE: 26.922 Val MAE: 36.538 Epoch time: 205.075 seconds \n",
      "Epoch: 124 Train loss: 11.777 Aux train loss: 7.273 Val loss: 4.975 Aux val loss: 3.032 Train MAE: 28.652 Val MAE: 24.925 Epoch time: 213.592 seconds \n",
      "Epoch: 125 Train loss: 11.976 Aux train loss: 7.365 Val loss: 5.114 Aux val loss: 3.195 Train MAE: 28.534 Val MAE: 33.049 Epoch time: 213.228 seconds \n",
      "Epoch: 126 Train loss: 11.252 Aux train loss: 6.994 Val loss: 5.027 Aux val loss: 3.100 Train MAE: 29.092 Val MAE: 27.008 Epoch time: 212.203 seconds \n",
      "Epoch: 127 Train loss: 10.934 Aux train loss: 6.716 Val loss: 5.112 Aux val loss: 3.136 Train MAE: 28.460 Val MAE: 33.025 Epoch time: 209.378 seconds \n",
      "Epoch: 128 Train loss: 11.251 Aux train loss: 6.954 Val loss: 4.900 Aux val loss: 3.059 Train MAE: 26.527 Val MAE: 23.527 Epoch time: 212.007 seconds best\n",
      "Epoch: 129 Train loss: 10.966 Aux train loss: 6.771 Val loss: 5.057 Aux val loss: 3.102 Train MAE: 28.487 Val MAE: 28.302 Epoch time: 210.788 seconds \n",
      "Epoch: 130 Train loss: 12.513 Aux train loss: 7.580 Val loss: 5.087 Aux val loss: 3.167 Train MAE: 28.117 Val MAE: 29.807 Epoch time: 210.481 seconds \n",
      "Epoch: 131 Train loss: 12.661 Aux train loss: 6.934 Val loss: 5.066 Aux val loss: 3.074 Train MAE: 25.736 Val MAE: 23.888 Epoch time: 208.180 seconds \n",
      "Epoch: 132 Train loss: 11.651 Aux train loss: 7.121 Val loss: 4.999 Aux val loss: 3.080 Train MAE: 27.347 Val MAE: 27.667 Epoch time: 208.899 seconds \n",
      "Epoch: 133 Train loss: 11.533 Aux train loss: 7.119 Val loss: 4.989 Aux val loss: 3.049 Train MAE: 26.356 Val MAE: 22.628 Epoch time: 205.128 seconds best\n",
      "Epoch: 134 Train loss: 11.803 Aux train loss: 7.306 Val loss: 5.162 Aux val loss: 3.046 Train MAE: 28.631 Val MAE: 44.421 Epoch time: 208.688 seconds \n",
      "Epoch: 135 Train loss: 11.395 Aux train loss: 7.009 Val loss: 4.933 Aux val loss: 3.019 Train MAE: 27.458 Val MAE: 26.592 Epoch time: 210.315 seconds \n",
      "Epoch: 136 Train loss: 10.840 Aux train loss: 6.674 Val loss: 4.896 Aux val loss: 3.021 Train MAE: 26.014 Val MAE: 23.230 Epoch time: 211.394 seconds \n",
      "Epoch: 137 Train loss: 10.702 Aux train loss: 7.208 Val loss: 4.906 Aux val loss: 3.014 Train MAE: 25.305 Val MAE: 23.187 Epoch time: 207.015 seconds \n",
      "Epoch: 138 Train loss: 10.952 Aux train loss: 6.758 Val loss: 4.917 Aux val loss: 2.996 Train MAE: 25.259 Val MAE: 23.753 Epoch time: 208.372 seconds \n",
      "Epoch: 139 Train loss: 10.740 Aux train loss: 6.630 Val loss: 4.991 Aux val loss: 3.040 Train MAE: 23.924 Val MAE: 31.194 Epoch time: 208.685 seconds \n",
      "Epoch: 140 Train loss: 11.556 Aux train loss: 6.752 Val loss: 4.944 Aux val loss: 3.053 Train MAE: 27.287 Val MAE: 23.648 Epoch time: 205.911 seconds \n",
      "Epoch: 141 Train loss: 11.326 Aux train loss: 6.958 Val loss: 5.011 Aux val loss: 3.049 Train MAE: 26.692 Val MAE: 26.589 Epoch time: 207.601 seconds \n",
      "Epoch: 142 Train loss: 10.781 Aux train loss: 6.668 Val loss: 4.901 Aux val loss: 3.025 Train MAE: 25.452 Val MAE: 22.551 Epoch time: 207.457 seconds best\n",
      "Epoch: 143 Train loss: 11.343 Aux train loss: 6.986 Val loss: 5.022 Aux val loss: 3.067 Train MAE: 25.840 Val MAE: 23.571 Epoch time: 208.098 seconds \n",
      "Epoch: 144 Train loss: 10.522 Aux train loss: 6.502 Val loss: 4.971 Aux val loss: 3.070 Train MAE: 26.172 Val MAE: 23.553 Epoch time: 208.208 seconds \n",
      "Epoch: 145 Train loss: 10.924 Aux train loss: 6.746 Val loss: 4.967 Aux val loss: 3.021 Train MAE: 25.144 Val MAE: 35.633 Epoch time: 209.084 seconds \n",
      "Epoch: 146 Train loss: 11.120 Aux train loss: 6.871 Val loss: 4.956 Aux val loss: 3.060 Train MAE: 24.224 Val MAE: 27.942 Epoch time: 216.505 seconds \n",
      "Epoch: 147 Train loss: 11.213 Aux train loss: 6.951 Val loss: 4.875 Aux val loss: 2.999 Train MAE: 23.993 Val MAE: 23.478 Epoch time: 209.062 seconds \n",
      "Epoch: 148 Train loss: 10.922 Aux train loss: 6.749 Val loss: 4.932 Aux val loss: 3.025 Train MAE: 25.743 Val MAE: 25.250 Epoch time: 209.878 seconds \n",
      "Epoch: 149 Train loss: 11.122 Aux train loss: 6.876 Val loss: 4.966 Aux val loss: 3.016 Train MAE: 25.255 Val MAE: 25.121 Epoch time: 209.697 seconds \n",
      "Epoch: 150 Train loss: 11.069 Aux train loss: 6.784 Val loss: 4.947 Aux val loss: 3.049 Train MAE: 25.613 Val MAE: 21.782 Epoch time: 210.078 seconds best\n",
      "Epoch: 151 Train loss: 11.012 Aux train loss: 6.908 Val loss: 5.009 Aux val loss: 3.079 Train MAE: 24.609 Val MAE: 30.224 Epoch time: 209.853 seconds \n",
      "Epoch: 152 Train loss: 11.053 Aux train loss: 6.831 Val loss: 4.924 Aux val loss: 3.002 Train MAE: 23.442 Val MAE: 22.391 Epoch time: 212.059 seconds \n",
      "Epoch: 153 Train loss: 10.948 Aux train loss: 6.801 Val loss: 4.911 Aux val loss: 2.999 Train MAE: 23.825 Val MAE: 24.345 Epoch time: 211.094 seconds \n",
      "Epoch: 154 Train loss: 11.776 Aux train loss: 7.227 Val loss: 4.965 Aux val loss: 2.975 Train MAE: 25.159 Val MAE: 27.550 Epoch time: 206.706 seconds \n",
      "Epoch: 155 Train loss: 10.647 Aux train loss: 6.534 Val loss: 4.886 Aux val loss: 2.979 Train MAE: 23.436 Val MAE: 23.657 Epoch time: 209.086 seconds \n",
      "Epoch: 156 Train loss: 11.068 Aux train loss: 6.826 Val loss: 5.091 Aux val loss: 3.082 Train MAE: 24.743 Val MAE: 23.402 Epoch time: 209.946 seconds \n",
      "Epoch: 157 Train loss: 10.979 Aux train loss: 6.799 Val loss: 4.841 Aux val loss: 2.963 Train MAE: 24.526 Val MAE: 21.706 Epoch time: 206.190 seconds best\n",
      "Epoch: 158 Train loss: 10.355 Aux train loss: 6.387 Val loss: 4.924 Aux val loss: 3.027 Train MAE: 23.729 Val MAE: 30.558 Epoch time: 204.762 seconds \n",
      "Epoch: 159 Train loss: 10.617 Aux train loss: 6.594 Val loss: 4.868 Aux val loss: 2.974 Train MAE: 22.589 Val MAE: 26.518 Epoch time: 209.044 seconds \n",
      "Epoch: 160 Train loss: 10.996 Aux train loss: 6.766 Val loss: 4.888 Aux val loss: 2.995 Train MAE: 24.435 Val MAE: 26.861 Epoch time: 205.710 seconds \n",
      "Epoch: 161 Train loss: 10.493 Aux train loss: 6.491 Val loss: 4.876 Aux val loss: 2.976 Train MAE: 24.588 Val MAE: 23.243 Epoch time: 209.326 seconds \n",
      "Epoch: 162 Train loss: 11.408 Aux train loss: 6.875 Val loss: 4.905 Aux val loss: 2.986 Train MAE: 25.624 Val MAE: 26.881 Epoch time: 210.841 seconds \n",
      "Epoch: 163 Train loss: 10.236 Aux train loss: 6.307 Val loss: 4.961 Aux val loss: 2.973 Train MAE: 23.159 Val MAE: 27.839 Epoch time: 206.755 seconds \n",
      "Epoch: 164 Train loss: 10.346 Aux train loss: 6.371 Val loss: 4.857 Aux val loss: 2.947 Train MAE: 24.162 Val MAE: 23.317 Epoch time: 207.649 seconds \n",
      "Epoch: 165 Train loss: 11.100 Aux train loss: 6.824 Val loss: 4.818 Aux val loss: 2.961 Train MAE: 25.533 Val MAE: 27.268 Epoch time: 207.549 seconds \n",
      "Epoch: 166 Train loss: 10.117 Aux train loss: 6.245 Val loss: 4.907 Aux val loss: 3.013 Train MAE: 24.638 Val MAE: 26.602 Epoch time: 205.998 seconds \n",
      "Epoch: 167 Train loss: 10.751 Aux train loss: 6.679 Val loss: 4.835 Aux val loss: 2.982 Train MAE: 24.326 Val MAE: 24.809 Epoch time: 204.084 seconds \n",
      "Epoch: 168 Train loss: 11.133 Aux train loss: 6.872 Val loss: 4.914 Aux val loss: 2.980 Train MAE: 23.919 Val MAE: 23.634 Epoch time: 204.577 seconds \n",
      "Epoch: 169 Train loss: 10.066 Aux train loss: 6.228 Val loss: 4.827 Aux val loss: 2.955 Train MAE: 21.522 Val MAE: 27.503 Epoch time: 207.070 seconds \n",
      "Epoch: 170 Train loss: 10.205 Aux train loss: 6.306 Val loss: 4.892 Aux val loss: 2.986 Train MAE: 22.536 Val MAE: 27.642 Epoch time: 204.044 seconds \n",
      "Epoch: 171 Train loss: 10.913 Aux train loss: 6.719 Val loss: 4.973 Aux val loss: 3.038 Train MAE: 23.641 Val MAE: 29.484 Epoch time: 203.550 seconds \n",
      "Epoch: 172 Train loss: 11.413 Aux train loss: 7.040 Val loss: 4.811 Aux val loss: 2.973 Train MAE: 26.730 Val MAE: 27.687 Epoch time: 204.565 seconds \n",
      "Epoch: 173 Train loss: 10.178 Aux train loss: 6.297 Val loss: 4.887 Aux val loss: 2.966 Train MAE: 24.069 Val MAE: 23.277 Epoch time: 207.892 seconds \n",
      "Epoch: 174 Train loss: 10.694 Aux train loss: 6.569 Val loss: 4.902 Aux val loss: 2.986 Train MAE: 24.985 Val MAE: 26.215 Epoch time: 203.734 seconds \n",
      "Epoch: 175 Train loss: 10.643 Aux train loss: 6.599 Val loss: 4.971 Aux val loss: 3.067 Train MAE: 21.749 Val MAE: 29.736 Epoch time: 203.768 seconds \n",
      "Epoch: 176 Train loss: 10.824 Aux train loss: 6.671 Val loss: 5.026 Aux val loss: 2.983 Train MAE: 23.516 Val MAE: 42.414 Epoch time: 207.187 seconds \n",
      "Epoch: 177 Train loss: 10.388 Aux train loss: 6.420 Val loss: 4.814 Aux val loss: 2.940 Train MAE: 23.940 Val MAE: 23.956 Epoch time: 203.862 seconds \n",
      "Epoch: 178 Train loss: 11.088 Aux train loss: 6.852 Val loss: 4.796 Aux val loss: 2.919 Train MAE: 24.308 Val MAE: 27.619 Epoch time: 204.511 seconds \n",
      "Epoch: 179 Train loss: 10.986 Aux train loss: 6.799 Val loss: 4.814 Aux val loss: 2.952 Train MAE: 23.326 Val MAE: 22.399 Epoch time: 210.231 seconds \n",
      "Epoch: 180 Train loss: 10.202 Aux train loss: 6.316 Val loss: 4.882 Aux val loss: 3.003 Train MAE: 25.235 Val MAE: 26.832 Epoch time: 207.738 seconds \n",
      "Epoch: 181 Train loss: 10.365 Aux train loss: 6.382 Val loss: 4.793 Aux val loss: 2.922 Train MAE: 23.581 Val MAE: 23.328 Epoch time: 208.611 seconds \n",
      "Epoch: 182 Train loss: 10.542 Aux train loss: 6.494 Val loss: 4.989 Aux val loss: 2.989 Train MAE: 23.951 Val MAE: 23.814 Epoch time: 205.503 seconds \n",
      "Epoch: 183 Train loss: 10.911 Aux train loss: 6.755 Val loss: 4.941 Aux val loss: 3.001 Train MAE: 21.672 Val MAE: 35.868 Epoch time: 204.784 seconds \n",
      "Epoch: 184 Train loss: 10.683 Aux train loss: 6.579 Val loss: 4.892 Aux val loss: 2.998 Train MAE: 25.237 Val MAE: 22.325 Epoch time: 208.678 seconds \n",
      "Epoch: 185 Train loss: 10.606 Aux train loss: 6.558 Val loss: 4.902 Aux val loss: 2.963 Train MAE: 24.255 Val MAE: 25.449 Epoch time: 204.601 seconds \n",
      "Epoch: 186 Train loss: 10.598 Aux train loss: 6.533 Val loss: 4.877 Aux val loss: 2.981 Train MAE: 22.368 Val MAE: 27.627 Epoch time: 207.662 seconds \n",
      "Epoch: 187 Train loss: 11.519 Aux train loss: 7.045 Val loss: 4.909 Aux val loss: 3.011 Train MAE: 24.945 Val MAE: 23.558 Epoch time: 208.920 seconds \n",
      "Epoch: 188 Train loss: 10.726 Aux train loss: 6.623 Val loss: 4.836 Aux val loss: 2.949 Train MAE: 23.817 Val MAE: 21.741 Epoch time: 205.991 seconds \n",
      "Epoch: 189 Train loss: 11.174 Aux train loss: 6.919 Val loss: 4.906 Aux val loss: 2.977 Train MAE: 24.231 Val MAE: 31.485 Epoch time: 206.270 seconds \n",
      "Epoch: 190 Train loss: 10.781 Aux train loss: 6.689 Val loss: 4.825 Aux val loss: 2.933 Train MAE: 22.712 Val MAE: 23.984 Epoch time: 205.252 seconds \n",
      "Epoch: 191 Train loss: 10.279 Aux train loss: 6.370 Val loss: 4.844 Aux val loss: 2.956 Train MAE: 23.276 Val MAE: 24.482 Epoch time: 206.846 seconds \n",
      "Epoch: 192 Train loss: 10.652 Aux train loss: 6.589 Val loss: 4.846 Aux val loss: 2.954 Train MAE: 21.880 Val MAE: 28.391 Epoch time: 207.800 seconds \n",
      "Epoch: 193 Train loss: 11.318 Aux train loss: 6.992 Val loss: 4.892 Aux val loss: 3.001 Train MAE: 25.011 Val MAE: 28.890 Epoch time: 207.570 seconds \n",
      "Epoch: 194 Train loss: 11.111 Aux train loss: 6.858 Val loss: 4.808 Aux val loss: 2.957 Train MAE: 23.210 Val MAE: 23.351 Epoch time: 210.239 seconds \n",
      "Epoch: 195 Train loss: 10.451 Aux train loss: 6.465 Val loss: 4.779 Aux val loss: 2.913 Train MAE: 21.683 Val MAE: 28.663 Epoch time: 213.193 seconds \n",
      "Epoch: 196 Train loss: 11.283 Aux train loss: 6.711 Val loss: 4.893 Aux val loss: 2.984 Train MAE: 23.289 Val MAE: 27.286 Epoch time: 204.509 seconds \n",
      "Epoch: 197 Train loss: 10.267 Aux train loss: 6.320 Val loss: 4.834 Aux val loss: 2.929 Train MAE: 23.891 Val MAE: 26.107 Epoch time: 210.527 seconds \n",
      "Epoch: 198 Train loss: 11.325 Aux train loss: 7.012 Val loss: 4.870 Aux val loss: 2.991 Train MAE: 23.780 Val MAE: 30.239 Epoch time: 207.461 seconds \n",
      "Epoch: 199 Train loss: 11.067 Aux train loss: 6.806 Val loss: 4.872 Aux val loss: 3.008 Train MAE: 25.302 Val MAE: 25.944 Epoch time: 207.386 seconds \n",
      "Epoch: 200 Train loss: 11.356 Aux train loss: 6.355 Val loss: 4.961 Aux val loss: 3.026 Train MAE: 23.714 Val MAE: 23.281 Epoch time: 207.105 seconds \n",
      "Epoch: 201 Train loss: 28.295 Aux train loss: 6.046 Val loss: 4.732 Aux val loss: 2.871 Train MAE: 18.468 Val MAE: 23.137 Epoch time: 208.117 seconds \n",
      "Epoch: 202 Train loss: 9.483 Aux train loss: 5.856 Val loss: 4.726 Aux val loss: 2.880 Train MAE: 16.829 Val MAE: 24.349 Epoch time: 206.058 seconds \n",
      "Epoch: 203 Train loss: 9.757 Aux train loss: 6.035 Val loss: 4.740 Aux val loss: 2.853 Train MAE: 16.474 Val MAE: 20.895 Epoch time: 207.091 seconds best\n",
      "Epoch: 204 Train loss: 9.698 Aux train loss: 6.020 Val loss: 4.719 Aux val loss: 2.848 Train MAE: 15.817 Val MAE: 19.808 Epoch time: 207.980 seconds best\n",
      "Epoch: 205 Train loss: 9.674 Aux train loss: 5.972 Val loss: 4.734 Aux val loss: 2.869 Train MAE: 15.939 Val MAE: 21.937 Epoch time: 202.819 seconds \n",
      "Epoch: 206 Train loss: 9.338 Aux train loss: 5.780 Val loss: 4.716 Aux val loss: 2.851 Train MAE: 15.389 Val MAE: 21.993 Epoch time: 199.406 seconds \n",
      "Epoch: 207 Train loss: 9.947 Aux train loss: 6.175 Val loss: 4.715 Aux val loss: 2.855 Train MAE: 15.575 Val MAE: 24.258 Epoch time: 198.958 seconds \n",
      "Epoch: 208 Train loss: 9.449 Aux train loss: 5.871 Val loss: 4.734 Aux val loss: 2.859 Train MAE: 14.696 Val MAE: 22.641 Epoch time: 199.065 seconds \n",
      "Epoch: 209 Train loss: 9.806 Aux train loss: 6.078 Val loss: 4.704 Aux val loss: 2.847 Train MAE: 15.543 Val MAE: 23.928 Epoch time: 198.801 seconds \n",
      "Epoch: 210 Train loss: 9.669 Aux train loss: 5.989 Val loss: 4.707 Aux val loss: 2.858 Train MAE: 15.475 Val MAE: 22.945 Epoch time: 201.591 seconds \n",
      "Epoch: 211 Train loss: 9.812 Aux train loss: 6.078 Val loss: 4.684 Aux val loss: 2.846 Train MAE: 15.464 Val MAE: 20.074 Epoch time: 200.757 seconds \n",
      "Epoch: 212 Train loss: 9.500 Aux train loss: 5.885 Val loss: 4.688 Aux val loss: 2.847 Train MAE: 14.826 Val MAE: 21.564 Epoch time: 199.429 seconds \n",
      "Epoch: 213 Train loss: 9.357 Aux train loss: 5.805 Val loss: 4.718 Aux val loss: 2.842 Train MAE: 14.661 Val MAE: 19.390 Epoch time: 199.183 seconds best\n",
      "Epoch: 214 Train loss: 9.539 Aux train loss: 5.895 Val loss: 4.679 Aux val loss: 2.829 Train MAE: 15.238 Val MAE: 21.155 Epoch time: 199.686 seconds \n",
      "Epoch: 215 Train loss: 9.375 Aux train loss: 5.809 Val loss: 4.706 Aux val loss: 2.847 Train MAE: 14.806 Val MAE: 23.831 Epoch time: 198.897 seconds \n",
      "Epoch: 216 Train loss: 9.220 Aux train loss: 5.727 Val loss: 4.688 Aux val loss: 2.846 Train MAE: 15.372 Val MAE: 23.548 Epoch time: 198.088 seconds \n",
      "Epoch: 217 Train loss: 9.588 Aux train loss: 5.921 Val loss: 4.698 Aux val loss: 2.836 Train MAE: 15.791 Val MAE: 20.671 Epoch time: 198.078 seconds \n",
      "Epoch: 218 Train loss: 9.095 Aux train loss: 5.643 Val loss: 4.698 Aux val loss: 2.848 Train MAE: 14.334 Val MAE: 21.627 Epoch time: 199.261 seconds \n",
      "Epoch: 219 Train loss: 9.509 Aux train loss: 5.914 Val loss: 4.692 Aux val loss: 2.843 Train MAE: 15.289 Val MAE: 20.433 Epoch time: 199.205 seconds \n",
      "Epoch: 220 Train loss: 9.735 Aux train loss: 6.029 Val loss: 4.664 Aux val loss: 2.831 Train MAE: 14.739 Val MAE: 20.586 Epoch time: 197.195 seconds \n",
      "Epoch: 221 Train loss: 8.958 Aux train loss: 5.567 Val loss: 4.699 Aux val loss: 2.843 Train MAE: 13.644 Val MAE: 24.095 Epoch time: 198.355 seconds \n",
      "Epoch: 222 Train loss: 9.496 Aux train loss: 5.887 Val loss: 4.710 Aux val loss: 2.836 Train MAE: 14.642 Val MAE: 21.261 Epoch time: 197.965 seconds \n",
      "Epoch: 223 Train loss: 10.413 Aux train loss: 6.434 Val loss: 4.715 Aux val loss: 2.834 Train MAE: 15.518 Val MAE: 19.817 Epoch time: 197.742 seconds \n",
      "Epoch: 224 Train loss: 9.999 Aux train loss: 6.193 Val loss: 4.683 Aux val loss: 2.816 Train MAE: 14.769 Val MAE: 20.918 Epoch time: 198.027 seconds \n",
      "Epoch: 225 Train loss: 9.528 Aux train loss: 5.908 Val loss: 4.696 Aux val loss: 2.835 Train MAE: 14.860 Val MAE: 19.140 Epoch time: 197.562 seconds best\n",
      "Epoch: 226 Train loss: 9.900 Aux train loss: 6.126 Val loss: 4.666 Aux val loss: 2.819 Train MAE: 14.987 Val MAE: 23.447 Epoch time: 198.790 seconds \n",
      "Epoch: 227 Train loss: 9.583 Aux train loss: 5.935 Val loss: 4.650 Aux val loss: 2.827 Train MAE: 14.530 Val MAE: 19.848 Epoch time: 198.072 seconds \n",
      "Epoch: 228 Train loss: 9.667 Aux train loss: 6.002 Val loss: 4.694 Aux val loss: 2.845 Train MAE: 14.109 Val MAE: 24.262 Epoch time: 198.036 seconds \n",
      "Epoch: 229 Train loss: 10.298 Aux train loss: 6.361 Val loss: 4.680 Aux val loss: 2.841 Train MAE: 14.742 Val MAE: 21.058 Epoch time: 197.045 seconds \n",
      "Epoch: 230 Train loss: 9.602 Aux train loss: 5.955 Val loss: 4.679 Aux val loss: 2.840 Train MAE: 14.833 Val MAE: 22.253 Epoch time: 197.973 seconds \n",
      "Epoch: 231 Train loss: 9.494 Aux train loss: 5.891 Val loss: 4.643 Aux val loss: 2.813 Train MAE: 14.258 Val MAE: 20.463 Epoch time: 197.673 seconds \n",
      "Epoch: 232 Train loss: 9.029 Aux train loss: 5.594 Val loss: 4.662 Aux val loss: 2.836 Train MAE: 14.638 Val MAE: 19.717 Epoch time: 197.964 seconds \n",
      "Epoch: 233 Train loss: 9.292 Aux train loss: 5.769 Val loss: 4.654 Aux val loss: 2.833 Train MAE: 13.357 Val MAE: 19.445 Epoch time: 197.279 seconds \n",
      "Epoch: 234 Train loss: 9.484 Aux train loss: 5.883 Val loss: 4.639 Aux val loss: 2.833 Train MAE: 14.777 Val MAE: 19.860 Epoch time: 197.132 seconds \n",
      "Epoch: 235 Train loss: 9.352 Aux train loss: 5.795 Val loss: 4.650 Aux val loss: 2.837 Train MAE: 14.210 Val MAE: 21.318 Epoch time: 197.199 seconds \n",
      "Epoch: 236 Train loss: 10.320 Aux train loss: 6.391 Val loss: 4.648 Aux val loss: 2.832 Train MAE: 14.631 Val MAE: 21.888 Epoch time: 197.750 seconds \n",
      "Epoch: 237 Train loss: 9.442 Aux train loss: 5.881 Val loss: 4.658 Aux val loss: 2.831 Train MAE: 14.126 Val MAE: 20.387 Epoch time: 198.219 seconds \n",
      "Epoch: 238 Train loss: 9.401 Aux train loss: 5.834 Val loss: 4.662 Aux val loss: 2.825 Train MAE: 14.961 Val MAE: 21.125 Epoch time: 197.361 seconds \n",
      "Epoch: 239 Train loss: 9.860 Aux train loss: 6.124 Val loss: 4.715 Aux val loss: 2.858 Train MAE: 13.928 Val MAE: 22.097 Epoch time: 197.678 seconds \n",
      "Epoch: 240 Train loss: 9.916 Aux train loss: 6.157 Val loss: 4.663 Aux val loss: 2.843 Train MAE: 14.756 Val MAE: 22.310 Epoch time: 197.514 seconds \n",
      "Epoch: 241 Train loss: 9.500 Aux train loss: 5.884 Val loss: 4.661 Aux val loss: 2.834 Train MAE: 13.845 Val MAE: 19.734 Epoch time: 197.035 seconds \n",
      "Epoch: 242 Train loss: 8.958 Aux train loss: 5.577 Val loss: 4.687 Aux val loss: 2.845 Train MAE: 13.738 Val MAE: 25.778 Epoch time: 197.649 seconds \n",
      "Epoch: 243 Train loss: 9.757 Aux train loss: 6.044 Val loss: 4.670 Aux val loss: 2.845 Train MAE: 14.386 Val MAE: 19.597 Epoch time: 197.502 seconds \n",
      "Epoch: 244 Train loss: 9.594 Aux train loss: 5.950 Val loss: 4.650 Aux val loss: 2.818 Train MAE: 14.482 Val MAE: 22.344 Epoch time: 197.596 seconds \n",
      "Epoch: 245 Train loss: 9.966 Aux train loss: 6.175 Val loss: 4.647 Aux val loss: 2.822 Train MAE: 14.140 Val MAE: 20.299 Epoch time: 197.730 seconds \n",
      "Epoch: 246 Train loss: 9.088 Aux train loss: 5.635 Val loss: 4.627 Aux val loss: 2.816 Train MAE: 14.518 Val MAE: 20.343 Epoch time: 197.618 seconds \n",
      "Epoch: 247 Train loss: 10.077 Aux train loss: 6.242 Val loss: 4.687 Aux val loss: 2.835 Train MAE: 14.771 Val MAE: 24.087 Epoch time: 197.492 seconds \n",
      "Epoch: 248 Train loss: 9.356 Aux train loss: 5.816 Val loss: 4.646 Aux val loss: 2.824 Train MAE: 14.000 Val MAE: 20.993 Epoch time: 197.516 seconds \n",
      "Epoch: 249 Train loss: 9.283 Aux train loss: 5.760 Val loss: 4.627 Aux val loss: 2.804 Train MAE: 14.607 Val MAE: 20.381 Epoch time: 197.005 seconds \n",
      "Epoch: 250 Train loss: 9.227 Aux train loss: 5.722 Val loss: 4.677 Aux val loss: 2.830 Train MAE: 14.381 Val MAE: 25.258 Epoch time: 197.428 seconds \n",
      "Epoch: 251 Train loss: 9.392 Aux train loss: 5.831 Val loss: 4.654 Aux val loss: 2.828 Train MAE: 14.435 Val MAE: 20.608 Epoch time: 197.305 seconds \n",
      "Epoch: 252 Train loss: 8.852 Aux train loss: 5.498 Val loss: 4.652 Aux val loss: 2.832 Train MAE: 13.750 Val MAE: 21.393 Epoch time: 197.710 seconds \n",
      "Epoch: 253 Train loss: 9.451 Aux train loss: 5.866 Val loss: 4.621 Aux val loss: 2.814 Train MAE: 13.605 Val MAE: 20.833 Epoch time: 197.486 seconds \n",
      "Epoch: 254 Train loss: 9.534 Aux train loss: 5.925 Val loss: 4.646 Aux val loss: 2.823 Train MAE: 14.257 Val MAE: 23.058 Epoch time: 197.507 seconds \n",
      "Epoch: 255 Train loss: 9.625 Aux train loss: 5.970 Val loss: 4.657 Aux val loss: 2.827 Train MAE: 13.410 Val MAE: 21.531 Epoch time: 197.610 seconds \n",
      "Epoch: 256 Train loss: 8.987 Aux train loss: 5.593 Val loss: 4.647 Aux val loss: 2.828 Train MAE: 13.573 Val MAE: 20.373 Epoch time: 197.381 seconds \n",
      "Epoch: 257 Train loss: 8.991 Aux train loss: 5.567 Val loss: 4.670 Aux val loss: 2.850 Train MAE: 13.455 Val MAE: 21.305 Epoch time: 197.826 seconds \n",
      "Epoch: 258 Train loss: 8.735 Aux train loss: 5.434 Val loss: 4.640 Aux val loss: 2.828 Train MAE: 13.410 Val MAE: 22.208 Epoch time: 197.225 seconds \n",
      "Epoch: 259 Train loss: 9.276 Aux train loss: 5.768 Val loss: 4.660 Aux val loss: 2.826 Train MAE: 13.789 Val MAE: 20.911 Epoch time: 198.061 seconds \n",
      "Epoch: 260 Train loss: 9.361 Aux train loss: 5.811 Val loss: 4.665 Aux val loss: 2.842 Train MAE: 14.224 Val MAE: 19.556 Epoch time: 197.068 seconds \n",
      "Epoch: 261 Train loss: 9.139 Aux train loss: 5.686 Val loss: 4.672 Aux val loss: 2.833 Train MAE: 13.930 Val MAE: 24.072 Epoch time: 197.371 seconds \n",
      "Epoch: 262 Train loss: 8.826 Aux train loss: 5.496 Val loss: 4.654 Aux val loss: 2.847 Train MAE: 13.344 Val MAE: 19.496 Epoch time: 197.197 seconds \n",
      "Epoch: 263 Train loss: 9.020 Aux train loss: 5.603 Val loss: 4.640 Aux val loss: 2.823 Train MAE: 13.605 Val MAE: 20.978 Epoch time: 197.194 seconds \n",
      "Epoch: 264 Train loss: 8.631 Aux train loss: 5.339 Val loss: 4.631 Aux val loss: 2.822 Train MAE: 13.962 Val MAE: 21.181 Epoch time: 197.837 seconds \n",
      "Epoch: 265 Train loss: 9.332 Aux train loss: 5.806 Val loss: 4.622 Aux val loss: 2.811 Train MAE: 13.663 Val MAE: 20.023 Epoch time: 200.591 seconds \n",
      "Epoch: 266 Train loss: 8.936 Aux train loss: 5.566 Val loss: 4.632 Aux val loss: 2.808 Train MAE: 13.396 Val MAE: 19.386 Epoch time: 204.605 seconds \n",
      "Epoch: 267 Train loss: 8.932 Aux train loss: 5.571 Val loss: 4.618 Aux val loss: 2.813 Train MAE: 13.797 Val MAE: 20.079 Epoch time: 204.388 seconds \n",
      "Epoch: 268 Train loss: 8.562 Aux train loss: 5.324 Val loss: 4.631 Aux val loss: 2.808 Train MAE: 13.324 Val MAE: 21.412 Epoch time: 204.429 seconds \n",
      "Epoch: 269 Train loss: 8.812 Aux train loss: 5.477 Val loss: 4.620 Aux val loss: 2.818 Train MAE: 13.688 Val MAE: 22.329 Epoch time: 204.858 seconds \n",
      "Epoch: 270 Train loss: 9.193 Aux train loss: 5.699 Val loss: 4.621 Aux val loss: 2.814 Train MAE: 13.772 Val MAE: 22.403 Epoch time: 204.271 seconds \n",
      "Epoch: 271 Train loss: 8.850 Aux train loss: 5.510 Val loss: 4.589 Aux val loss: 2.807 Train MAE: 13.343 Val MAE: 19.582 Epoch time: 209.316 seconds \n",
      "Epoch: 272 Train loss: 9.669 Aux train loss: 6.007 Val loss: 4.622 Aux val loss: 2.807 Train MAE: 14.792 Val MAE: 21.088 Epoch time: 209.084 seconds \n",
      "Epoch: 273 Train loss: 9.279 Aux train loss: 5.764 Val loss: 4.626 Aux val loss: 2.805 Train MAE: 14.056 Val MAE: 21.285 Epoch time: 207.574 seconds \n",
      "Epoch: 274 Train loss: 9.199 Aux train loss: 5.705 Val loss: 4.607 Aux val loss: 2.803 Train MAE: 13.784 Val MAE: 20.595 Epoch time: 204.990 seconds \n",
      "Epoch: 275 Train loss: 8.925 Aux train loss: 5.510 Val loss: 4.658 Aux val loss: 2.828 Train MAE: 13.423 Val MAE: 21.933 Epoch time: 205.071 seconds \n",
      "Epoch: 276 Train loss: 9.443 Aux train loss: 5.874 Val loss: 4.611 Aux val loss: 2.803 Train MAE: 13.507 Val MAE: 20.442 Epoch time: 204.715 seconds \n",
      "Epoch: 277 Train loss: 8.932 Aux train loss: 5.541 Val loss: 4.648 Aux val loss: 2.816 Train MAE: 13.163 Val MAE: 22.443 Epoch time: 204.850 seconds \n",
      "Epoch: 278 Train loss: 9.339 Aux train loss: 5.800 Val loss: 4.644 Aux val loss: 2.827 Train MAE: 13.627 Val MAE: 19.218 Epoch time: 204.636 seconds \n",
      "Epoch: 279 Train loss: 9.321 Aux train loss: 5.800 Val loss: 4.650 Aux val loss: 2.830 Train MAE: 13.633 Val MAE: 20.925 Epoch time: 204.686 seconds \n",
      "Epoch: 280 Train loss: 8.872 Aux train loss: 5.517 Val loss: 4.614 Aux val loss: 2.797 Train MAE: 13.257 Val MAE: 19.769 Epoch time: 203.770 seconds \n",
      "Epoch: 281 Train loss: 9.149 Aux train loss: 5.692 Val loss: 4.629 Aux val loss: 2.813 Train MAE: 13.129 Val MAE: 21.159 Epoch time: 205.069 seconds \n",
      "Epoch: 282 Train loss: 8.808 Aux train loss: 5.497 Val loss: 4.620 Aux val loss: 2.802 Train MAE: 13.425 Val MAE: 20.533 Epoch time: 206.104 seconds \n",
      "Epoch: 283 Train loss: 9.537 Aux train loss: 5.919 Val loss: 4.601 Aux val loss: 2.804 Train MAE: 13.367 Val MAE: 19.811 Epoch time: 206.068 seconds \n",
      "Epoch: 284 Train loss: 9.253 Aux train loss: 5.741 Val loss: 4.645 Aux val loss: 2.827 Train MAE: 13.277 Val MAE: 20.342 Epoch time: 204.667 seconds \n",
      "Epoch: 285 Train loss: 9.231 Aux train loss: 5.728 Val loss: 4.662 Aux val loss: 2.832 Train MAE: 13.841 Val MAE: 20.707 Epoch time: 206.121 seconds \n",
      "Epoch: 286 Train loss: 9.319 Aux train loss: 5.793 Val loss: 4.623 Aux val loss: 2.800 Train MAE: 13.830 Val MAE: 19.927 Epoch time: 205.206 seconds \n",
      "Epoch: 287 Train loss: 8.336 Aux train loss: 5.174 Val loss: 4.616 Aux val loss: 2.806 Train MAE: 12.920 Val MAE: 21.351 Epoch time: 205.083 seconds \n",
      "Epoch: 288 Train loss: 9.021 Aux train loss: 5.609 Val loss: 4.627 Aux val loss: 2.802 Train MAE: 13.162 Val MAE: 19.997 Epoch time: 204.933 seconds \n",
      "Epoch: 289 Train loss: 8.883 Aux train loss: 5.543 Val loss: 4.680 Aux val loss: 2.837 Train MAE: 13.116 Val MAE: 24.344 Epoch time: 204.455 seconds \n",
      "Epoch: 290 Train loss: 9.297 Aux train loss: 5.772 Val loss: 4.656 Aux val loss: 2.833 Train MAE: 13.426 Val MAE: 20.444 Epoch time: 206.269 seconds \n",
      "Epoch: 291 Train loss: 8.345 Aux train loss: 5.215 Val loss: 4.657 Aux val loss: 2.810 Train MAE: 13.162 Val MAE: 21.618 Epoch time: 204.426 seconds \n",
      "Epoch: 292 Train loss: 8.874 Aux train loss: 5.516 Val loss: 4.660 Aux val loss: 2.816 Train MAE: 13.717 Val MAE: 22.778 Epoch time: 204.798 seconds \n",
      "Epoch: 293 Train loss: 9.370 Aux train loss: 5.813 Val loss: 4.615 Aux val loss: 2.800 Train MAE: 13.471 Val MAE: 20.534 Epoch time: 205.827 seconds \n",
      "Epoch: 294 Train loss: 8.936 Aux train loss: 5.555 Val loss: 4.598 Aux val loss: 2.799 Train MAE: 13.270 Val MAE: 20.901 Epoch time: 205.251 seconds \n",
      "Epoch: 295 Train loss: 8.834 Aux train loss: 5.497 Val loss: 4.617 Aux val loss: 2.800 Train MAE: 12.900 Val MAE: 19.684 Epoch time: 204.657 seconds \n",
      "Epoch: 296 Train loss: 9.142 Aux train loss: 5.684 Val loss: 4.657 Aux val loss: 2.829 Train MAE: 13.237 Val MAE: 21.403 Epoch time: 207.764 seconds \n",
      "Epoch: 297 Train loss: 8.493 Aux train loss: 5.279 Val loss: 4.662 Aux val loss: 2.821 Train MAE: 12.992 Val MAE: 21.581 Epoch time: 212.426 seconds \n",
      "Epoch: 298 Train loss: 9.587 Aux train loss: 5.970 Val loss: 4.682 Aux val loss: 2.837 Train MAE: 14.389 Val MAE: 22.330 Epoch time: 212.251 seconds \n",
      "Epoch: 299 Train loss: 9.031 Aux train loss: 5.598 Val loss: 4.653 Aux val loss: 2.816 Train MAE: 13.325 Val MAE: 22.877 Epoch time: 211.247 seconds \n",
      "Epoch: 300 Train loss: 9.291 Aux train loss: 5.794 Val loss: 4.646 Aux val loss: 2.810 Train MAE: 13.539 Val MAE: 21.470 Epoch time: 204.942 seconds \n",
      "Epoch: 301 Train loss: 9.596 Aux train loss: 5.967 Val loss: 4.657 Aux val loss: 2.820 Train MAE: 14.478 Val MAE: 19.964 Epoch time: 199.673 seconds \n",
      "Epoch: 302 Train loss: 9.052 Aux train loss: 5.630 Val loss: 4.639 Aux val loss: 2.818 Train MAE: 13.180 Val MAE: 20.493 Epoch time: 200.729 seconds \n",
      "Epoch: 303 Train loss: 9.080 Aux train loss: 5.648 Val loss: 4.678 Aux val loss: 2.834 Train MAE: 12.917 Val MAE: 19.984 Epoch time: 201.149 seconds \n",
      "Epoch: 304 Train loss: 8.288 Aux train loss: 5.162 Val loss: 4.650 Aux val loss: 2.816 Train MAE: 12.764 Val MAE: 20.760 Epoch time: 202.015 seconds \n",
      "Epoch: 305 Train loss: 9.609 Aux train loss: 5.962 Val loss: 4.670 Aux val loss: 2.824 Train MAE: 13.864 Val MAE: 20.075 Epoch time: 198.437 seconds \n",
      "Epoch: 306 Train loss: 8.755 Aux train loss: 5.459 Val loss: 4.653 Aux val loss: 2.831 Train MAE: 12.970 Val MAE: 21.698 Epoch time: 198.224 seconds \n",
      "Epoch: 307 Train loss: 9.037 Aux train loss: 5.620 Val loss: 4.616 Aux val loss: 2.801 Train MAE: 12.790 Val MAE: 22.271 Epoch time: 198.655 seconds \n",
      "Epoch: 308 Train loss: 9.159 Aux train loss: 5.695 Val loss: 4.621 Aux val loss: 2.807 Train MAE: 13.222 Val MAE: 20.841 Epoch time: 197.402 seconds \n",
      "Epoch: 309 Train loss: 8.625 Aux train loss: 5.375 Val loss: 4.672 Aux val loss: 2.816 Train MAE: 12.946 Val MAE: 23.902 Epoch time: 197.161 seconds \n",
      "Epoch: 310 Train loss: 8.774 Aux train loss: 5.455 Val loss: 4.671 Aux val loss: 2.835 Train MAE: 12.699 Val MAE: 21.909 Epoch time: 198.328 seconds \n",
      "Epoch: 311 Train loss: 8.400 Aux train loss: 5.223 Val loss: 4.681 Aux val loss: 2.836 Train MAE: 13.166 Val MAE: 20.999 Epoch time: 198.136 seconds \n",
      "Epoch: 312 Train loss: 8.891 Aux train loss: 5.532 Val loss: 4.628 Aux val loss: 2.802 Train MAE: 12.237 Val MAE: 21.167 Epoch time: 198.754 seconds \n",
      "Epoch: 313 Train loss: 9.013 Aux train loss: 5.607 Val loss: 4.689 Aux val loss: 2.827 Train MAE: 12.925 Val MAE: 20.216 Epoch time: 198.388 seconds \n",
      "Epoch: 314 Train loss: 9.234 Aux train loss: 5.742 Val loss: 4.651 Aux val loss: 2.816 Train MAE: 12.932 Val MAE: 21.353 Epoch time: 198.093 seconds \n",
      "Epoch: 315 Train loss: 9.086 Aux train loss: 5.657 Val loss: 4.636 Aux val loss: 2.805 Train MAE: 13.311 Val MAE: 20.410 Epoch time: 198.651 seconds \n",
      "Epoch: 316 Train loss: 9.084 Aux train loss: 5.654 Val loss: 4.686 Aux val loss: 2.834 Train MAE: 12.456 Val MAE: 19.027 Epoch time: 198.079 seconds best\n",
      "Epoch: 317 Train loss: 8.665 Aux train loss: 5.388 Val loss: 4.608 Aux val loss: 2.790 Train MAE: 12.962 Val MAE: 22.337 Epoch time: 198.029 seconds \n",
      "Epoch: 318 Train loss: 9.237 Aux train loss: 5.743 Val loss: 4.677 Aux val loss: 2.823 Train MAE: 12.679 Val MAE: 27.003 Epoch time: 198.114 seconds \n",
      "Epoch: 319 Train loss: 8.932 Aux train loss: 5.556 Val loss: 4.597 Aux val loss: 2.797 Train MAE: 13.418 Val MAE: 21.398 Epoch time: 198.573 seconds \n",
      "Epoch: 320 Train loss: 8.687 Aux train loss: 5.401 Val loss: 4.629 Aux val loss: 2.805 Train MAE: 12.354 Val MAE: 21.826 Epoch time: 198.241 seconds \n",
      "Epoch: 321 Train loss: 9.677 Aux train loss: 5.995 Val loss: 4.628 Aux val loss: 2.812 Train MAE: 13.159 Val MAE: 21.652 Epoch time: 198.388 seconds \n",
      "Epoch: 322 Train loss: 8.874 Aux train loss: 5.530 Val loss: 4.654 Aux val loss: 2.808 Train MAE: 12.672 Val MAE: 23.068 Epoch time: 198.281 seconds \n",
      "Epoch: 323 Train loss: 8.436 Aux train loss: 5.239 Val loss: 4.679 Aux val loss: 2.827 Train MAE: 13.300 Val MAE: 21.821 Epoch time: 198.460 seconds \n",
      "Epoch: 324 Train loss: 9.152 Aux train loss: 5.721 Val loss: 4.647 Aux val loss: 2.814 Train MAE: 12.979 Val MAE: 19.626 Epoch time: 198.485 seconds \n",
      "Epoch: 325 Train loss: 9.601 Aux train loss: 5.972 Val loss: 4.674 Aux val loss: 2.836 Train MAE: 14.448 Val MAE: 20.265 Epoch time: 198.304 seconds \n",
      "Epoch: 326 Train loss: 9.182 Aux train loss: 5.712 Val loss: 4.649 Aux val loss: 2.815 Train MAE: 12.504 Val MAE: 24.885 Epoch time: 197.819 seconds \n",
      "Epoch: 327 Train loss: 8.873 Aux train loss: 5.506 Val loss: 4.681 Aux val loss: 2.840 Train MAE: 12.258 Val MAE: 22.085 Epoch time: 197.997 seconds \n",
      "Epoch: 328 Train loss: 9.016 Aux train loss: 5.609 Val loss: 4.680 Aux val loss: 2.843 Train MAE: 13.109 Val MAE: 22.083 Epoch time: 198.591 seconds \n",
      "Epoch: 329 Train loss: 9.174 Aux train loss: 5.716 Val loss: 4.596 Aux val loss: 2.795 Train MAE: 12.757 Val MAE: 21.879 Epoch time: 198.156 seconds \n",
      "Epoch: 330 Train loss: 9.304 Aux train loss: 5.763 Val loss: 4.648 Aux val loss: 2.821 Train MAE: 13.390 Val MAE: 21.117 Epoch time: 197.992 seconds \n",
      "Epoch: 331 Train loss: 8.754 Aux train loss: 5.458 Val loss: 4.609 Aux val loss: 2.808 Train MAE: 12.664 Val MAE: 20.885 Epoch time: 197.910 seconds \n",
      "Epoch: 332 Train loss: 9.424 Aux train loss: 5.868 Val loss: 4.623 Aux val loss: 2.809 Train MAE: 12.237 Val MAE: 21.528 Epoch time: 198.481 seconds \n",
      "Epoch: 333 Train loss: 9.284 Aux train loss: 5.813 Val loss: 4.653 Aux val loss: 2.811 Train MAE: 12.953 Val MAE: 20.211 Epoch time: 198.242 seconds \n",
      "Epoch: 334 Train loss: 8.973 Aux train loss: 5.578 Val loss: 4.597 Aux val loss: 2.790 Train MAE: 12.768 Val MAE: 21.879 Epoch time: 198.653 seconds \n",
      "Epoch: 335 Train loss: 8.655 Aux train loss: 5.379 Val loss: 4.629 Aux val loss: 2.796 Train MAE: 12.770 Val MAE: 22.757 Epoch time: 198.056 seconds \n",
      "Epoch: 336 Train loss: 8.434 Aux train loss: 5.254 Val loss: 4.606 Aux val loss: 2.800 Train MAE: 12.652 Val MAE: 21.187 Epoch time: 198.730 seconds \n",
      "Epoch: 337 Train loss: 9.026 Aux train loss: 5.626 Val loss: 4.608 Aux val loss: 2.797 Train MAE: 12.496 Val MAE: 22.559 Epoch time: 198.502 seconds \n",
      "Epoch: 338 Train loss: 9.361 Aux train loss: 5.828 Val loss: 4.681 Aux val loss: 2.834 Train MAE: 13.229 Val MAE: 21.592 Epoch time: 198.813 seconds \n",
      "Epoch: 339 Train loss: 8.810 Aux train loss: 5.488 Val loss: 4.634 Aux val loss: 2.805 Train MAE: 12.366 Val MAE: 21.425 Epoch time: 197.700 seconds \n",
      "Epoch: 340 Train loss: 9.617 Aux train loss: 5.968 Val loss: 4.688 Aux val loss: 2.835 Train MAE: 12.832 Val MAE: 21.160 Epoch time: 198.570 seconds \n",
      "Epoch: 341 Train loss: 8.784 Aux train loss: 5.461 Val loss: 4.592 Aux val loss: 2.790 Train MAE: 12.419 Val MAE: 22.185 Epoch time: 197.967 seconds \n",
      "Epoch: 342 Train loss: 8.711 Aux train loss: 5.418 Val loss: 4.624 Aux val loss: 2.798 Train MAE: 12.563 Val MAE: 22.650 Epoch time: 196.987 seconds \n",
      "Epoch: 343 Train loss: 8.813 Aux train loss: 5.490 Val loss: 4.630 Aux val loss: 2.797 Train MAE: 12.530 Val MAE: 21.876 Epoch time: 197.589 seconds \n",
      "Epoch: 344 Train loss: 8.417 Aux train loss: 5.254 Val loss: 4.611 Aux val loss: 2.789 Train MAE: 12.059 Val MAE: 21.285 Epoch time: 197.041 seconds \n",
      "Epoch: 345 Train loss: 8.625 Aux train loss: 5.386 Val loss: 4.618 Aux val loss: 2.806 Train MAE: 13.071 Val MAE: 22.955 Epoch time: 197.783 seconds \n",
      "Epoch: 346 Train loss: 8.590 Aux train loss: 5.360 Val loss: 4.655 Aux val loss: 2.814 Train MAE: 12.261 Val MAE: 23.855 Epoch time: 197.162 seconds \n",
      "Epoch: 347 Train loss: 8.665 Aux train loss: 5.392 Val loss: 4.628 Aux val loss: 2.800 Train MAE: 12.082 Val MAE: 21.422 Epoch time: 197.110 seconds \n",
      "Epoch: 348 Train loss: 9.177 Aux train loss: 5.709 Val loss: 4.582 Aux val loss: 2.784 Train MAE: 12.910 Val MAE: 21.490 Epoch time: 197.387 seconds \n",
      "Epoch: 349 Train loss: 8.345 Aux train loss: 5.219 Val loss: 4.600 Aux val loss: 2.794 Train MAE: 12.106 Val MAE: 21.894 Epoch time: 197.636 seconds \n",
      "Epoch: 350 Train loss: 8.843 Aux train loss: 5.506 Val loss: 4.619 Aux val loss: 2.808 Train MAE: 12.786 Val MAE: 25.714 Epoch time: 197.104 seconds \n",
      "Epoch: 351 Train loss: 8.375 Aux train loss: 5.209 Val loss: 4.621 Aux val loss: 2.806 Train MAE: 12.856 Val MAE: 19.613 Epoch time: 196.950 seconds \n",
      "Epoch: 352 Train loss: 9.317 Aux train loss: 5.732 Val loss: 4.602 Aux val loss: 2.788 Train MAE: 13.294 Val MAE: 21.139 Epoch time: 197.629 seconds \n",
      "Epoch: 353 Train loss: 9.067 Aux train loss: 5.654 Val loss: 4.670 Aux val loss: 2.832 Train MAE: 12.890 Val MAE: 20.760 Epoch time: 197.195 seconds \n",
      "Epoch: 354 Train loss: 8.731 Aux train loss: 5.451 Val loss: 4.621 Aux val loss: 2.794 Train MAE: 12.316 Val MAE: 21.838 Epoch time: 197.153 seconds \n",
      "Epoch: 355 Train loss: 8.530 Aux train loss: 5.332 Val loss: 4.620 Aux val loss: 2.807 Train MAE: 12.973 Val MAE: 20.573 Epoch time: 197.379 seconds \n",
      "Epoch: 356 Train loss: 8.497 Aux train loss: 5.259 Val loss: 4.636 Aux val loss: 2.803 Train MAE: 12.266 Val MAE: 22.141 Epoch time: 197.958 seconds \n",
      "Epoch: 357 Train loss: 9.346 Aux train loss: 5.814 Val loss: 4.597 Aux val loss: 2.797 Train MAE: 11.867 Val MAE: 20.246 Epoch time: 197.767 seconds \n",
      "Epoch: 358 Train loss: 8.636 Aux train loss: 5.377 Val loss: 4.622 Aux val loss: 2.802 Train MAE: 12.277 Val MAE: 19.885 Epoch time: 197.773 seconds \n",
      "Epoch: 359 Train loss: 9.022 Aux train loss: 5.628 Val loss: 4.628 Aux val loss: 2.810 Train MAE: 12.786 Val MAE: 22.173 Epoch time: 197.167 seconds \n",
      "Epoch: 360 Train loss: 8.929 Aux train loss: 5.552 Val loss: 4.636 Aux val loss: 2.808 Train MAE: 12.786 Val MAE: 23.110 Epoch time: 197.522 seconds \n",
      "Epoch: 361 Train loss: 8.624 Aux train loss: 5.369 Val loss: 4.608 Aux val loss: 2.786 Train MAE: 12.210 Val MAE: 20.548 Epoch time: 196.988 seconds \n",
      "Epoch: 362 Train loss: 9.433 Aux train loss: 5.873 Val loss: 4.619 Aux val loss: 2.800 Train MAE: 12.831 Val MAE: 21.472 Epoch time: 197.529 seconds \n",
      "Epoch: 363 Train loss: 8.319 Aux train loss: 5.201 Val loss: 4.654 Aux val loss: 2.810 Train MAE: 12.357 Val MAE: 20.932 Epoch time: 197.508 seconds \n",
      "Epoch: 364 Train loss: 8.359 Aux train loss: 5.196 Val loss: 4.660 Aux val loss: 2.817 Train MAE: 12.359 Val MAE: 25.501 Epoch time: 197.576 seconds \n",
      "Epoch: 365 Train loss: 8.528 Aux train loss: 5.325 Val loss: 4.635 Aux val loss: 2.808 Train MAE: 12.002 Val MAE: 20.748 Epoch time: 197.194 seconds \n",
      "Epoch: 366 Train loss: 8.142 Aux train loss: 5.083 Val loss: 4.632 Aux val loss: 2.810 Train MAE: 12.765 Val MAE: 22.044 Epoch time: 197.407 seconds \n",
      "Epoch: 367 Train loss: 8.837 Aux train loss: 5.517 Val loss: 4.636 Aux val loss: 2.806 Train MAE: 13.239 Val MAE: 19.748 Epoch time: 197.484 seconds \n",
      "Epoch: 368 Train loss: 8.992 Aux train loss: 5.607 Val loss: 4.638 Aux val loss: 2.803 Train MAE: 13.103 Val MAE: 21.062 Epoch time: 197.215 seconds \n",
      "Epoch: 369 Train loss: 8.128 Aux train loss: 5.070 Val loss: 4.629 Aux val loss: 2.798 Train MAE: 12.165 Val MAE: 20.624 Epoch time: 197.721 seconds \n",
      "Epoch: 370 Train loss: 8.634 Aux train loss: 5.390 Val loss: 4.642 Aux val loss: 2.800 Train MAE: 12.172 Val MAE: 23.500 Epoch time: 196.445 seconds \n",
      "Epoch: 371 Train loss: 9.029 Aux train loss: 5.635 Val loss: 4.621 Aux val loss: 2.801 Train MAE: 13.458 Val MAE: 20.653 Epoch time: 196.897 seconds \n",
      "Epoch: 372 Train loss: 8.692 Aux train loss: 5.421 Val loss: 4.600 Aux val loss: 2.780 Train MAE: 11.684 Val MAE: 20.458 Epoch time: 198.065 seconds \n",
      "Epoch: 373 Train loss: 8.949 Aux train loss: 5.592 Val loss: 4.679 Aux val loss: 2.802 Train MAE: 13.332 Val MAE: 21.592 Epoch time: 197.287 seconds \n",
      "Epoch: 374 Train loss: 8.550 Aux train loss: 5.323 Val loss: 4.591 Aux val loss: 2.786 Train MAE: 12.311 Val MAE: 20.560 Epoch time: 197.475 seconds \n",
      "Epoch: 375 Train loss: 9.072 Aux train loss: 5.667 Val loss: 4.620 Aux val loss: 2.797 Train MAE: 12.440 Val MAE: 21.074 Epoch time: 196.569 seconds \n",
      "Epoch: 376 Train loss: 8.582 Aux train loss: 5.357 Val loss: 4.567 Aux val loss: 2.778 Train MAE: 11.691 Val MAE: 21.034 Epoch time: 196.699 seconds \n",
      "Epoch: 377 Train loss: 9.137 Aux train loss: 5.710 Val loss: 4.605 Aux val loss: 2.795 Train MAE: 12.586 Val MAE: 19.898 Epoch time: 197.604 seconds \n",
      "Epoch: 378 Train loss: 8.690 Aux train loss: 5.428 Val loss: 4.609 Aux val loss: 2.788 Train MAE: 12.083 Val MAE: 20.898 Epoch time: 196.955 seconds \n",
      "Epoch: 379 Train loss: 9.012 Aux train loss: 5.617 Val loss: 4.613 Aux val loss: 2.789 Train MAE: 12.496 Val MAE: 21.404 Epoch time: 197.613 seconds \n",
      "Epoch: 380 Train loss: 8.581 Aux train loss: 5.371 Val loss: 4.675 Aux val loss: 2.809 Train MAE: 11.521 Val MAE: 20.900 Epoch time: 197.604 seconds \n",
      "Epoch: 381 Train loss: 8.337 Aux train loss: 5.188 Val loss: 4.662 Aux val loss: 2.822 Train MAE: 12.376 Val MAE: 21.943 Epoch time: 197.903 seconds \n",
      "Epoch: 382 Train loss: 8.865 Aux train loss: 5.524 Val loss: 4.674 Aux val loss: 2.812 Train MAE: 12.008 Val MAE: 21.478 Epoch time: 197.219 seconds \n",
      "Epoch: 383 Train loss: 9.191 Aux train loss: 5.739 Val loss: 4.625 Aux val loss: 2.804 Train MAE: 12.588 Val MAE: 21.612 Epoch time: 198.291 seconds \n",
      "Epoch: 384 Train loss: 9.067 Aux train loss: 5.657 Val loss: 4.627 Aux val loss: 2.788 Train MAE: 12.048 Val MAE: 22.661 Epoch time: 197.430 seconds \n",
      "Epoch: 385 Train loss: 8.667 Aux train loss: 5.421 Val loss: 4.579 Aux val loss: 2.778 Train MAE: 11.639 Val MAE: 21.137 Epoch time: 196.592 seconds \n",
      "Epoch: 386 Train loss: 8.944 Aux train loss: 5.565 Val loss: 4.585 Aux val loss: 2.779 Train MAE: 12.410 Val MAE: 20.377 Epoch time: 197.027 seconds \n",
      "Epoch: 387 Train loss: 8.161 Aux train loss: 5.083 Val loss: 4.582 Aux val loss: 2.771 Train MAE: 11.682 Val MAE: 21.951 Epoch time: 197.152 seconds \n",
      "Epoch: 388 Train loss: 8.803 Aux train loss: 5.484 Val loss: 4.633 Aux val loss: 2.795 Train MAE: 12.287 Val MAE: 20.562 Epoch time: 196.893 seconds \n",
      "Epoch: 389 Train loss: 9.157 Aux train loss: 5.687 Val loss: 4.609 Aux val loss: 2.790 Train MAE: 12.359 Val MAE: 21.550 Epoch time: 197.653 seconds \n",
      "Epoch: 390 Train loss: 8.863 Aux train loss: 5.538 Val loss: 4.603 Aux val loss: 2.791 Train MAE: 12.369 Val MAE: 23.398 Epoch time: 197.047 seconds \n",
      "Epoch: 391 Train loss: 8.317 Aux train loss: 5.185 Val loss: 4.625 Aux val loss: 2.797 Train MAE: 12.268 Val MAE: 20.186 Epoch time: 198.157 seconds \n",
      "Epoch: 392 Train loss: 8.870 Aux train loss: 5.521 Val loss: 4.598 Aux val loss: 2.789 Train MAE: 12.397 Val MAE: 21.544 Epoch time: 197.758 seconds \n",
      "Epoch: 393 Train loss: 7.943 Aux train loss: 4.969 Val loss: 4.629 Aux val loss: 2.782 Train MAE: 11.406 Val MAE: 21.878 Epoch time: 198.420 seconds \n",
      "Epoch: 394 Train loss: 8.578 Aux train loss: 5.352 Val loss: 4.597 Aux val loss: 2.793 Train MAE: 12.336 Val MAE: 21.260 Epoch time: 198.137 seconds \n",
      "Epoch: 395 Train loss: 8.563 Aux train loss: 5.350 Val loss: 4.641 Aux val loss: 2.795 Train MAE: 11.634 Val MAE: 21.381 Epoch time: 198.231 seconds \n",
      "Epoch: 396 Train loss: 9.162 Aux train loss: 5.709 Val loss: 4.654 Aux val loss: 2.813 Train MAE: 12.071 Val MAE: 23.298 Epoch time: 198.081 seconds \n",
      "Epoch: 397 Train loss: 8.751 Aux train loss: 5.459 Val loss: 4.611 Aux val loss: 2.778 Train MAE: 12.346 Val MAE: 22.313 Epoch time: 198.552 seconds \n",
      "Epoch: 398 Train loss: 8.779 Aux train loss: 5.469 Val loss: 4.613 Aux val loss: 2.785 Train MAE: 12.094 Val MAE: 19.390 Epoch time: 198.872 seconds \n",
      "Epoch: 399 Train loss: 8.661 Aux train loss: 5.357 Val loss: 4.640 Aux val loss: 2.804 Train MAE: 12.198 Val MAE: 21.612 Epoch time: 198.602 seconds \n",
      "Epoch: 400 Train loss: 8.522 Aux train loss: 5.339 Val loss: 4.651 Aux val loss: 2.807 Train MAE: 11.735 Val MAE: 24.680 Epoch time: 197.747 seconds \n",
      "Epoch: 401 Train loss: 8.728 Aux train loss: 5.438 Val loss: 4.621 Aux val loss: 2.788 Train MAE: 10.918 Val MAE: 22.172 Epoch time: 197.849 seconds \n",
      "Epoch: 402 Train loss: 8.799 Aux train loss: 5.480 Val loss: 4.607 Aux val loss: 2.784 Train MAE: 10.039 Val MAE: 21.192 Epoch time: 198.324 seconds \n",
      "Epoch: 403 Train loss: 8.304 Aux train loss: 5.195 Val loss: 4.578 Aux val loss: 2.767 Train MAE: 10.352 Val MAE: 20.917 Epoch time: 198.238 seconds \n",
      "Epoch: 404 Train loss: 8.793 Aux train loss: 5.489 Val loss: 4.584 Aux val loss: 2.771 Train MAE: 10.546 Val MAE: 20.136 Epoch time: 198.738 seconds \n",
      "Epoch: 405 Train loss: 8.677 Aux train loss: 5.425 Val loss: 4.597 Aux val loss: 2.778 Train MAE: 10.424 Val MAE: 20.779 Epoch time: 198.131 seconds \n",
      "Epoch: 406 Train loss: 9.292 Aux train loss: 5.800 Val loss: 4.593 Aux val loss: 2.777 Train MAE: 10.732 Val MAE: 20.858 Epoch time: 197.732 seconds \n",
      "Epoch: 407 Train loss: 8.468 Aux train loss: 5.302 Val loss: 4.595 Aux val loss: 2.776 Train MAE: 9.992 Val MAE: 21.129 Epoch time: 198.466 seconds \n",
      "Epoch: 408 Train loss: 8.929 Aux train loss: 5.582 Val loss: 4.578 Aux val loss: 2.764 Train MAE: 10.630 Val MAE: 20.572 Epoch time: 198.725 seconds \n",
      "Epoch: 409 Train loss: 8.673 Aux train loss: 5.429 Val loss: 4.579 Aux val loss: 2.770 Train MAE: 10.251 Val MAE: 20.405 Epoch time: 198.135 seconds \n",
      "Epoch: 410 Train loss: 8.293 Aux train loss: 5.183 Val loss: 4.585 Aux val loss: 2.772 Train MAE: 10.079 Val MAE: 19.978 Epoch time: 198.131 seconds \n",
      "Epoch: 411 Train loss: 8.218 Aux train loss: 5.151 Val loss: 4.578 Aux val loss: 2.763 Train MAE: 9.926 Val MAE: 20.893 Epoch time: 197.981 seconds \n",
      "Epoch: 412 Train loss: 8.570 Aux train loss: 5.353 Val loss: 4.608 Aux val loss: 2.779 Train MAE: 9.918 Val MAE: 20.294 Epoch time: 198.402 seconds \n",
      "Epoch: 413 Train loss: 8.192 Aux train loss: 5.127 Val loss: 4.575 Aux val loss: 2.764 Train MAE: 9.888 Val MAE: 20.995 Epoch time: 199.158 seconds \n",
      "Epoch: 414 Train loss: 8.114 Aux train loss: 5.088 Val loss: 4.582 Aux val loss: 2.769 Train MAE: 10.508 Val MAE: 20.686 Epoch time: 197.635 seconds \n",
      "Epoch: 415 Train loss: 8.456 Aux train loss: 5.281 Val loss: 4.570 Aux val loss: 2.760 Train MAE: 10.024 Val MAE: 20.188 Epoch time: 198.495 seconds \n",
      "Epoch: 416 Train loss: 7.890 Aux train loss: 4.944 Val loss: 4.586 Aux val loss: 2.765 Train MAE: 9.870 Val MAE: 21.326 Epoch time: 198.927 seconds \n",
      "Epoch: 417 Train loss: 9.144 Aux train loss: 5.723 Val loss: 4.599 Aux val loss: 2.781 Train MAE: 10.129 Val MAE: 20.414 Epoch time: 198.321 seconds \n",
      "Epoch: 418 Train loss: 8.719 Aux train loss: 5.449 Val loss: 4.586 Aux val loss: 2.774 Train MAE: 10.221 Val MAE: 19.475 Epoch time: 198.468 seconds \n",
      "Epoch: 419 Train loss: 8.781 Aux train loss: 5.498 Val loss: 4.586 Aux val loss: 2.769 Train MAE: 9.785 Val MAE: 20.176 Epoch time: 197.334 seconds \n",
      "Epoch: 420 Train loss: 8.474 Aux train loss: 5.293 Val loss: 4.583 Aux val loss: 2.769 Train MAE: 9.805 Val MAE: 20.110 Epoch time: 198.252 seconds \n",
      "Epoch: 421 Train loss: 8.206 Aux train loss: 5.127 Val loss: 4.600 Aux val loss: 2.775 Train MAE: 9.674 Val MAE: 20.267 Epoch time: 198.023 seconds \n",
      "Epoch: 422 Train loss: 8.414 Aux train loss: 5.265 Val loss: 4.599 Aux val loss: 2.771 Train MAE: 9.896 Val MAE: 21.240 Epoch time: 198.398 seconds \n",
      "Epoch: 423 Train loss: 8.693 Aux train loss: 5.441 Val loss: 4.602 Aux val loss: 2.774 Train MAE: 10.422 Val MAE: 23.050 Epoch time: 197.847 seconds \n",
      "Epoch: 424 Train loss: 8.355 Aux train loss: 5.227 Val loss: 4.579 Aux val loss: 2.764 Train MAE: 10.259 Val MAE: 20.467 Epoch time: 197.769 seconds \n",
      "Epoch: 425 Train loss: 9.032 Aux train loss: 5.649 Val loss: 4.575 Aux val loss: 2.764 Train MAE: 10.385 Val MAE: 20.196 Epoch time: 198.030 seconds \n",
      "Epoch: 426 Train loss: 8.236 Aux train loss: 5.144 Val loss: 4.593 Aux val loss: 2.765 Train MAE: 9.652 Val MAE: 20.533 Epoch time: 198.733 seconds \n",
      "Epoch: 427 Train loss: 8.544 Aux train loss: 5.342 Val loss: 4.590 Aux val loss: 2.775 Train MAE: 10.518 Val MAE: 20.465 Epoch time: 198.137 seconds \n",
      "Epoch: 428 Train loss: 8.208 Aux train loss: 5.156 Val loss: 4.580 Aux val loss: 2.768 Train MAE: 10.178 Val MAE: 21.202 Epoch time: 197.757 seconds \n",
      "Epoch: 429 Train loss: 8.556 Aux train loss: 5.340 Val loss: 4.585 Aux val loss: 2.771 Train MAE: 9.981 Val MAE: 20.381 Epoch time: 197.201 seconds \n",
      "Epoch: 430 Train loss: 8.377 Aux train loss: 5.240 Val loss: 4.610 Aux val loss: 2.779 Train MAE: 10.159 Val MAE: 20.914 Epoch time: 198.242 seconds \n",
      "Epoch: 431 Train loss: 8.630 Aux train loss: 5.389 Val loss: 4.591 Aux val loss: 2.769 Train MAE: 9.600 Val MAE: 20.179 Epoch time: 198.081 seconds \n",
      "Epoch: 432 Train loss: 8.174 Aux train loss: 5.120 Val loss: 4.600 Aux val loss: 2.772 Train MAE: 10.200 Val MAE: 20.085 Epoch time: 198.018 seconds \n",
      "Epoch: 433 Train loss: 8.313 Aux train loss: 5.207 Val loss: 4.599 Aux val loss: 2.776 Train MAE: 9.867 Val MAE: 21.478 Epoch time: 198.159 seconds \n",
      "Epoch: 434 Train loss: 8.762 Aux train loss: 5.466 Val loss: 4.578 Aux val loss: 2.772 Train MAE: 9.807 Val MAE: 20.032 Epoch time: 198.066 seconds \n",
      "Epoch: 435 Train loss: 8.584 Aux train loss: 5.371 Val loss: 4.571 Aux val loss: 2.761 Train MAE: 9.820 Val MAE: 20.718 Epoch time: 198.857 seconds \n",
      "Epoch: 436 Train loss: 8.398 Aux train loss: 5.252 Val loss: 4.590 Aux val loss: 2.769 Train MAE: 9.972 Val MAE: 21.327 Epoch time: 197.633 seconds \n",
      "Epoch: 437 Train loss: 8.318 Aux train loss: 5.198 Val loss: 4.590 Aux val loss: 2.778 Train MAE: 9.654 Val MAE: 20.089 Epoch time: 197.506 seconds \n",
      "Epoch: 438 Train loss: 8.698 Aux train loss: 5.442 Val loss: 4.579 Aux val loss: 2.769 Train MAE: 10.473 Val MAE: 20.390 Epoch time: 198.006 seconds \n",
      "Epoch: 439 Train loss: 7.914 Aux train loss: 4.948 Val loss: 4.588 Aux val loss: 2.768 Train MAE: 10.285 Val MAE: 21.088 Epoch time: 198.472 seconds \n",
      "Epoch: 440 Train loss: 8.242 Aux train loss: 5.145 Val loss: 4.579 Aux val loss: 2.770 Train MAE: 10.086 Val MAE: 19.573 Epoch time: 198.543 seconds \n",
      "Epoch: 441 Train loss: 8.558 Aux train loss: 5.361 Val loss: 4.586 Aux val loss: 2.770 Train MAE: 9.376 Val MAE: 20.698 Epoch time: 197.894 seconds \n",
      "Epoch: 442 Train loss: 8.728 Aux train loss: 5.470 Val loss: 4.605 Aux val loss: 2.778 Train MAE: 9.678 Val MAE: 20.313 Epoch time: 198.004 seconds \n",
      "Epoch: 443 Train loss: 8.998 Aux train loss: 5.622 Val loss: 4.590 Aux val loss: 2.771 Train MAE: 9.959 Val MAE: 19.767 Epoch time: 199.051 seconds \n",
      "Epoch: 444 Train loss: 8.844 Aux train loss: 5.524 Val loss: 4.588 Aux val loss: 2.772 Train MAE: 10.026 Val MAE: 20.547 Epoch time: 197.798 seconds \n",
      "Epoch: 445 Train loss: 8.380 Aux train loss: 5.234 Val loss: 4.599 Aux val loss: 2.776 Train MAE: 9.890 Val MAE: 21.125 Epoch time: 198.643 seconds \n",
      "Epoch: 446 Train loss: 8.799 Aux train loss: 5.510 Val loss: 4.566 Aux val loss: 2.756 Train MAE: 9.677 Val MAE: 20.439 Epoch time: 198.585 seconds \n",
      "Epoch: 447 Train loss: 8.912 Aux train loss: 5.568 Val loss: 4.588 Aux val loss: 2.769 Train MAE: 10.273 Val MAE: 20.879 Epoch time: 197.769 seconds \n",
      "Epoch: 448 Train loss: 8.278 Aux train loss: 5.187 Val loss: 4.591 Aux val loss: 2.768 Train MAE: 10.005 Val MAE: 20.814 Epoch time: 197.867 seconds \n",
      "Epoch: 449 Train loss: 8.704 Aux train loss: 5.453 Val loss: 4.581 Aux val loss: 2.766 Train MAE: 10.032 Val MAE: 20.582 Epoch time: 197.551 seconds \n",
      "Epoch: 450 Train loss: 8.650 Aux train loss: 5.404 Val loss: 4.565 Aux val loss: 2.754 Train MAE: 10.062 Val MAE: 20.681 Epoch time: 198.234 seconds \n",
      "Epoch: 451 Train loss: 8.856 Aux train loss: 5.543 Val loss: 4.578 Aux val loss: 2.763 Train MAE: 9.636 Val MAE: 20.054 Epoch time: 197.202 seconds \n",
      "Epoch: 452 Train loss: 8.231 Aux train loss: 5.150 Val loss: 4.574 Aux val loss: 2.761 Train MAE: 9.685 Val MAE: 20.646 Epoch time: 198.070 seconds \n",
      "Epoch: 453 Train loss: 8.365 Aux train loss: 5.229 Val loss: 4.568 Aux val loss: 2.763 Train MAE: 10.267 Val MAE: 20.485 Epoch time: 197.811 seconds \n",
      "Epoch: 454 Train loss: 8.336 Aux train loss: 5.209 Val loss: 4.586 Aux val loss: 2.766 Train MAE: 9.886 Val MAE: 20.342 Epoch time: 197.945 seconds \n",
      "Epoch: 455 Train loss: 8.801 Aux train loss: 5.509 Val loss: 4.601 Aux val loss: 2.773 Train MAE: 9.733 Val MAE: 21.723 Epoch time: 197.303 seconds \n",
      "Epoch: 456 Train loss: 8.964 Aux train loss: 5.631 Val loss: 4.585 Aux val loss: 2.768 Train MAE: 10.185 Val MAE: 21.729 Epoch time: 198.302 seconds \n",
      "Epoch: 457 Train loss: 8.710 Aux train loss: 5.440 Val loss: 4.581 Aux val loss: 2.764 Train MAE: 10.299 Val MAE: 21.317 Epoch time: 198.022 seconds \n",
      "Epoch: 458 Train loss: 8.437 Aux train loss: 5.295 Val loss: 4.593 Aux val loss: 2.775 Train MAE: 9.757 Val MAE: 20.975 Epoch time: 197.796 seconds \n",
      "Epoch: 459 Train loss: 7.830 Aux train loss: 4.905 Val loss: 4.581 Aux val loss: 2.767 Train MAE: 9.483 Val MAE: 20.229 Epoch time: 197.523 seconds \n",
      "Epoch: 460 Train loss: 8.644 Aux train loss: 5.410 Val loss: 4.595 Aux val loss: 2.772 Train MAE: 9.906 Val MAE: 21.607 Epoch time: 197.475 seconds \n",
      "Epoch: 461 Train loss: 8.491 Aux train loss: 5.296 Val loss: 4.569 Aux val loss: 2.758 Train MAE: 10.237 Val MAE: 21.170 Epoch time: 197.892 seconds \n",
      "Epoch: 462 Train loss: 8.451 Aux train loss: 5.291 Val loss: 4.576 Aux val loss: 2.760 Train MAE: 9.977 Val MAE: 20.619 Epoch time: 197.432 seconds \n",
      "Epoch: 463 Train loss: 7.964 Aux train loss: 4.991 Val loss: 4.586 Aux val loss: 2.772 Train MAE: 9.813 Val MAE: 20.136 Epoch time: 197.817 seconds \n",
      "Epoch: 464 Train loss: 8.706 Aux train loss: 5.464 Val loss: 4.578 Aux val loss: 2.768 Train MAE: 10.078 Val MAE: 21.074 Epoch time: 197.655 seconds \n",
      "Epoch: 465 Train loss: 8.851 Aux train loss: 5.543 Val loss: 4.571 Aux val loss: 2.764 Train MAE: 10.311 Val MAE: 19.775 Epoch time: 197.458 seconds \n",
      "Epoch: 466 Train loss: 8.414 Aux train loss: 5.264 Val loss: 4.574 Aux val loss: 2.768 Train MAE: 10.357 Val MAE: 21.038 Epoch time: 197.997 seconds \n",
      "Epoch: 467 Train loss: 8.634 Aux train loss: 5.406 Val loss: 4.584 Aux val loss: 2.768 Train MAE: 10.119 Val MAE: 20.332 Epoch time: 198.620 seconds \n",
      "Epoch: 468 Train loss: 8.028 Aux train loss: 5.035 Val loss: 4.570 Aux val loss: 2.760 Train MAE: 9.607 Val MAE: 19.546 Epoch time: 196.828 seconds \n",
      "Epoch: 469 Train loss: 8.453 Aux train loss: 5.298 Val loss: 4.603 Aux val loss: 2.776 Train MAE: 9.598 Val MAE: 20.627 Epoch time: 197.682 seconds \n",
      "Epoch: 470 Train loss: 8.183 Aux train loss: 5.117 Val loss: 4.588 Aux val loss: 2.768 Train MAE: 9.730 Val MAE: 19.941 Epoch time: 197.550 seconds \n",
      "Epoch: 471 Train loss: 8.172 Aux train loss: 5.122 Val loss: 4.587 Aux val loss: 2.776 Train MAE: 10.056 Val MAE: 21.263 Epoch time: 198.256 seconds \n",
      "Epoch: 472 Train loss: 8.283 Aux train loss: 5.182 Val loss: 4.593 Aux val loss: 2.777 Train MAE: 9.500 Val MAE: 20.217 Epoch time: 197.168 seconds \n",
      "Epoch: 473 Train loss: 8.190 Aux train loss: 5.116 Val loss: 4.576 Aux val loss: 2.762 Train MAE: 9.165 Val MAE: 20.324 Epoch time: 198.572 seconds \n",
      "Epoch: 474 Train loss: 7.969 Aux train loss: 4.997 Val loss: 4.568 Aux val loss: 2.761 Train MAE: 9.484 Val MAE: 19.904 Epoch time: 198.142 seconds \n",
      "Epoch: 475 Train loss: 8.041 Aux train loss: 5.048 Val loss: 4.577 Aux val loss: 2.766 Train MAE: 9.488 Val MAE: 20.747 Epoch time: 197.731 seconds \n",
      "Epoch: 476 Train loss: 8.261 Aux train loss: 5.170 Val loss: 4.588 Aux val loss: 2.768 Train MAE: 9.758 Val MAE: 21.101 Epoch time: 197.573 seconds \n",
      "Epoch: 477 Train loss: 8.226 Aux train loss: 5.162 Val loss: 4.592 Aux val loss: 2.766 Train MAE: 9.455 Val MAE: 20.632 Epoch time: 197.755 seconds \n",
      "Epoch: 478 Train loss: 8.669 Aux train loss: 5.445 Val loss: 4.591 Aux val loss: 2.770 Train MAE: 10.389 Val MAE: 19.749 Epoch time: 196.987 seconds \n",
      "Epoch: 479 Train loss: 8.711 Aux train loss: 5.452 Val loss: 4.582 Aux val loss: 2.766 Train MAE: 9.752 Val MAE: 19.852 Epoch time: 197.491 seconds \n",
      "Epoch: 480 Train loss: 8.688 Aux train loss: 5.429 Val loss: 4.585 Aux val loss: 2.768 Train MAE: 10.022 Val MAE: 20.496 Epoch time: 197.074 seconds \n",
      "Epoch: 481 Train loss: 8.193 Aux train loss: 5.132 Val loss: 4.598 Aux val loss: 2.771 Train MAE: 9.930 Val MAE: 23.392 Epoch time: 197.604 seconds \n",
      "Epoch: 482 Train loss: 8.474 Aux train loss: 5.311 Val loss: 4.589 Aux val loss: 2.768 Train MAE: 9.675 Val MAE: 20.751 Epoch time: 198.055 seconds \n",
      "Epoch: 483 Train loss: 7.974 Aux train loss: 5.005 Val loss: 4.580 Aux val loss: 2.772 Train MAE: 9.554 Val MAE: 20.247 Epoch time: 198.217 seconds \n",
      "Epoch: 484 Train loss: 8.343 Aux train loss: 5.229 Val loss: 4.578 Aux val loss: 2.763 Train MAE: 9.483 Val MAE: 21.213 Epoch time: 198.371 seconds \n",
      "Epoch: 485 Train loss: 8.162 Aux train loss: 5.111 Val loss: 4.576 Aux val loss: 2.759 Train MAE: 10.003 Val MAE: 20.029 Epoch time: 196.870 seconds \n",
      "Epoch: 486 Train loss: 8.629 Aux train loss: 5.428 Val loss: 4.586 Aux val loss: 2.769 Train MAE: 9.823 Val MAE: 21.334 Epoch time: 198.074 seconds \n",
      "Epoch: 487 Train loss: 8.372 Aux train loss: 5.226 Val loss: 4.557 Aux val loss: 2.756 Train MAE: 10.154 Val MAE: 20.332 Epoch time: 197.282 seconds \n",
      "Epoch: 488 Train loss: 8.723 Aux train loss: 5.447 Val loss: 4.566 Aux val loss: 2.762 Train MAE: 10.356 Val MAE: 19.694 Epoch time: 197.602 seconds \n",
      "Epoch: 489 Train loss: 8.035 Aux train loss: 5.046 Val loss: 4.581 Aux val loss: 2.766 Train MAE: 9.688 Val MAE: 21.974 Epoch time: 197.135 seconds \n",
      "Epoch: 490 Train loss: 8.534 Aux train loss: 5.339 Val loss: 4.590 Aux val loss: 2.771 Train MAE: 9.389 Val MAE: 22.264 Epoch time: 196.920 seconds \n",
      "Epoch: 491 Train loss: 8.095 Aux train loss: 5.090 Val loss: 4.573 Aux val loss: 2.761 Train MAE: 9.381 Val MAE: 19.935 Epoch time: 197.446 seconds \n",
      "Epoch: 492 Train loss: 8.528 Aux train loss: 5.345 Val loss: 4.561 Aux val loss: 2.753 Train MAE: 10.068 Val MAE: 19.701 Epoch time: 197.874 seconds \n",
      "Epoch: 493 Train loss: 8.121 Aux train loss: 5.079 Val loss: 4.589 Aux val loss: 2.770 Train MAE: 9.817 Val MAE: 20.349 Epoch time: 197.641 seconds \n",
      "Epoch: 494 Train loss: 9.045 Aux train loss: 5.662 Val loss: 4.576 Aux val loss: 2.763 Train MAE: 10.103 Val MAE: 20.652 Epoch time: 197.598 seconds \n",
      "Epoch: 495 Train loss: 7.730 Aux train loss: 4.859 Val loss: 4.600 Aux val loss: 2.772 Train MAE: 9.083 Val MAE: 21.284 Epoch time: 197.125 seconds \n",
      "Epoch: 496 Train loss: 8.901 Aux train loss: 5.588 Val loss: 4.577 Aux val loss: 2.769 Train MAE: 9.655 Val MAE: 20.184 Epoch time: 198.007 seconds \n",
      "Epoch: 497 Train loss: 8.859 Aux train loss: 5.553 Val loss: 4.587 Aux val loss: 2.764 Train MAE: 10.092 Val MAE: 20.108 Epoch time: 197.618 seconds \n",
      "Epoch: 498 Train loss: 8.408 Aux train loss: 5.259 Val loss: 4.556 Aux val loss: 2.751 Train MAE: 9.905 Val MAE: 20.752 Epoch time: 197.510 seconds \n",
      "Epoch: 499 Train loss: 8.392 Aux train loss: 5.250 Val loss: 4.552 Aux val loss: 2.748 Train MAE: 9.989 Val MAE: 19.995 Epoch time: 197.964 seconds \n",
      "Epoch: 500 Train loss: 7.807 Aux train loss: 4.900 Val loss: 4.571 Aux val loss: 2.759 Train MAE: 9.622 Val MAE: 21.340 Epoch time: 198.121 seconds \n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc_per_node=2 module11/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient17_freeze_gd --epochs=500 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01605799-a43c-4481-aae8-f932a3e414f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Val set MAE: 19.03 RMSE: 55.95\n",
      "GT count: tensor([30.,  8.], device='cuda:0')\n",
      "Predicted count: tensor([35.3365,  3.8726], device='cuda:0')\n",
      "Test set MAE: 20.03 RMSE: 89.55\n",
      "GT count: tensor([17., 34.], device='cuda:0')\n",
      "Predicted count: tensor([ 9.1079, 31.9821], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 torchrun --nproc_per_node=1 module11/evaluate.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient17_freeze_gd --epochs=500 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2b698-73dd-4a3a-bc49-d829f48df28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09136a-df0c-4c7a-868e-848ecc0b7ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0862c9a-29fc-425f-86b4-76b041fbeec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new logic to fix backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffac17f7-a69d-44ba-998b-06330c64a24e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING BACKBONE\n",
      "==================================================\n",
      "Using device: cuda\n",
      "Backbone structure:\n",
      "  patch_embed.proj: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  patch_embed.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.downsample: Identity()\n",
      "  layers_0.blocks.0.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.0.drop_path1: Identity()\n",
      "  layers_0.blocks.0.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.mlp.norm: Identity()\n",
      "  layers_0.blocks.0.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.drop_path2: Identity()\n",
      "  layers_0.blocks.1.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.1.drop_path1: DropPath(drop_prob=0.004)\n",
      "  layers_0.blocks.1.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.mlp.norm: Identity()\n",
      "  layers_0.blocks.1.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.drop_path2: DropPath(drop_prob=0.004)\n",
      "  layers_1.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  layers_1.blocks.0.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.0.drop_path1: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.0.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.mlp.norm: Identity()\n",
      "  layers_1.blocks.0.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.drop_path2: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.1.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.1.drop_path1: DropPath(drop_prob=0.013)\n",
      "  layers_1.blocks.1.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.mlp.norm: Identity()\n",
      "  layers_1.blocks.1.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.drop_path2: DropPath(drop_prob=0.013)\n",
      "  layers_2.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  layers_2.blocks.0.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.0.drop_path1: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.0.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.mlp.norm: Identity()\n",
      "  layers_2.blocks.0.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.drop_path2: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.1.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.1.drop_path1: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.1.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.mlp.norm: Identity()\n",
      "  layers_2.blocks.1.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.drop_path2: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.2.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.2.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.2.drop_path1: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.2.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.2.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.2.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.mlp.norm: Identity()\n",
      "  layers_2.blocks.2.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.drop_path2: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.3.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.3.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.3.drop_path1: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.3.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.3.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.3.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.mlp.norm: Identity()\n",
      "  layers_2.blocks.3.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.drop_path2: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.4.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.4.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.4.drop_path1: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.4.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.4.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.4.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.mlp.norm: Identity()\n",
      "  layers_2.blocks.4.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.drop_path2: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.5.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.5.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.5.drop_path1: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.5.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.5.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.5.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.mlp.norm: Identity()\n",
      "  layers_2.blocks.5.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.drop_path2: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.6.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.6.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.6.drop_path1: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.6.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.6.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.6.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.mlp.norm: Identity()\n",
      "  layers_2.blocks.6.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.drop_path2: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.7.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.7.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.7.drop_path1: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.7.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.7.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.7.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.mlp.norm: Identity()\n",
      "  layers_2.blocks.7.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.drop_path2: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.8.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.8.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.8.drop_path1: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.8.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.8.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.8.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.mlp.norm: Identity()\n",
      "  layers_2.blocks.8.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.drop_path2: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.9.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.9.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.9.drop_path1: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.9.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.9.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.9.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.mlp.norm: Identity()\n",
      "  layers_2.blocks.9.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.drop_path2: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.10.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.10.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.10.drop_path1: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.10.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.10.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.10.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.mlp.norm: Identity()\n",
      "  layers_2.blocks.10.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.drop_path2: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.11.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.11.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.11.drop_path1: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.11.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.11.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.11.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.mlp.norm: Identity()\n",
      "  layers_2.blocks.11.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.drop_path2: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.12.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.12.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.12.drop_path1: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.12.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.12.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.12.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.mlp.norm: Identity()\n",
      "  layers_2.blocks.12.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.drop_path2: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.13.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.13.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.13.drop_path1: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.13.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.13.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.13.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.mlp.norm: Identity()\n",
      "  layers_2.blocks.13.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.drop_path2: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.14.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.14.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.14.drop_path1: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.14.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.14.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.14.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.mlp.norm: Identity()\n",
      "  layers_2.blocks.14.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.drop_path2: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.15.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.15.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.15.drop_path1: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.15.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.15.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.15.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.mlp.norm: Identity()\n",
      "  layers_2.blocks.15.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.drop_path2: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.16.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.16.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.16.drop_path1: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.16.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.16.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.16.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.mlp.norm: Identity()\n",
      "  layers_2.blocks.16.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.drop_path2: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.17.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.17.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.17.drop_path1: DropPath(drop_prob=0.091)\n",
      "  layers_2.blocks.17.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.17.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.17.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.mlp.norm: Identity()\n",
      "  layers_2.blocks.17.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.drop_path2: DropPath(drop_prob=0.091)\n",
      "  layers_3.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  layers_3.blocks.0.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.0.drop_path1: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.0.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.mlp.norm: Identity()\n",
      "  layers_3.blocks.0.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.drop_path2: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.1.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.1.drop_path1: DropPath(drop_prob=0.100)\n",
      "  layers_3.blocks.1.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.mlp.norm: Identity()\n",
      "  layers_3.blocks.1.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.drop_path2: DropPath(drop_prob=0.100)\n",
      "\n",
      "Detected layer name pattern: layers_0, layers_1, layers_2, layers_3\n",
      "Creating new parameter mapping from GroundingDINO...\n",
      "/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "\n",
      "GroundingDINO Backbone Structure:\n",
      "  embeddings.patch_embeddings.projection: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  embeddings.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  embeddings.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.layernorm_before: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.drop_path: Identity()\n",
      "  encoder.layers.0.blocks.0.layernorm_after: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.0.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  encoder.layers.0.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.0.blocks.0.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.layernorm_before: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.drop_path: SwinDropPath(p=0.004347826354205608)\n",
      "  encoder.layers.0.blocks.1.layernorm_after: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.1.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  encoder.layers.0.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.0.blocks.1.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  encoder.layers.0.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.layernorm_before: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.query: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.key: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.value: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.0.attention.output.dense: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.0.drop_path: SwinDropPath(p=0.008695652708411217)\n",
      "  encoder.layers.1.blocks.0.layernorm_after: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.intermediate.dense: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  encoder.layers.1.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.1.blocks.0.output.dense: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.layernorm_before: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.query: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.key: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.value: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.attention.output.dense: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.drop_path: SwinDropPath(p=0.013043479062616825)\n",
      "  encoder.layers.1.blocks.1.layernorm_after: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.1.intermediate.dense: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  encoder.layers.1.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.1.blocks.1.output.dense: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  encoder.layers.1.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.0.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.0.drop_path: SwinDropPath(p=0.017391305416822433)\n",
      "  encoder.layers.2.blocks.0.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.0.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.drop_path: SwinDropPath(p=0.021739132702350616)\n",
      "  encoder.layers.2.blocks.1.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.1.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.1.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.drop_path: SwinDropPath(p=0.02608695812523365)\n",
      "  encoder.layers.2.blocks.2.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.2.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.2.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.2.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.drop_path: SwinDropPath(p=0.030434783548116684)\n",
      "  encoder.layers.2.blocks.3.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.3.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.3.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.3.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.drop_path: SwinDropPath(p=0.03478261083364487)\n",
      "  encoder.layers.2.blocks.4.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.4.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.4.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.4.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.drop_path: SwinDropPath(p=0.03913043811917305)\n",
      "  encoder.layers.2.blocks.5.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.5.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.5.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.5.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.drop_path: SwinDropPath(p=0.04347826540470123)\n",
      "  encoder.layers.2.blocks.6.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.6.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.6.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.6.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.drop_path: SwinDropPath(p=0.04782608896493912)\n",
      "  encoder.layers.2.blocks.7.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.7.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.7.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.7.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.drop_path: SwinDropPath(p=0.052173912525177)\n",
      "  encoder.layers.2.blocks.8.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.8.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.8.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.8.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.drop_path: SwinDropPath(p=0.056521736085414886)\n",
      "  encoder.layers.2.blocks.9.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.9.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.9.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.9.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.drop_path: SwinDropPath(p=0.06086956337094307)\n",
      "  encoder.layers.2.blocks.10.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.10.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.10.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.10.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.drop_path: SwinDropPath(p=0.06521739065647125)\n",
      "  encoder.layers.2.blocks.11.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.11.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.11.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.11.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.drop_path: SwinDropPath(p=0.06956521421670914)\n",
      "  encoder.layers.2.blocks.12.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.12.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.12.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.12.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.drop_path: SwinDropPath(p=0.07391304522752762)\n",
      "  encoder.layers.2.blocks.13.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.13.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.13.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.13.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.drop_path: SwinDropPath(p=0.0782608687877655)\n",
      "  encoder.layers.2.blocks.14.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.14.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.14.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.14.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.drop_path: SwinDropPath(p=0.08260869979858398)\n",
      "  encoder.layers.2.blocks.15.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.15.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.15.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.15.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.drop_path: SwinDropPath(p=0.08695652335882187)\n",
      "  encoder.layers.2.blocks.16.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.16.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.16.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.16.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.drop_path: SwinDropPath(p=0.09130434691905975)\n",
      "  encoder.layers.2.blocks.17.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.17.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.17.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.17.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  encoder.layers.2.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.layernorm_before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.query: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.key: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.value: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.0.attention.output.dense: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.0.drop_path: SwinDropPath(p=0.09565217792987823)\n",
      "  encoder.layers.3.blocks.0.layernorm_after: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.intermediate.dense: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  encoder.layers.3.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.3.blocks.0.output.dense: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.layernorm_before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.query: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.key: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.value: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.attention.output.dense: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.drop_path: SwinDropPath(p=0.10000000149011612)\n",
      "  encoder.layers.3.blocks.1.layernorm_after: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.1.intermediate.dense: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  encoder.layers.3.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.3.blocks.1.output.dense: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  hidden_states_norms.stage2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  hidden_states_norms.stage3: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  hidden_states_norms.stage4: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "Parameter stats sebelum mapping:\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "\n",
      "Sample TIMM keys:\n",
      "  patch_embed.proj.weight: torch.Size([128, 3, 4, 4])\n",
      "  patch_embed.proj.bias: torch.Size([128])\n",
      "  patch_embed.norm.weight: torch.Size([128])\n",
      "  patch_embed.norm.bias: torch.Size([128])\n",
      "  layers_0.blocks.0.norm1.weight: torch.Size([128])\n",
      "  layers_0.blocks.0.norm1.bias: torch.Size([128])\n",
      "  layers_0.blocks.0.attn.relative_position_bias_table: torch.Size([169, 4])\n",
      "  layers_0.blocks.0.attn.qkv.weight: torch.Size([384, 128])\n",
      "  layers_0.blocks.0.attn.qkv.bias: torch.Size([384])\n",
      "  layers_0.blocks.0.attn.proj.weight: torch.Size([128, 128])\n",
      "\n",
      "Sample GroundingDINO keys:\n",
      "  embeddings.patch_embeddings.projection.weight: torch.Size([128, 3, 4, 4])\n",
      "  embeddings.patch_embeddings.projection.bias: torch.Size([128])\n",
      "  embeddings.norm.weight: torch.Size([128])\n",
      "  embeddings.norm.bias: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.layernorm_before.weight: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.layernorm_before.bias: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.attention.self.relative_position_bias_table: torch.Size([529, 4])\n",
      "  encoder.layers.0.blocks.0.attention.self.relative_position_index: torch.Size([144, 144])\n",
      "  encoder.layers.0.blocks.0.attention.self.query.weight: torch.Size([128, 128])\n",
      "  encoder.layers.0.blocks.0.attention.self.query.bias: torch.Size([128])\n",
      "Mapped 301/325 parameters successfully\n",
      "QKV parameters: 24/24 potential matches found\n",
      "Saved mapped parameters to ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "\n",
      "Parameter stats setelah mapping:\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "Backbone parameters frozen (requires_grad=False)\n",
      "Backbone in EVAL mode\n",
      "Input tensor shape: torch.Size([2, 3, 512, 512])\n",
      "\n",
      "Testing forward_multiscale...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -4.6752, max: 4.6311, mean: 0.0011, std: 0.9995\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -9.5395, max: 15.8090, mean: 0.0544, std: 1.2382\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -71.2870, max: 386.0356, mean: 0.2688, std: 6.7572\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -123.1154, max: 56.6445, mean: -0.0206, std: 3.7044\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "\n",
      "Testing forward_concatenated...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -4.6752, max: 4.6311, mean: 0.0011, std: 0.9995\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -9.5395, max: 15.8090, mean: 0.0544, std: 1.2382\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -71.2870, max: 386.0356, mean: 0.2688, std: 6.7572\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -123.1154, max: 56.6445, mean: -0.0206, std: 3.7044\n",
      "Permuting feature dimensions from BHWC to BCHW\n",
      "Target size after reduction: (64, 64)\n",
      "Concatenated feature shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n",
      "\n",
      "==================================================\n",
      "BACKBONE TEST COMPLETE\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "TESTING BACKBONE\n",
      "==================================================\n",
      "Using device: cuda\n",
      "Backbone structure:\n",
      "  patch_embed.proj: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  patch_embed.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.downsample: Identity()\n",
      "  layers_0.blocks.0.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.0.drop_path1: Identity()\n",
      "  layers_0.blocks.0.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.mlp.norm: Identity()\n",
      "  layers_0.blocks.0.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.drop_path2: Identity()\n",
      "  layers_0.blocks.1.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.1.drop_path1: DropPath(drop_prob=0.004)\n",
      "  layers_0.blocks.1.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.mlp.norm: Identity()\n",
      "  layers_0.blocks.1.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.drop_path2: DropPath(drop_prob=0.004)\n",
      "  layers_1.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  layers_1.blocks.0.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.0.drop_path1: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.0.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.mlp.norm: Identity()\n",
      "  layers_1.blocks.0.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.drop_path2: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.1.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.1.drop_path1: DropPath(drop_prob=0.013)\n",
      "  layers_1.blocks.1.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.mlp.norm: Identity()\n",
      "  layers_1.blocks.1.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.drop_path2: DropPath(drop_prob=0.013)\n",
      "  layers_2.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  layers_2.blocks.0.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.0.drop_path1: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.0.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.mlp.norm: Identity()\n",
      "  layers_2.blocks.0.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.drop_path2: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.1.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.1.drop_path1: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.1.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.mlp.norm: Identity()\n",
      "  layers_2.blocks.1.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.drop_path2: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.2.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.2.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.2.drop_path1: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.2.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.2.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.2.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.mlp.norm: Identity()\n",
      "  layers_2.blocks.2.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.drop_path2: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.3.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.3.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.3.drop_path1: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.3.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.3.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.3.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.mlp.norm: Identity()\n",
      "  layers_2.blocks.3.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.drop_path2: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.4.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.4.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.4.drop_path1: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.4.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.4.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.4.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.mlp.norm: Identity()\n",
      "  layers_2.blocks.4.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.drop_path2: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.5.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.5.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.5.drop_path1: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.5.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.5.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.5.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.mlp.norm: Identity()\n",
      "  layers_2.blocks.5.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.drop_path2: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.6.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.6.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.6.drop_path1: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.6.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.6.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.6.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.mlp.norm: Identity()\n",
      "  layers_2.blocks.6.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.drop_path2: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.7.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.7.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.7.drop_path1: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.7.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.7.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.7.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.mlp.norm: Identity()\n",
      "  layers_2.blocks.7.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.drop_path2: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.8.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.8.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.8.drop_path1: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.8.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.8.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.8.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.mlp.norm: Identity()\n",
      "  layers_2.blocks.8.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.drop_path2: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.9.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.9.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.9.drop_path1: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.9.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.9.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.9.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.mlp.norm: Identity()\n",
      "  layers_2.blocks.9.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.drop_path2: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.10.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.10.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.10.drop_path1: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.10.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.10.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.10.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.mlp.norm: Identity()\n",
      "  layers_2.blocks.10.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.drop_path2: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.11.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.11.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.11.drop_path1: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.11.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.11.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.11.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.mlp.norm: Identity()\n",
      "  layers_2.blocks.11.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.drop_path2: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.12.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.12.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.12.drop_path1: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.12.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.12.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.12.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.mlp.norm: Identity()\n",
      "  layers_2.blocks.12.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.drop_path2: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.13.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.13.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.13.drop_path1: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.13.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.13.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.13.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.mlp.norm: Identity()\n",
      "  layers_2.blocks.13.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.drop_path2: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.14.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.14.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.14.drop_path1: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.14.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.14.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.14.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.mlp.norm: Identity()\n",
      "  layers_2.blocks.14.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.drop_path2: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.15.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.15.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.15.drop_path1: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.15.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.15.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.15.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.mlp.norm: Identity()\n",
      "  layers_2.blocks.15.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.drop_path2: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.16.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.16.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.16.drop_path1: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.16.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.16.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.16.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.mlp.norm: Identity()\n",
      "  layers_2.blocks.16.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.drop_path2: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.17.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.17.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.17.drop_path1: DropPath(drop_prob=0.091)\n",
      "  layers_2.blocks.17.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.17.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.17.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.mlp.norm: Identity()\n",
      "  layers_2.blocks.17.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.drop_path2: DropPath(drop_prob=0.091)\n",
      "  layers_3.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  layers_3.blocks.0.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.0.drop_path1: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.0.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.mlp.norm: Identity()\n",
      "  layers_3.blocks.0.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.drop_path2: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.1.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.1.drop_path1: DropPath(drop_prob=0.100)\n",
      "  layers_3.blocks.1.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.mlp.norm: Identity()\n",
      "  layers_3.blocks.1.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.drop_path2: DropPath(drop_prob=0.100)\n",
      "\n",
      "Detected layer name pattern: layers_0, layers_1, layers_2, layers_3\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "Backbone parameters set to trainable (requires_grad=True)\n",
      "Backbone in TRAIN mode\n",
      "Input tensor shape: torch.Size([2, 3, 512, 512])\n",
      "\n",
      "Testing forward_multiscale...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -5.2421, max: 4.9014, mean: 0.0012, std: 0.9997\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -9.4857, max: 15.6301, mean: 0.0540, std: 1.2457\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -98.5599, max: 474.7997, mean: 0.2645, std: 6.8325\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -122.7312, max: 59.6137, mean: -0.0197, std: 3.7409\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "\n",
      "Testing forward_concatenated...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -5.2421, max: 4.9014, mean: 0.0012, std: 0.9997\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -9.4857, max: 15.6301, mean: 0.0540, std: 1.2457\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -61.2455, max: 350.1863, mean: 0.2497, std: 6.5495\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -132.5683, max: 61.1003, mean: -0.0217, std: 3.8831\n",
      "Permuting feature dimensions from BHWC to BCHW\n",
      "Target size after reduction: (64, 64)\n",
      "Concatenated feature shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n",
      "\n",
      "Testing backward pass...\n",
      "Dummy loss: 0.06668376177549362\n",
      "Top 5 gradients by norm:\n",
      "  backbone.layers_0.blocks.1.attn.proj.weight: 0.314110\n",
      "  backbone.layers_0.blocks.0.attn.proj.weight: 0.311743\n",
      "  backbone.layers_0.blocks.0.mlp.fc2.weight: 0.268009\n",
      "  backbone.layers_0.blocks.1.mlp.fc1.weight: 0.263032\n",
      "  backbone.layers_0.blocks.1.attn.qkv.weight: 0.210039\n",
      "Optimizer step completed\n",
      "\n",
      "==================================================\n",
      "BACKBONE TEST COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!python module11/debug_backbone.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb5cbec-1189-4392-980b-cf7d3b0bb6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0320 15:17:41.202000 475015 site-packages/torch/distributed/run.py:793] \n",
      "W0320 15:17:41.202000 475015 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0320 15:17:41.202000 475015 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0320 15:17:41.202000 475015 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "0\n",
      "1\n",
      "Epoch: 1 Train loss: 23.768 Aux train loss: 10.940 Val loss: 6.478 Aux val loss: 3.846 Train MAE: 91.133 Val MAE: 55.577 Epoch time: 196.783 seconds best\n",
      "Epoch: 2 Train loss: 36.907 Aux train loss: 10.927 Val loss: 6.385 Aux val loss: 3.855 Train MAE: 61.638 Val MAE: 43.191 Epoch time: 200.824 seconds best\n",
      "Epoch: 3 Train loss: 14.906 Aux train loss: 8.986 Val loss: 6.218 Aux val loss: 3.763 Train MAE: 42.095 Val MAE: 32.505 Epoch time: 206.673 seconds best\n",
      "Epoch: 4 Train loss: 15.053 Aux train loss: 9.010 Val loss: 6.438 Aux val loss: 3.793 Train MAE: 39.850 Val MAE: 62.985 Epoch time: 207.127 seconds \n",
      "Epoch: 5 Train loss: 14.861 Aux train loss: 8.982 Val loss: 6.083 Aux val loss: 3.646 Train MAE: 37.457 Val MAE: 47.532 Epoch time: 205.535 seconds \n",
      "Epoch: 6 Train loss: 14.249 Aux train loss: 8.554 Val loss: 5.867 Aux val loss: 3.506 Train MAE: 37.769 Val MAE: 26.240 Epoch time: 206.273 seconds best\n",
      "Epoch: 7 Train loss: 13.850 Aux train loss: 8.292 Val loss: 5.651 Aux val loss: 3.466 Train MAE: 35.533 Val MAE: 22.989 Epoch time: 205.728 seconds best\n",
      "Epoch: 8 Train loss: 12.881 Aux train loss: 7.802 Val loss: 5.627 Aux val loss: 3.417 Train MAE: 30.775 Val MAE: 27.726 Epoch time: 206.079 seconds \n",
      "Epoch: 9 Train loss: 13.168 Aux train loss: 7.953 Val loss: 5.565 Aux val loss: 3.385 Train MAE: 30.705 Val MAE: 40.603 Epoch time: 206.444 seconds \n",
      "Epoch: 10 Train loss: 12.794 Aux train loss: 7.717 Val loss: 5.484 Aux val loss: 3.315 Train MAE: 30.147 Val MAE: 34.932 Epoch time: 204.257 seconds \n",
      "Epoch: 11 Train loss: 12.449 Aux train loss: 7.527 Val loss: 5.409 Aux val loss: 3.261 Train MAE: 30.761 Val MAE: 34.773 Epoch time: 205.773 seconds \n",
      "Epoch: 12 Train loss: 12.728 Aux train loss: 7.673 Val loss: 5.339 Aux val loss: 3.237 Train MAE: 31.912 Val MAE: 39.965 Epoch time: 206.467 seconds \n",
      "Epoch: 13 Train loss: 11.664 Aux train loss: 7.031 Val loss: 5.181 Aux val loss: 3.150 Train MAE: 28.958 Val MAE: 20.632 Epoch time: 205.276 seconds best\n",
      "Epoch: 14 Train loss: 12.299 Aux train loss: 7.424 Val loss: 5.449 Aux val loss: 3.243 Train MAE: 28.818 Val MAE: 45.366 Epoch time: 205.240 seconds \n",
      "Epoch: 15 Train loss: 11.232 Aux train loss: 6.798 Val loss: 5.374 Aux val loss: 3.148 Train MAE: 29.162 Val MAE: 22.872 Epoch time: 204.834 seconds \n",
      "Epoch: 16 Train loss: 11.517 Aux train loss: 6.930 Val loss: 5.087 Aux val loss: 3.063 Train MAE: 27.773 Val MAE: 25.661 Epoch time: 206.473 seconds \n",
      "Epoch: 17 Train loss: 11.405 Aux train loss: 6.884 Val loss: 5.168 Aux val loss: 3.189 Train MAE: 26.842 Val MAE: 29.096 Epoch time: 205.782 seconds \n",
      "Epoch: 18 Train loss: 11.821 Aux train loss: 7.111 Val loss: 5.074 Aux val loss: 3.063 Train MAE: 27.319 Val MAE: 21.535 Epoch time: 204.873 seconds \n",
      "Epoch: 19 Train loss: 10.305 Aux train loss: 6.229 Val loss: 5.333 Aux val loss: 3.001 Train MAE: 23.059 Val MAE: 21.940 Epoch time: 205.428 seconds \n",
      "Epoch: 20 Train loss: 11.237 Aux train loss: 6.819 Val loss: 5.174 Aux val loss: 3.084 Train MAE: 25.026 Val MAE: 32.687 Epoch time: 205.611 seconds \n",
      "Epoch: 21 Train loss: 10.461 Aux train loss: 6.364 Val loss: 5.285 Aux val loss: 3.134 Train MAE: 22.360 Val MAE: 34.007 Epoch time: 205.959 seconds \n",
      "Epoch: 22 Train loss: 10.337 Aux train loss: 6.213 Val loss: 4.760 Aux val loss: 2.893 Train MAE: 24.544 Val MAE: 23.599 Epoch time: 205.848 seconds \n",
      "Epoch: 23 Train loss: 10.630 Aux train loss: 6.437 Val loss: 4.809 Aux val loss: 2.848 Train MAE: 24.632 Val MAE: 25.151 Epoch time: 205.916 seconds \n",
      "Epoch: 24 Train loss: 10.605 Aux train loss: 6.391 Val loss: 4.811 Aux val loss: 2.917 Train MAE: 21.722 Val MAE: 21.511 Epoch time: 205.600 seconds \n",
      "Epoch: 25 Train loss: 10.707 Aux train loss: 6.138 Val loss: 4.727 Aux val loss: 2.839 Train MAE: 25.247 Val MAE: 32.896 Epoch time: 205.876 seconds \n",
      "Epoch: 26 Train loss: 10.323 Aux train loss: 6.178 Val loss: 4.742 Aux val loss: 2.826 Train MAE: 22.983 Val MAE: 32.101 Epoch time: 204.984 seconds \n",
      "Epoch: 27 Train loss: 10.788 Aux train loss: 6.485 Val loss: 4.676 Aux val loss: 2.789 Train MAE: 22.853 Val MAE: 18.245 Epoch time: 205.359 seconds best\n",
      "Epoch: 28 Train loss: 10.048 Aux train loss: 6.072 Val loss: 4.636 Aux val loss: 2.828 Train MAE: 21.132 Val MAE: 17.827 Epoch time: 205.912 seconds best\n",
      "Epoch: 29 Train loss: 10.465 Aux train loss: 6.292 Val loss: 4.728 Aux val loss: 2.837 Train MAE: 23.201 Val MAE: 34.719 Epoch time: 204.987 seconds \n",
      "Epoch: 30 Train loss: 9.344 Aux train loss: 5.600 Val loss: 4.913 Aux val loss: 2.962 Train MAE: 20.355 Val MAE: 41.306 Epoch time: 205.246 seconds \n",
      "Epoch: 31 Train loss: 9.928 Aux train loss: 6.011 Val loss: 4.579 Aux val loss: 2.777 Train MAE: 19.358 Val MAE: 20.795 Epoch time: 206.056 seconds \n",
      "Epoch: 32 Train loss: 9.820 Aux train loss: 5.952 Val loss: 4.647 Aux val loss: 2.792 Train MAE: 19.906 Val MAE: 31.066 Epoch time: 205.507 seconds \n",
      "Epoch: 33 Train loss: 11.218 Aux train loss: 5.989 Val loss: 4.651 Aux val loss: 2.780 Train MAE: 22.068 Val MAE: 20.271 Epoch time: 205.365 seconds \n",
      "Epoch: 34 Train loss: 9.474 Aux train loss: 5.688 Val loss: 4.748 Aux val loss: 2.819 Train MAE: 20.424 Val MAE: 27.575 Epoch time: 205.984 seconds \n",
      "Epoch: 35 Train loss: 10.356 Aux train loss: 7.095 Val loss: 4.576 Aux val loss: 2.801 Train MAE: 20.181 Val MAE: 20.452 Epoch time: 202.685 seconds \n",
      "Epoch: 36 Train loss: 9.793 Aux train loss: 5.926 Val loss: 4.709 Aux val loss: 2.783 Train MAE: 19.832 Val MAE: 39.194 Epoch time: 200.123 seconds \n",
      "Epoch: 37 Train loss: 9.914 Aux train loss: 5.951 Val loss: 4.691 Aux val loss: 2.768 Train MAE: 21.902 Val MAE: 24.930 Epoch time: 199.835 seconds \n",
      "Epoch: 38 Train loss: 9.859 Aux train loss: 5.937 Val loss: 4.672 Aux val loss: 2.774 Train MAE: 20.760 Val MAE: 22.372 Epoch time: 199.275 seconds \n",
      "Epoch: 39 Train loss: 10.365 Aux train loss: 6.281 Val loss: 4.465 Aux val loss: 2.709 Train MAE: 20.033 Val MAE: 16.702 Epoch time: 199.752 seconds best\n",
      "Epoch: 40 Train loss: 9.578 Aux train loss: 5.806 Val loss: 4.699 Aux val loss: 2.855 Train MAE: 18.422 Val MAE: 20.062 Epoch time: 200.299 seconds \n",
      "Epoch: 41 Train loss: 9.522 Aux train loss: 5.749 Val loss: 4.601 Aux val loss: 2.692 Train MAE: 19.202 Val MAE: 28.601 Epoch time: 200.368 seconds \n",
      "Epoch: 42 Train loss: 10.050 Aux train loss: 6.060 Val loss: 4.553 Aux val loss: 2.750 Train MAE: 21.021 Val MAE: 20.600 Epoch time: 200.495 seconds \n",
      "Epoch: 43 Train loss: 9.500 Aux train loss: 5.726 Val loss: 4.521 Aux val loss: 2.730 Train MAE: 17.856 Val MAE: 18.717 Epoch time: 199.536 seconds \n",
      "Epoch: 44 Train loss: 9.616 Aux train loss: 5.591 Val loss: 4.539 Aux val loss: 2.689 Train MAE: 21.098 Val MAE: 29.796 Epoch time: 198.592 seconds \n",
      "Epoch: 45 Train loss: 9.426 Aux train loss: 5.684 Val loss: 4.534 Aux val loss: 2.712 Train MAE: 18.616 Val MAE: 30.919 Epoch time: 198.902 seconds \n",
      "Epoch: 46 Train loss: 8.590 Aux train loss: 5.176 Val loss: 4.372 Aux val loss: 2.647 Train MAE: 17.880 Val MAE: 16.092 Epoch time: 198.848 seconds best\n",
      "Epoch: 47 Train loss: 9.545 Aux train loss: 5.805 Val loss: 4.447 Aux val loss: 2.668 Train MAE: 17.857 Val MAE: 18.202 Epoch time: 199.127 seconds \n",
      "Epoch: 48 Train loss: 9.634 Aux train loss: 5.815 Val loss: 4.458 Aux val loss: 2.686 Train MAE: 17.807 Val MAE: 21.378 Epoch time: 199.963 seconds \n",
      "Epoch: 49 Train loss: 8.764 Aux train loss: 5.293 Val loss: 4.803 Aux val loss: 2.645 Train MAE: 17.331 Val MAE: 22.388 Epoch time: 199.819 seconds \n",
      "Epoch: 50 Train loss: 8.879 Aux train loss: 5.374 Val loss: 4.399 Aux val loss: 2.696 Train MAE: 17.124 Val MAE: 18.525 Epoch time: 198.929 seconds \n",
      "Epoch: 51 Train loss: 9.069 Aux train loss: 5.465 Val loss: 4.489 Aux val loss: 2.685 Train MAE: 18.915 Val MAE: 28.065 Epoch time: 199.351 seconds \n",
      "Epoch: 52 Train loss: 9.789 Aux train loss: 5.913 Val loss: 4.321 Aux val loss: 2.638 Train MAE: 18.633 Val MAE: 19.136 Epoch time: 198.960 seconds \n",
      "Epoch: 53 Train loss: 9.231 Aux train loss: 5.594 Val loss: 4.322 Aux val loss: 2.605 Train MAE: 17.836 Val MAE: 18.468 Epoch time: 198.821 seconds \n",
      "Epoch: 54 Train loss: 8.808 Aux train loss: 5.243 Val loss: 6.471 Aux val loss: 2.590 Train MAE: 18.136 Val MAE: 21.787 Epoch time: 199.531 seconds \n",
      "Epoch: 55 Train loss: 9.079 Aux train loss: 5.508 Val loss: 4.481 Aux val loss: 2.737 Train MAE: 17.031 Val MAE: 27.151 Epoch time: 198.916 seconds \n",
      "Epoch: 56 Train loss: 8.991 Aux train loss: 5.451 Val loss: 4.379 Aux val loss: 2.619 Train MAE: 17.736 Val MAE: 29.240 Epoch time: 197.303 seconds \n",
      "Epoch: 57 Train loss: 8.678 Aux train loss: 5.242 Val loss: 4.424 Aux val loss: 2.660 Train MAE: 16.744 Val MAE: 27.504 Epoch time: 197.680 seconds \n",
      "Epoch: 58 Train loss: 9.392 Aux train loss: 5.647 Val loss: 4.379 Aux val loss: 2.637 Train MAE: 18.389 Val MAE: 29.856 Epoch time: 198.001 seconds \n",
      "Epoch: 59 Train loss: 8.834 Aux train loss: 5.366 Val loss: 4.287 Aux val loss: 2.594 Train MAE: 17.453 Val MAE: 18.068 Epoch time: 198.426 seconds \n",
      "Epoch: 60 Train loss: 9.214 Aux train loss: 5.610 Val loss: 4.328 Aux val loss: 2.585 Train MAE: 16.853 Val MAE: 23.145 Epoch time: 198.448 seconds \n",
      "Epoch: 61 Train loss: 8.426 Aux train loss: 5.105 Val loss: 4.235 Aux val loss: 2.548 Train MAE: 16.520 Val MAE: 23.455 Epoch time: 199.347 seconds \n",
      "Epoch: 62 Train loss: 8.273 Aux train loss: 5.008 Val loss: 4.235 Aux val loss: 2.594 Train MAE: 17.143 Val MAE: 20.534 Epoch time: 199.107 seconds \n",
      "Epoch: 63 Train loss: 9.238 Aux train loss: 6.110 Val loss: 4.259 Aux val loss: 2.573 Train MAE: 15.962 Val MAE: 23.778 Epoch time: 199.311 seconds \n",
      "Epoch: 64 Train loss: 8.906 Aux train loss: 5.387 Val loss: 4.404 Aux val loss: 2.624 Train MAE: 17.919 Val MAE: 23.585 Epoch time: 199.533 seconds \n",
      "Epoch: 65 Train loss: 8.926 Aux train loss: 5.392 Val loss: 4.200 Aux val loss: 2.615 Train MAE: 16.838 Val MAE: 22.154 Epoch time: 199.362 seconds \n",
      "Epoch: 66 Train loss: 8.912 Aux train loss: 5.406 Val loss: 4.288 Aux val loss: 2.593 Train MAE: 18.633 Val MAE: 19.192 Epoch time: 200.142 seconds \n",
      "Epoch: 67 Train loss: 8.774 Aux train loss: 5.318 Val loss: 4.224 Aux val loss: 2.603 Train MAE: 16.978 Val MAE: 18.825 Epoch time: 202.808 seconds \n",
      "Epoch: 68 Train loss: 8.673 Aux train loss: 5.263 Val loss: 4.285 Aux val loss: 2.650 Train MAE: 17.239 Val MAE: 17.511 Epoch time: 206.861 seconds \n",
      "Epoch: 69 Train loss: 9.044 Aux train loss: 5.513 Val loss: 4.247 Aux val loss: 2.576 Train MAE: 16.417 Val MAE: 23.503 Epoch time: 206.008 seconds \n",
      "Epoch: 70 Train loss: 8.737 Aux train loss: 5.299 Val loss: 4.215 Aux val loss: 2.563 Train MAE: 15.582 Val MAE: 16.843 Epoch time: 208.023 seconds \n",
      "Epoch: 71 Train loss: 8.413 Aux train loss: 5.136 Val loss: 4.317 Aux val loss: 2.520 Train MAE: 15.389 Val MAE: 22.357 Epoch time: 205.172 seconds \n",
      "Epoch: 72 Train loss: 8.658 Aux train loss: 5.226 Val loss: 4.232 Aux val loss: 2.592 Train MAE: 17.772 Val MAE: 16.726 Epoch time: 205.563 seconds \n",
      "Epoch: 73 Train loss: 8.781 Aux train loss: 5.305 Val loss: 4.305 Aux val loss: 2.565 Train MAE: 18.290 Val MAE: 22.289 Epoch time: 205.533 seconds \n",
      "Epoch: 74 Train loss: 8.971 Aux train loss: 5.424 Val loss: 4.464 Aux val loss: 2.714 Train MAE: 17.158 Val MAE: 20.669 Epoch time: 207.180 seconds \n",
      "Epoch: 75 Train loss: 8.515 Aux train loss: 5.170 Val loss: 4.238 Aux val loss: 2.590 Train MAE: 16.616 Val MAE: 17.876 Epoch time: 205.759 seconds \n",
      "Epoch: 76 Train loss: 8.251 Aux train loss: 5.019 Val loss: 4.199 Aux val loss: 2.542 Train MAE: 16.065 Val MAE: 15.669 Epoch time: 204.714 seconds best\n",
      "Epoch: 77 Train loss: 8.449 Aux train loss: 5.091 Val loss: 4.291 Aux val loss: 2.570 Train MAE: 15.286 Val MAE: 23.078 Epoch time: 205.600 seconds \n",
      "Epoch: 78 Train loss: 8.022 Aux train loss: 4.861 Val loss: 4.185 Aux val loss: 2.558 Train MAE: 16.628 Val MAE: 15.718 Epoch time: 205.653 seconds \n",
      "Epoch: 79 Train loss: 8.980 Aux train loss: 5.470 Val loss: 4.266 Aux val loss: 2.568 Train MAE: 17.956 Val MAE: 19.488 Epoch time: 206.942 seconds \n",
      "Epoch: 80 Train loss: 8.138 Aux train loss: 4.957 Val loss: 4.281 Aux val loss: 2.616 Train MAE: 15.484 Val MAE: 22.348 Epoch time: 206.810 seconds \n",
      "Epoch: 81 Train loss: 9.228 Aux train loss: 5.595 Val loss: 4.156 Aux val loss: 2.520 Train MAE: 16.006 Val MAE: 16.834 Epoch time: 204.876 seconds \n",
      "Epoch: 82 Train loss: 8.321 Aux train loss: 5.035 Val loss: 4.236 Aux val loss: 2.648 Train MAE: 15.229 Val MAE: 19.855 Epoch time: 205.357 seconds \n",
      "Epoch: 83 Train loss: 8.350 Aux train loss: 5.055 Val loss: 4.224 Aux val loss: 2.568 Train MAE: 15.602 Val MAE: 27.431 Epoch time: 207.533 seconds \n",
      "Epoch: 84 Train loss: 8.296 Aux train loss: 5.042 Val loss: 4.363 Aux val loss: 2.645 Train MAE: 14.889 Val MAE: 23.522 Epoch time: 208.699 seconds \n",
      "Epoch: 85 Train loss: 8.471 Aux train loss: 5.126 Val loss: 4.308 Aux val loss: 2.626 Train MAE: 14.722 Val MAE: 27.343 Epoch time: 207.283 seconds \n",
      "Epoch: 86 Train loss: 8.214 Aux train loss: 4.997 Val loss: 4.340 Aux val loss: 2.615 Train MAE: 15.109 Val MAE: 28.105 Epoch time: 204.733 seconds \n",
      "Epoch: 87 Train loss: 8.303 Aux train loss: 5.059 Val loss: 4.280 Aux val loss: 2.581 Train MAE: 15.734 Val MAE: 19.540 Epoch time: 206.745 seconds \n",
      "Epoch: 88 Train loss: 8.700 Aux train loss: 5.283 Val loss: 4.332 Aux val loss: 2.603 Train MAE: 15.790 Val MAE: 18.630 Epoch time: 205.155 seconds \n",
      "Epoch: 89 Train loss: 8.275 Aux train loss: 5.009 Val loss: 4.184 Aux val loss: 2.576 Train MAE: 15.223 Val MAE: 16.519 Epoch time: 205.942 seconds \n",
      "Epoch: 90 Train loss: 8.463 Aux train loss: 5.154 Val loss: 4.226 Aux val loss: 2.551 Train MAE: 15.879 Val MAE: 26.423 Epoch time: 206.307 seconds \n",
      "Epoch: 91 Train loss: 8.395 Aux train loss: 5.075 Val loss: 4.305 Aux val loss: 2.550 Train MAE: 16.297 Val MAE: 20.796 Epoch time: 206.222 seconds \n",
      "Epoch: 92 Train loss: 8.673 Aux train loss: 5.271 Val loss: 4.198 Aux val loss: 2.535 Train MAE: 15.865 Val MAE: 19.896 Epoch time: 207.580 seconds \n",
      "Epoch: 93 Train loss: 8.240 Aux train loss: 5.002 Val loss: 4.136 Aux val loss: 2.503 Train MAE: 14.365 Val MAE: 21.555 Epoch time: 207.195 seconds \n",
      "Epoch: 94 Train loss: 8.566 Aux train loss: 5.157 Val loss: 4.153 Aux val loss: 2.503 Train MAE: 16.051 Val MAE: 17.312 Epoch time: 206.028 seconds \n",
      "Epoch: 95 Train loss: 8.481 Aux train loss: 5.172 Val loss: 4.241 Aux val loss: 2.539 Train MAE: 15.125 Val MAE: 20.727 Epoch time: 205.334 seconds \n",
      "Epoch: 96 Train loss: 8.882 Aux train loss: 5.404 Val loss: 4.086 Aux val loss: 2.493 Train MAE: 15.614 Val MAE: 16.357 Epoch time: 206.604 seconds \n",
      "Epoch: 97 Train loss: 8.000 Aux train loss: 4.851 Val loss: 4.172 Aux val loss: 2.527 Train MAE: 15.483 Val MAE: 21.728 Epoch time: 205.196 seconds \n",
      "Epoch: 98 Train loss: 8.626 Aux train loss: 5.243 Val loss: 4.361 Aux val loss: 2.557 Train MAE: 15.643 Val MAE: 16.261 Epoch time: 205.571 seconds \n",
      "Epoch: 99 Train loss: 8.329 Aux train loss: 5.042 Val loss: 4.152 Aux val loss: 2.538 Train MAE: 15.324 Val MAE: 18.916 Epoch time: 206.637 seconds \n",
      "Epoch: 100 Train loss: 8.487 Aux train loss: 5.148 Val loss: 4.283 Aux val loss: 2.604 Train MAE: 15.538 Val MAE: 18.661 Epoch time: 204.843 seconds \n",
      "Epoch: 101 Train loss: 8.059 Aux train loss: 4.913 Val loss: 4.134 Aux val loss: 2.529 Train MAE: 15.182 Val MAE: 18.845 Epoch time: 201.943 seconds \n",
      "Epoch: 102 Train loss: 8.328 Aux train loss: 5.038 Val loss: 4.304 Aux val loss: 2.531 Train MAE: 14.845 Val MAE: 24.292 Epoch time: 199.361 seconds \n",
      "Epoch: 103 Train loss: 8.427 Aux train loss: 5.148 Val loss: 4.185 Aux val loss: 2.570 Train MAE: 14.854 Val MAE: 21.315 Epoch time: 200.956 seconds \n",
      "Epoch: 104 Train loss: 8.135 Aux train loss: 4.957 Val loss: 4.366 Aux val loss: 2.578 Train MAE: 15.154 Val MAE: 23.400 Epoch time: 201.322 seconds \n",
      "Epoch: 105 Train loss: 7.963 Aux train loss: 4.798 Val loss: 4.158 Aux val loss: 2.523 Train MAE: 15.428 Val MAE: 18.769 Epoch time: 200.767 seconds \n",
      "Epoch: 106 Train loss: 8.421 Aux train loss: 5.089 Val loss: 4.169 Aux val loss: 2.506 Train MAE: 15.210 Val MAE: 24.808 Epoch time: 200.176 seconds \n",
      "Epoch: 107 Train loss: 7.463 Aux train loss: 4.505 Val loss: 4.165 Aux val loss: 2.528 Train MAE: 14.126 Val MAE: 17.325 Epoch time: 200.070 seconds \n",
      "Epoch: 108 Train loss: 7.939 Aux train loss: 4.821 Val loss: 4.086 Aux val loss: 2.490 Train MAE: 15.277 Val MAE: 18.811 Epoch time: 200.557 seconds \n",
      "Epoch: 109 Train loss: 7.762 Aux train loss: 4.729 Val loss: 4.155 Aux val loss: 2.503 Train MAE: 13.695 Val MAE: 18.823 Epoch time: 200.151 seconds \n",
      "Epoch: 110 Train loss: 8.071 Aux train loss: 4.895 Val loss: 4.215 Aux val loss: 2.506 Train MAE: 15.598 Val MAE: 25.274 Epoch time: 199.338 seconds \n",
      "Epoch: 111 Train loss: 7.999 Aux train loss: 4.854 Val loss: 4.268 Aux val loss: 2.702 Train MAE: 14.438 Val MAE: 18.794 Epoch time: 199.474 seconds \n",
      "Epoch: 112 Train loss: 7.832 Aux train loss: 4.744 Val loss: 4.213 Aux val loss: 2.512 Train MAE: 13.817 Val MAE: 24.021 Epoch time: 200.314 seconds \n",
      "Epoch: 113 Train loss: 8.142 Aux train loss: 4.933 Val loss: 4.152 Aux val loss: 2.662 Train MAE: 15.345 Val MAE: 23.121 Epoch time: 199.661 seconds \n",
      "Epoch: 114 Train loss: 7.978 Aux train loss: 4.845 Val loss: 4.085 Aux val loss: 2.479 Train MAE: 15.760 Val MAE: 15.553 Epoch time: 199.725 seconds best\n",
      "Epoch: 115 Train loss: 7.799 Aux train loss: 4.723 Val loss: 4.491 Aux val loss: 2.495 Train MAE: 13.862 Val MAE: 22.125 Epoch time: 198.753 seconds \n",
      "Epoch: 116 Train loss: 7.668 Aux train loss: 4.647 Val loss: 4.072 Aux val loss: 2.499 Train MAE: 13.465 Val MAE: 15.116 Epoch time: 198.964 seconds best\n",
      "Epoch: 117 Train loss: 8.205 Aux train loss: 4.955 Val loss: 4.103 Aux val loss: 2.482 Train MAE: 15.392 Val MAE: 23.237 Epoch time: 199.758 seconds \n",
      "Epoch: 118 Train loss: 8.094 Aux train loss: 4.922 Val loss: 4.246 Aux val loss: 2.582 Train MAE: 15.113 Val MAE: 22.648 Epoch time: 199.807 seconds \n",
      "Epoch: 119 Train loss: 7.691 Aux train loss: 4.674 Val loss: 4.173 Aux val loss: 2.564 Train MAE: 14.805 Val MAE: 22.443 Epoch time: 199.520 seconds \n",
      "Epoch: 120 Train loss: 8.606 Aux train loss: 5.239 Val loss: 4.252 Aux val loss: 2.531 Train MAE: 15.117 Val MAE: 26.134 Epoch time: 198.559 seconds \n",
      "Epoch: 121 Train loss: 8.037 Aux train loss: 4.879 Val loss: 4.152 Aux val loss: 2.537 Train MAE: 15.413 Val MAE: 22.924 Epoch time: 198.702 seconds \n",
      "Epoch: 122 Train loss: 8.250 Aux train loss: 5.016 Val loss: 4.356 Aux val loss: 2.606 Train MAE: 14.186 Val MAE: 21.386 Epoch time: 197.645 seconds \n",
      "Epoch: 123 Train loss: 8.255 Aux train loss: 5.002 Val loss: 4.205 Aux val loss: 2.510 Train MAE: 16.099 Val MAE: 18.447 Epoch time: 198.492 seconds \n",
      "Epoch: 124 Train loss: 8.236 Aux train loss: 5.008 Val loss: 4.119 Aux val loss: 2.473 Train MAE: 14.806 Val MAE: 18.949 Epoch time: 197.632 seconds \n",
      "Epoch: 125 Train loss: 8.390 Aux train loss: 5.122 Val loss: 4.143 Aux val loss: 2.593 Train MAE: 15.044 Val MAE: 20.554 Epoch time: 197.758 seconds \n",
      "Epoch: 126 Train loss: 7.814 Aux train loss: 4.756 Val loss: 4.064 Aux val loss: 2.487 Train MAE: 15.109 Val MAE: 14.812 Epoch time: 197.927 seconds best\n",
      "Epoch: 127 Train loss: 7.516 Aux train loss: 4.572 Val loss: 4.304 Aux val loss: 2.476 Train MAE: 14.533 Val MAE: 23.581 Epoch time: 197.516 seconds \n",
      "Epoch: 128 Train loss: 7.911 Aux train loss: 4.826 Val loss: 4.096 Aux val loss: 2.478 Train MAE: 14.275 Val MAE: 16.388 Epoch time: 197.881 seconds \n",
      "Epoch: 129 Train loss: 7.608 Aux train loss: 4.614 Val loss: 4.064 Aux val loss: 2.477 Train MAE: 13.712 Val MAE: 17.352 Epoch time: 197.908 seconds \n",
      "Epoch: 130 Train loss: 8.355 Aux train loss: 5.073 Val loss: 4.231 Aux val loss: 2.593 Train MAE: 13.565 Val MAE: 23.972 Epoch time: 199.642 seconds \n",
      "Epoch: 131 Train loss: 7.942 Aux train loss: 4.836 Val loss: 8.458 Aux val loss: 2.470 Train MAE: 14.909 Val MAE: 20.916 Epoch time: 198.644 seconds \n",
      "Epoch: 132 Train loss: 8.172 Aux train loss: 4.947 Val loss: 4.092 Aux val loss: 2.489 Train MAE: 13.955 Val MAE: 14.743 Epoch time: 198.716 seconds best\n",
      "Epoch: 133 Train loss: 8.145 Aux train loss: 4.933 Val loss: 4.328 Aux val loss: 2.486 Train MAE: 13.863 Val MAE: 14.000 Epoch time: 198.820 seconds best\n",
      "Epoch: 134 Train loss: 8.152 Aux train loss: 4.964 Val loss: 4.201 Aux val loss: 2.515 Train MAE: 15.336 Val MAE: 20.377 Epoch time: 198.320 seconds \n",
      "Epoch: 135 Train loss: 7.835 Aux train loss: 4.751 Val loss: 4.077 Aux val loss: 2.431 Train MAE: 13.928 Val MAE: 20.848 Epoch time: 199.022 seconds \n",
      "Epoch: 136 Train loss: 7.595 Aux train loss: 4.607 Val loss: 4.155 Aux val loss: 2.497 Train MAE: 13.563 Val MAE: 18.630 Epoch time: 199.587 seconds \n",
      "Epoch: 137 Train loss: 7.547 Aux train loss: 4.569 Val loss: 4.093 Aux val loss: 2.464 Train MAE: 14.294 Val MAE: 26.134 Epoch time: 199.252 seconds \n",
      "Epoch: 138 Train loss: 7.803 Aux train loss: 4.740 Val loss: 4.079 Aux val loss: 2.455 Train MAE: 15.061 Val MAE: 25.202 Epoch time: 198.677 seconds \n",
      "Epoch: 139 Train loss: 7.516 Aux train loss: 4.594 Val loss: 4.058 Aux val loss: 2.447 Train MAE: 13.577 Val MAE: 21.386 Epoch time: 199.800 seconds \n",
      "Epoch: 140 Train loss: 7.613 Aux train loss: 4.618 Val loss: 4.191 Aux val loss: 2.493 Train MAE: 13.825 Val MAE: 23.505 Epoch time: 198.470 seconds \n",
      "Epoch: 141 Train loss: 8.015 Aux train loss: 4.884 Val loss: 4.094 Aux val loss: 2.536 Train MAE: 15.069 Val MAE: 22.338 Epoch time: 198.555 seconds \n",
      "Epoch: 142 Train loss: 7.747 Aux train loss: 4.714 Val loss: 4.086 Aux val loss: 2.475 Train MAE: 13.516 Val MAE: 16.890 Epoch time: 198.607 seconds \n",
      "Epoch: 143 Train loss: 7.974 Aux train loss: 4.871 Val loss: 4.176 Aux val loss: 2.534 Train MAE: 14.018 Val MAE: 23.669 Epoch time: 204.577 seconds \n",
      "Epoch: 144 Train loss: 7.356 Aux train loss: 4.475 Val loss: 4.366 Aux val loss: 2.641 Train MAE: 12.822 Val MAE: 30.356 Epoch time: 205.180 seconds \n",
      "Epoch: 145 Train loss: 7.807 Aux train loss: 4.731 Val loss: 4.052 Aux val loss: 2.467 Train MAE: 15.227 Val MAE: 21.400 Epoch time: 205.246 seconds \n",
      "Epoch: 146 Train loss: 7.835 Aux train loss: 4.776 Val loss: 4.045 Aux val loss: 2.458 Train MAE: 12.673 Val MAE: 16.019 Epoch time: 204.375 seconds \n",
      "Epoch: 147 Train loss: 8.008 Aux train loss: 4.839 Val loss: 4.180 Aux val loss: 2.467 Train MAE: 13.558 Val MAE: 21.449 Epoch time: 207.053 seconds \n",
      "Epoch: 148 Train loss: 7.686 Aux train loss: 4.670 Val loss: 4.054 Aux val loss: 2.462 Train MAE: 13.460 Val MAE: 18.815 Epoch time: 204.292 seconds \n",
      "Epoch: 149 Train loss: 8.007 Aux train loss: 4.842 Val loss: 4.045 Aux val loss: 2.493 Train MAE: 12.521 Val MAE: 13.770 Epoch time: 204.780 seconds best\n",
      "Epoch: 150 Train loss: 7.692 Aux train loss: 4.680 Val loss: 4.036 Aux val loss: 2.465 Train MAE: 13.138 Val MAE: 21.499 Epoch time: 204.661 seconds \n",
      "Epoch: 151 Train loss: 7.886 Aux train loss: 4.778 Val loss: 4.095 Aux val loss: 2.508 Train MAE: 12.114 Val MAE: 27.450 Epoch time: 205.048 seconds \n",
      "Epoch: 152 Train loss: 7.916 Aux train loss: 4.820 Val loss: 4.066 Aux val loss: 2.451 Train MAE: 12.865 Val MAE: 25.683 Epoch time: 205.175 seconds \n",
      "Epoch: 153 Train loss: 7.841 Aux train loss: 4.786 Val loss: 4.138 Aux val loss: 2.475 Train MAE: 13.758 Val MAE: 19.512 Epoch time: 204.837 seconds \n",
      "Epoch: 154 Train loss: 8.373 Aux train loss: 5.194 Val loss: 4.209 Aux val loss: 2.591 Train MAE: 13.482 Val MAE: 25.614 Epoch time: 203.697 seconds \n",
      "Epoch: 155 Train loss: 7.501 Aux train loss: 4.585 Val loss: 4.058 Aux val loss: 2.491 Train MAE: 12.288 Val MAE: 18.266 Epoch time: 205.100 seconds \n",
      "Epoch: 156 Train loss: 8.016 Aux train loss: 4.846 Val loss: 4.026 Aux val loss: 2.459 Train MAE: 14.511 Val MAE: 15.601 Epoch time: 206.347 seconds \n",
      "Epoch: 157 Train loss: 7.918 Aux train loss: 4.769 Val loss: 4.120 Aux val loss: 2.490 Train MAE: 14.685 Val MAE: 17.663 Epoch time: 204.966 seconds \n",
      "Epoch: 158 Train loss: 7.261 Aux train loss: 4.432 Val loss: 4.210 Aux val loss: 2.571 Train MAE: 13.091 Val MAE: 24.892 Epoch time: 209.267 seconds \n",
      "Epoch: 159 Train loss: 7.577 Aux train loss: 4.617 Val loss: 4.007 Aux val loss: 2.429 Train MAE: 12.981 Val MAE: 17.759 Epoch time: 205.671 seconds \n",
      "Epoch: 160 Train loss: 7.737 Aux train loss: 4.689 Val loss: 4.189 Aux val loss: 2.494 Train MAE: 12.751 Val MAE: 20.244 Epoch time: 205.472 seconds \n",
      "Epoch: 161 Train loss: 7.326 Aux train loss: 4.467 Val loss: 4.061 Aux val loss: 2.426 Train MAE: 12.896 Val MAE: 15.672 Epoch time: 206.840 seconds \n",
      "Epoch: 162 Train loss: 7.769 Aux train loss: 4.725 Val loss: 4.164 Aux val loss: 2.527 Train MAE: 13.443 Val MAE: 16.452 Epoch time: 204.699 seconds \n",
      "Epoch: 163 Train loss: 7.385 Aux train loss: 4.496 Val loss: 4.106 Aux val loss: 2.509 Train MAE: 15.379 Val MAE: 18.777 Epoch time: 205.222 seconds \n",
      "Epoch: 164 Train loss: 7.372 Aux train loss: 4.481 Val loss: 4.089 Aux val loss: 2.475 Train MAE: 12.960 Val MAE: 28.643 Epoch time: 204.620 seconds \n",
      "Epoch: 165 Train loss: 8.053 Aux train loss: 4.885 Val loss: 4.074 Aux val loss: 2.470 Train MAE: 13.396 Val MAE: 21.851 Epoch time: 204.094 seconds \n",
      "Epoch: 166 Train loss: 7.076 Aux train loss: 4.336 Val loss: 4.047 Aux val loss: 2.440 Train MAE: 12.506 Val MAE: 19.567 Epoch time: 205.181 seconds \n",
      "Epoch: 167 Train loss: 7.572 Aux train loss: 4.973 Val loss: 4.070 Aux val loss: 2.509 Train MAE: 12.013 Val MAE: 17.045 Epoch time: 208.158 seconds \n",
      "Epoch: 168 Train loss: 7.900 Aux train loss: 4.799 Val loss: 4.166 Aux val loss: 2.460 Train MAE: 13.556 Val MAE: 20.058 Epoch time: 206.242 seconds \n",
      "Epoch: 169 Train loss: 7.175 Aux train loss: 4.366 Val loss: 4.042 Aux val loss: 2.459 Train MAE: 12.604 Val MAE: 18.560 Epoch time: 205.860 seconds \n",
      "Epoch: 170 Train loss: 7.181 Aux train loss: 4.405 Val loss: 4.070 Aux val loss: 2.518 Train MAE: 13.145 Val MAE: 20.927 Epoch time: 205.999 seconds \n",
      "Epoch: 171 Train loss: 7.798 Aux train loss: 4.733 Val loss: 4.024 Aux val loss: 2.450 Train MAE: 13.654 Val MAE: 24.921 Epoch time: 205.100 seconds \n",
      "Epoch: 172 Train loss: 8.042 Aux train loss: 4.898 Val loss: 4.182 Aux val loss: 2.506 Train MAE: 14.032 Val MAE: 24.408 Epoch time: 207.362 seconds \n",
      "Epoch: 173 Train loss: 7.158 Aux train loss: 4.341 Val loss: 3.944 Aux val loss: 2.385 Train MAE: 13.544 Val MAE: 13.894 Epoch time: 206.174 seconds \n",
      "Epoch: 174 Train loss: 61.352 Aux train loss: 4.565 Val loss: 4.162 Aux val loss: 2.527 Train MAE: 15.940 Val MAE: 19.884 Epoch time: 205.392 seconds \n",
      "Epoch: 175 Train loss: 7.617 Aux train loss: 4.650 Val loss: 4.076 Aux val loss: 2.505 Train MAE: 12.060 Val MAE: 14.832 Epoch time: 204.778 seconds \n",
      "Epoch: 176 Train loss: 7.660 Aux train loss: 4.668 Val loss: 3.979 Aux val loss: 2.419 Train MAE: 12.043 Val MAE: 14.635 Epoch time: 204.911 seconds \n",
      "Epoch: 177 Train loss: 7.349 Aux train loss: 4.448 Val loss: 3.980 Aux val loss: 2.427 Train MAE: 12.655 Val MAE: 15.021 Epoch time: 203.200 seconds \n",
      "Epoch: 178 Train loss: 9.139 Aux train loss: 4.903 Val loss: 4.116 Aux val loss: 2.468 Train MAE: 12.857 Val MAE: 17.397 Epoch time: 199.997 seconds \n",
      "Epoch: 179 Train loss: 8.044 Aux train loss: 4.886 Val loss: 4.103 Aux val loss: 2.473 Train MAE: 13.652 Val MAE: 25.173 Epoch time: 199.502 seconds \n",
      "Epoch: 180 Train loss: 7.188 Aux train loss: 4.358 Val loss: 3.971 Aux val loss: 2.422 Train MAE: 13.475 Val MAE: 13.400 Epoch time: 199.396 seconds best\n",
      "Epoch: 181 Train loss: 7.352 Aux train loss: 4.486 Val loss: 3.991 Aux val loss: 2.454 Train MAE: 12.687 Val MAE: 14.652 Epoch time: 199.953 seconds \n",
      "Epoch: 182 Train loss: 7.499 Aux train loss: 4.558 Val loss: 3.974 Aux val loss: 2.427 Train MAE: 13.479 Val MAE: 21.224 Epoch time: 200.121 seconds \n",
      "Epoch: 183 Train loss: 7.902 Aux train loss: 4.844 Val loss: 3.983 Aux val loss: 2.422 Train MAE: 12.639 Val MAE: 16.429 Epoch time: 200.759 seconds \n",
      "Epoch: 184 Train loss: 7.616 Aux train loss: 4.630 Val loss: 4.091 Aux val loss: 2.401 Train MAE: 12.808 Val MAE: 21.852 Epoch time: 200.349 seconds \n",
      "Epoch: 185 Train loss: 7.514 Aux train loss: 4.581 Val loss: 3.978 Aux val loss: 2.407 Train MAE: 13.759 Val MAE: 16.089 Epoch time: 199.472 seconds \n",
      "Epoch: 186 Train loss: 7.608 Aux train loss: 4.639 Val loss: 3.977 Aux val loss: 2.436 Train MAE: 12.445 Val MAE: 17.180 Epoch time: 198.930 seconds \n",
      "Epoch: 187 Train loss: 8.145 Aux train loss: 4.946 Val loss: 4.090 Aux val loss: 2.438 Train MAE: 13.681 Val MAE: 25.042 Epoch time: 198.832 seconds \n",
      "Epoch: 188 Train loss: 7.687 Aux train loss: 4.686 Val loss: 4.085 Aux val loss: 2.474 Train MAE: 13.152 Val MAE: 19.600 Epoch time: 199.135 seconds \n",
      "Epoch: 189 Train loss: 7.915 Aux train loss: 4.800 Val loss: 4.068 Aux val loss: 2.445 Train MAE: 12.927 Val MAE: 16.024 Epoch time: 199.838 seconds \n",
      "Epoch: 190 Train loss: 7.571 Aux train loss: 4.636 Val loss: 4.070 Aux val loss: 2.457 Train MAE: 12.473 Val MAE: 16.003 Epoch time: 200.431 seconds \n",
      "Epoch: 191 Train loss: 7.302 Aux train loss: 4.459 Val loss: 4.108 Aux val loss: 2.448 Train MAE: 12.233 Val MAE: 18.826 Epoch time: 199.952 seconds \n",
      "Epoch: 192 Train loss: 7.594 Aux train loss: 4.642 Val loss: 4.055 Aux val loss: 2.518 Train MAE: 12.983 Val MAE: 16.885 Epoch time: 199.313 seconds \n",
      "Epoch: 193 Train loss: 8.058 Aux train loss: 4.886 Val loss: 4.097 Aux val loss: 2.479 Train MAE: 13.815 Val MAE: 24.030 Epoch time: 199.540 seconds \n",
      "Epoch: 194 Train loss: 8.011 Aux train loss: 4.862 Val loss: 4.056 Aux val loss: 2.426 Train MAE: 13.162 Val MAE: 17.436 Epoch time: 199.912 seconds \n",
      "Epoch: 195 Train loss: 7.639 Aux train loss: 4.612 Val loss: 4.147 Aux val loss: 2.439 Train MAE: 13.115 Val MAE: 25.654 Epoch time: 199.643 seconds \n",
      "Epoch: 196 Train loss: 7.826 Aux train loss: 4.794 Val loss: 4.161 Aux val loss: 2.467 Train MAE: 12.794 Val MAE: 21.903 Epoch time: 199.716 seconds \n",
      "Epoch: 197 Train loss: 7.112 Aux train loss: 4.332 Val loss: 4.102 Aux val loss: 2.454 Train MAE: 12.830 Val MAE: 24.588 Epoch time: 199.698 seconds \n",
      "Epoch: 198 Train loss: 8.139 Aux train loss: 4.951 Val loss: 4.289 Aux val loss: 2.425 Train MAE: 12.537 Val MAE: 16.724 Epoch time: 198.617 seconds \n",
      "Epoch: 199 Train loss: 8.204 Aux train loss: 4.852 Val loss: 3.951 Aux val loss: 2.399 Train MAE: 14.684 Val MAE: 18.644 Epoch time: 199.223 seconds \n",
      "Epoch: 200 Train loss: 7.168 Aux train loss: 4.360 Val loss: 4.048 Aux val loss: 2.452 Train MAE: 13.870 Val MAE: 29.524 Epoch time: 198.585 seconds \n",
      "Epoch: 201 Train loss: 6.848 Aux train loss: 4.195 Val loss: 3.856 Aux val loss: 2.354 Train MAE: 8.916 Val MAE: 14.276 Epoch time: 199.358 seconds \n",
      "Epoch: 202 Train loss: 6.689 Aux train loss: 4.085 Val loss: 3.851 Aux val loss: 2.343 Train MAE: 8.082 Val MAE: 13.899 Epoch time: 199.464 seconds \n",
      "Epoch: 203 Train loss: 6.878 Aux train loss: 4.190 Val loss: 3.855 Aux val loss: 2.344 Train MAE: 7.900 Val MAE: 15.048 Epoch time: 198.385 seconds \n",
      "Epoch: 204 Train loss: 6.791 Aux train loss: 4.150 Val loss: 3.882 Aux val loss: 2.368 Train MAE: 7.834 Val MAE: 14.802 Epoch time: 198.620 seconds \n",
      "Epoch: 205 Train loss: 6.814 Aux train loss: 4.170 Val loss: 3.880 Aux val loss: 2.374 Train MAE: 7.949 Val MAE: 15.147 Epoch time: 198.542 seconds \n",
      "Epoch: 206 Train loss: 6.593 Aux train loss: 4.027 Val loss: 3.883 Aux val loss: 2.362 Train MAE: 7.545 Val MAE: 14.311 Epoch time: 198.760 seconds \n",
      "Epoch: 207 Train loss: 7.059 Aux train loss: 4.320 Val loss: 3.850 Aux val loss: 2.340 Train MAE: 8.007 Val MAE: 14.328 Epoch time: 198.070 seconds \n",
      "Epoch: 208 Train loss: 6.557 Aux train loss: 4.013 Val loss: 3.883 Aux val loss: 2.369 Train MAE: 7.409 Val MAE: 16.060 Epoch time: 198.034 seconds \n",
      "Epoch: 209 Train loss: 6.968 Aux train loss: 4.262 Val loss: 3.934 Aux val loss: 2.376 Train MAE: 7.346 Val MAE: 16.785 Epoch time: 198.350 seconds \n",
      "Epoch: 210 Train loss: 6.801 Aux train loss: 4.155 Val loss: 3.920 Aux val loss: 2.374 Train MAE: 7.485 Val MAE: 17.831 Epoch time: 198.803 seconds \n",
      "Epoch: 211 Train loss: 6.890 Aux train loss: 4.213 Val loss: 3.883 Aux val loss: 2.361 Train MAE: 7.497 Val MAE: 16.302 Epoch time: 198.719 seconds \n",
      "Epoch: 212 Train loss: 6.643 Aux train loss: 4.061 Val loss: 3.895 Aux val loss: 2.352 Train MAE: 7.155 Val MAE: 15.338 Epoch time: 198.401 seconds \n",
      "Epoch: 213 Train loss: 6.580 Aux train loss: 4.031 Val loss: 3.943 Aux val loss: 2.371 Train MAE: 6.969 Val MAE: 16.073 Epoch time: 198.379 seconds \n",
      "Epoch: 214 Train loss: 6.710 Aux train loss: 4.094 Val loss: 3.883 Aux val loss: 2.351 Train MAE: 7.160 Val MAE: 15.073 Epoch time: 198.850 seconds \n",
      "Epoch: 215 Train loss: 6.726 Aux train loss: 4.104 Val loss: 3.880 Aux val loss: 2.345 Train MAE: 7.076 Val MAE: 17.910 Epoch time: 198.569 seconds \n",
      "Epoch: 216 Train loss: 6.351 Aux train loss: 3.898 Val loss: 3.956 Aux val loss: 2.363 Train MAE: 7.082 Val MAE: 17.517 Epoch time: 198.092 seconds \n",
      "Epoch: 217 Train loss: 6.705 Aux train loss: 4.103 Val loss: 3.873 Aux val loss: 2.348 Train MAE: 7.145 Val MAE: 15.313 Epoch time: 198.217 seconds \n",
      "Epoch: 218 Train loss: 6.359 Aux train loss: 3.900 Val loss: 3.897 Aux val loss: 2.353 Train MAE: 7.009 Val MAE: 14.642 Epoch time: 198.910 seconds \n",
      "Epoch: 219 Train loss: 6.681 Aux train loss: 4.083 Val loss: 3.854 Aux val loss: 2.335 Train MAE: 7.288 Val MAE: 14.930 Epoch time: 199.278 seconds \n",
      "Epoch: 220 Train loss: 6.871 Aux train loss: 4.195 Val loss: 3.882 Aux val loss: 2.356 Train MAE: 7.287 Val MAE: 16.204 Epoch time: 197.911 seconds \n",
      "Epoch: 221 Train loss: 6.298 Aux train loss: 3.847 Val loss: 3.888 Aux val loss: 2.354 Train MAE: 7.033 Val MAE: 14.922 Epoch time: 198.002 seconds \n",
      "Epoch: 222 Train loss: 6.774 Aux train loss: 4.137 Val loss: 3.891 Aux val loss: 2.363 Train MAE: 7.019 Val MAE: 14.970 Epoch time: 198.178 seconds \n",
      "Epoch: 223 Train loss: 7.505 Aux train loss: 4.573 Val loss: 3.898 Aux val loss: 2.349 Train MAE: 7.398 Val MAE: 15.419 Epoch time: 197.796 seconds \n",
      "Epoch: 224 Train loss: 7.086 Aux train loss: 4.327 Val loss: 3.888 Aux val loss: 2.348 Train MAE: 6.700 Val MAE: 15.641 Epoch time: 198.489 seconds \n",
      "Epoch: 225 Train loss: 6.692 Aux train loss: 4.089 Val loss: 3.884 Aux val loss: 2.349 Train MAE: 7.092 Val MAE: 15.148 Epoch time: 198.192 seconds \n",
      "Epoch: 226 Train loss: 6.985 Aux train loss: 4.270 Val loss: 3.892 Aux val loss: 2.363 Train MAE: 7.232 Val MAE: 15.379 Epoch time: 197.892 seconds \n",
      "Epoch: 227 Train loss: 6.734 Aux train loss: 4.109 Val loss: 3.872 Aux val loss: 2.341 Train MAE: 6.987 Val MAE: 16.499 Epoch time: 197.746 seconds \n",
      "Epoch: 228 Train loss: 6.805 Aux train loss: 4.160 Val loss: 3.889 Aux val loss: 2.338 Train MAE: 7.069 Val MAE: 16.112 Epoch time: 197.629 seconds \n",
      "Epoch: 229 Train loss: 7.377 Aux train loss: 4.497 Val loss: 3.869 Aux val loss: 2.338 Train MAE: 6.954 Val MAE: 15.404 Epoch time: 197.522 seconds \n",
      "Epoch: 230 Train loss: 6.815 Aux train loss: 4.159 Val loss: 3.885 Aux val loss: 2.352 Train MAE: 6.960 Val MAE: 15.297 Epoch time: 198.251 seconds \n",
      "Epoch: 231 Train loss: 6.630 Aux train loss: 4.061 Val loss: 3.855 Aux val loss: 2.336 Train MAE: 6.868 Val MAE: 14.687 Epoch time: 197.935 seconds \n",
      "Epoch: 232 Train loss: 6.205 Aux train loss: 3.796 Val loss: 3.880 Aux val loss: 2.352 Train MAE: 7.053 Val MAE: 16.132 Epoch time: 198.069 seconds \n",
      "Epoch: 233 Train loss: 6.608 Aux train loss: 4.048 Val loss: 3.910 Aux val loss: 2.349 Train MAE: 6.999 Val MAE: 16.213 Epoch time: 197.929 seconds \n",
      "Epoch: 234 Train loss: 6.675 Aux train loss: 4.084 Val loss: 3.942 Aux val loss: 2.369 Train MAE: 7.043 Val MAE: 18.518 Epoch time: 197.854 seconds \n",
      "Epoch: 235 Train loss: 6.576 Aux train loss: 4.018 Val loss: 3.908 Aux val loss: 2.341 Train MAE: 7.064 Val MAE: 17.848 Epoch time: 197.939 seconds \n",
      "Epoch: 236 Train loss: 7.465 Aux train loss: 4.550 Val loss: 3.915 Aux val loss: 2.345 Train MAE: 7.167 Val MAE: 16.428 Epoch time: 197.633 seconds \n",
      "Epoch: 237 Train loss: 6.583 Aux train loss: 4.029 Val loss: 3.907 Aux val loss: 2.340 Train MAE: 7.009 Val MAE: 16.643 Epoch time: 198.906 seconds \n",
      "Epoch: 238 Train loss: 6.526 Aux train loss: 3.996 Val loss: 3.873 Aux val loss: 2.333 Train MAE: 7.133 Val MAE: 14.868 Epoch time: 197.979 seconds \n",
      "Epoch: 239 Train loss: 7.049 Aux train loss: 4.306 Val loss: 3.863 Aux val loss: 2.334 Train MAE: 6.986 Val MAE: 15.632 Epoch time: 197.804 seconds \n",
      "Epoch: 240 Train loss: 6.901 Aux train loss: 4.226 Val loss: 3.887 Aux val loss: 2.351 Train MAE: 6.966 Val MAE: 14.820 Epoch time: 197.991 seconds \n",
      "Epoch: 241 Train loss: 6.637 Aux train loss: 4.057 Val loss: 3.892 Aux val loss: 2.360 Train MAE: 6.711 Val MAE: 14.510 Epoch time: 197.487 seconds \n",
      "Epoch: 242 Train loss: 6.318 Aux train loss: 3.858 Val loss: 3.917 Aux val loss: 2.357 Train MAE: 6.721 Val MAE: 18.419 Epoch time: 197.898 seconds \n",
      "Epoch: 243 Train loss: 6.979 Aux train loss: 4.255 Val loss: 3.865 Aux val loss: 2.344 Train MAE: 6.761 Val MAE: 13.529 Epoch time: 197.681 seconds \n",
      "Epoch: 244 Train loss: 6.703 Aux train loss: 4.106 Val loss: 3.865 Aux val loss: 2.340 Train MAE: 6.935 Val MAE: 14.376 Epoch time: 198.057 seconds \n",
      "Epoch: 245 Train loss: 7.110 Aux train loss: 4.348 Val loss: 3.942 Aux val loss: 2.400 Train MAE: 6.695 Val MAE: 18.087 Epoch time: 197.706 seconds \n",
      "Epoch: 246 Train loss: 6.280 Aux train loss: 3.839 Val loss: 3.888 Aux val loss: 2.341 Train MAE: 6.644 Val MAE: 13.996 Epoch time: 198.279 seconds \n",
      "Epoch: 247 Train loss: 7.159 Aux train loss: 4.381 Val loss: 3.934 Aux val loss: 2.371 Train MAE: 6.943 Val MAE: 17.270 Epoch time: 197.656 seconds \n",
      "Epoch: 248 Train loss: 6.572 Aux train loss: 4.011 Val loss: 3.951 Aux val loss: 2.373 Train MAE: 6.814 Val MAE: 15.951 Epoch time: 197.830 seconds \n",
      "Epoch: 249 Train loss: 6.553 Aux train loss: 4.011 Val loss: 3.919 Aux val loss: 2.360 Train MAE: 6.813 Val MAE: 16.806 Epoch time: 197.402 seconds \n",
      "Epoch: 250 Train loss: 6.543 Aux train loss: 3.999 Val loss: 3.940 Aux val loss: 2.366 Train MAE: 6.824 Val MAE: 16.094 Epoch time: 197.842 seconds \n",
      "Epoch: 251 Train loss: 6.597 Aux train loss: 4.045 Val loss: 3.926 Aux val loss: 2.376 Train MAE: 6.497 Val MAE: 18.369 Epoch time: 197.960 seconds \n",
      "Epoch: 252 Train loss: 6.079 Aux train loss: 3.732 Val loss: 3.888 Aux val loss: 2.330 Train MAE: 6.782 Val MAE: 16.285 Epoch time: 198.063 seconds \n",
      "Epoch: 253 Train loss: 6.703 Aux train loss: 4.104 Val loss: 3.906 Aux val loss: 2.354 Train MAE: 6.794 Val MAE: 15.738 Epoch time: 197.353 seconds \n",
      "Epoch: 254 Train loss: 6.665 Aux train loss: 4.068 Val loss: 3.900 Aux val loss: 2.344 Train MAE: 6.736 Val MAE: 16.374 Epoch time: 197.824 seconds \n",
      "Epoch: 255 Train loss: 6.852 Aux train loss: 4.179 Val loss: 3.872 Aux val loss: 2.344 Train MAE: 6.512 Val MAE: 16.998 Epoch time: 198.269 seconds \n",
      "Epoch: 256 Train loss: 6.241 Aux train loss: 3.819 Val loss: 3.886 Aux val loss: 2.336 Train MAE: 6.344 Val MAE: 15.846 Epoch time: 197.681 seconds \n",
      "Epoch: 257 Train loss: 6.177 Aux train loss: 3.780 Val loss: 3.878 Aux val loss: 2.340 Train MAE: 6.564 Val MAE: 17.137 Epoch time: 197.718 seconds \n",
      "Epoch: 258 Train loss: 6.055 Aux train loss: 3.707 Val loss: 3.870 Aux val loss: 2.327 Train MAE: 6.331 Val MAE: 16.723 Epoch time: 197.987 seconds \n",
      "Epoch: 259 Train loss: 6.601 Aux train loss: 4.031 Val loss: 3.881 Aux val loss: 2.327 Train MAE: 6.584 Val MAE: 15.731 Epoch time: 198.655 seconds \n",
      "Epoch: 260 Train loss: 6.535 Aux train loss: 3.995 Val loss: 3.874 Aux val loss: 2.348 Train MAE: 6.993 Val MAE: 15.799 Epoch time: 197.953 seconds \n",
      "Epoch: 261 Train loss: 6.360 Aux train loss: 3.888 Val loss: 3.899 Aux val loss: 2.349 Train MAE: 6.819 Val MAE: 16.161 Epoch time: 197.722 seconds \n",
      "Epoch: 262 Train loss: 6.125 Aux train loss: 3.750 Val loss: 3.870 Aux val loss: 2.336 Train MAE: 6.662 Val MAE: 14.515 Epoch time: 197.794 seconds \n",
      "Epoch: 263 Train loss: 6.395 Aux train loss: 3.910 Val loss: 3.858 Aux val loss: 2.326 Train MAE: 6.428 Val MAE: 14.307 Epoch time: 198.068 seconds \n",
      "Epoch: 264 Train loss: 5.962 Aux train loss: 3.639 Val loss: 3.935 Aux val loss: 2.364 Train MAE: 6.627 Val MAE: 16.977 Epoch time: 198.725 seconds \n",
      "Epoch: 265 Train loss: 6.547 Aux train loss: 3.996 Val loss: 3.951 Aux val loss: 2.373 Train MAE: 6.417 Val MAE: 18.018 Epoch time: 197.883 seconds \n",
      "Epoch: 266 Train loss: 6.324 Aux train loss: 3.836 Val loss: 3.895 Aux val loss: 2.355 Train MAE: 6.541 Val MAE: 15.812 Epoch time: 197.851 seconds \n",
      "Epoch: 267 Train loss: 6.249 Aux train loss: 3.817 Val loss: 3.862 Aux val loss: 2.341 Train MAE: 6.817 Val MAE: 14.076 Epoch time: 198.851 seconds \n",
      "Epoch: 268 Train loss: 6.006 Aux train loss: 3.665 Val loss: 3.867 Aux val loss: 2.329 Train MAE: 6.683 Val MAE: 14.243 Epoch time: 197.926 seconds \n",
      "Epoch: 269 Train loss: 6.100 Aux train loss: 3.738 Val loss: 3.889 Aux val loss: 2.339 Train MAE: 6.221 Val MAE: 15.910 Epoch time: 198.345 seconds \n",
      "Epoch: 270 Train loss: 6.474 Aux train loss: 3.957 Val loss: 3.861 Aux val loss: 2.323 Train MAE: 6.419 Val MAE: 14.373 Epoch time: 198.548 seconds \n",
      "Epoch: 271 Train loss: 6.214 Aux train loss: 3.792 Val loss: 3.860 Aux val loss: 2.342 Train MAE: 6.593 Val MAE: 14.022 Epoch time: 197.852 seconds \n",
      "Epoch: 272 Train loss: 6.868 Aux train loss: 4.190 Val loss: 3.916 Aux val loss: 2.346 Train MAE: 6.969 Val MAE: 17.394 Epoch time: 197.681 seconds \n",
      "Epoch: 273 Train loss: 6.496 Aux train loss: 3.982 Val loss: 3.866 Aux val loss: 2.343 Train MAE: 6.456 Val MAE: 15.141 Epoch time: 198.312 seconds \n",
      "Epoch: 274 Train loss: 6.562 Aux train loss: 4.010 Val loss: 3.876 Aux val loss: 2.351 Train MAE: 7.024 Val MAE: 14.772 Epoch time: 198.470 seconds \n",
      "Epoch: 275 Train loss: 6.156 Aux train loss: 3.772 Val loss: 3.868 Aux val loss: 2.318 Train MAE: 6.876 Val MAE: 18.250 Epoch time: 198.153 seconds \n",
      "Epoch: 276 Train loss: 6.559 Aux train loss: 4.009 Val loss: 3.854 Aux val loss: 2.321 Train MAE: 6.963 Val MAE: 15.765 Epoch time: 198.341 seconds \n",
      "Epoch: 277 Train loss: 6.212 Aux train loss: 3.787 Val loss: 3.902 Aux val loss: 2.332 Train MAE: 6.736 Val MAE: 15.766 Epoch time: 199.173 seconds \n",
      "Epoch: 278 Train loss: 6.727 Aux train loss: 4.116 Val loss: 3.887 Aux val loss: 2.343 Train MAE: 6.565 Val MAE: 15.543 Epoch time: 198.621 seconds \n",
      "Epoch: 279 Train loss: 6.604 Aux train loss: 4.028 Val loss: 3.907 Aux val loss: 2.362 Train MAE: 6.734 Val MAE: 15.996 Epoch time: 198.398 seconds \n",
      "Epoch: 280 Train loss: 6.162 Aux train loss: 3.771 Val loss: 3.880 Aux val loss: 2.347 Train MAE: 6.397 Val MAE: 15.663 Epoch time: 198.679 seconds \n",
      "Epoch: 281 Train loss: 6.418 Aux train loss: 3.916 Val loss: 3.861 Aux val loss: 2.340 Train MAE: 6.710 Val MAE: 14.092 Epoch time: 199.256 seconds \n",
      "Epoch: 282 Train loss: 6.121 Aux train loss: 3.737 Val loss: 3.827 Aux val loss: 2.323 Train MAE: 6.618 Val MAE: 14.819 Epoch time: 199.157 seconds \n",
      "Epoch: 283 Train loss: 6.804 Aux train loss: 4.148 Val loss: 3.890 Aux val loss: 2.339 Train MAE: 6.741 Val MAE: 14.289 Epoch time: 198.554 seconds \n",
      "Epoch: 284 Train loss: 6.524 Aux train loss: 3.986 Val loss: 3.876 Aux val loss: 2.323 Train MAE: 6.802 Val MAE: 14.377 Epoch time: 198.446 seconds \n",
      "Epoch: 285 Train loss: 6.473 Aux train loss: 3.960 Val loss: 3.889 Aux val loss: 2.357 Train MAE: 6.968 Val MAE: 14.973 Epoch time: 199.210 seconds \n",
      "Epoch: 286 Train loss: 6.553 Aux train loss: 4.003 Val loss: 3.950 Aux val loss: 2.368 Train MAE: 6.639 Val MAE: 17.404 Epoch time: 198.865 seconds \n",
      "Epoch: 287 Train loss: 5.789 Aux train loss: 3.545 Val loss: 3.882 Aux val loss: 2.334 Train MAE: 6.298 Val MAE: 14.620 Epoch time: 198.975 seconds \n",
      "Epoch: 288 Train loss: 6.387 Aux train loss: 3.903 Val loss: 3.935 Aux val loss: 2.354 Train MAE: 6.492 Val MAE: 18.064 Epoch time: 198.092 seconds \n",
      "Epoch: 289 Train loss: 6.156 Aux train loss: 3.766 Val loss: 3.902 Aux val loss: 2.335 Train MAE: 6.467 Val MAE: 15.331 Epoch time: 198.419 seconds \n",
      "Epoch: 290 Train loss: 6.576 Aux train loss: 4.025 Val loss: 3.904 Aux val loss: 2.358 Train MAE: 6.757 Val MAE: 15.726 Epoch time: 199.404 seconds \n",
      "Epoch: 291 Train loss: 5.844 Aux train loss: 3.576 Val loss: 3.945 Aux val loss: 2.386 Train MAE: 6.819 Val MAE: 18.468 Epoch time: 198.793 seconds \n",
      "Epoch: 292 Train loss: 6.257 Aux train loss: 3.832 Val loss: 3.907 Aux val loss: 2.350 Train MAE: 6.322 Val MAE: 17.450 Epoch time: 198.022 seconds \n",
      "Epoch: 293 Train loss: 6.585 Aux train loss: 4.032 Val loss: 3.895 Aux val loss: 2.339 Train MAE: 6.513 Val MAE: 14.663 Epoch time: 198.671 seconds \n",
      "Epoch: 294 Train loss: 6.188 Aux train loss: 3.789 Val loss: 3.874 Aux val loss: 2.324 Train MAE: 6.345 Val MAE: 15.698 Epoch time: 199.066 seconds \n",
      "Epoch: 295 Train loss: 6.104 Aux train loss: 3.736 Val loss: 3.915 Aux val loss: 2.350 Train MAE: 6.464 Val MAE: 15.593 Epoch time: 198.573 seconds \n",
      "Epoch: 296 Train loss: 6.362 Aux train loss: 3.890 Val loss: 3.877 Aux val loss: 2.324 Train MAE: 6.597 Val MAE: 15.705 Epoch time: 199.364 seconds \n",
      "Epoch: 297 Train loss: 5.884 Aux train loss: 3.603 Val loss: 3.904 Aux val loss: 2.348 Train MAE: 6.475 Val MAE: 16.958 Epoch time: 198.589 seconds \n",
      "Epoch: 298 Train loss: 6.836 Aux train loss: 4.181 Val loss: 3.883 Aux val loss: 2.336 Train MAE: 6.195 Val MAE: 15.575 Epoch time: 199.085 seconds \n",
      "Epoch: 299 Train loss: 6.373 Aux train loss: 3.888 Val loss: 3.899 Aux val loss: 2.340 Train MAE: 6.273 Val MAE: 15.927 Epoch time: 198.906 seconds \n",
      "Epoch: 300 Train loss: 6.372 Aux train loss: 3.912 Val loss: 3.872 Aux val loss: 2.315 Train MAE: 6.522 Val MAE: 15.002 Epoch time: 197.910 seconds \n",
      "Epoch: 301 Train loss: 6.754 Aux train loss: 4.131 Val loss: 3.837 Aux val loss: 2.313 Train MAE: 6.413 Val MAE: 14.262 Epoch time: 198.361 seconds \n",
      "Epoch: 302 Train loss: 6.328 Aux train loss: 3.876 Val loss: 3.881 Aux val loss: 2.337 Train MAE: 6.454 Val MAE: 17.899 Epoch time: 198.839 seconds \n",
      "Epoch: 303 Train loss: 6.372 Aux train loss: 3.895 Val loss: 3.888 Aux val loss: 2.321 Train MAE: 6.195 Val MAE: 14.050 Epoch time: 199.018 seconds \n",
      "Epoch: 304 Train loss: 5.780 Aux train loss: 3.538 Val loss: 3.901 Aux val loss: 2.334 Train MAE: 6.215 Val MAE: 16.381 Epoch time: 198.502 seconds \n",
      "Epoch: 305 Train loss: 6.874 Aux train loss: 4.192 Val loss: 3.946 Aux val loss: 2.369 Train MAE: 6.625 Val MAE: 16.878 Epoch time: 198.794 seconds \n",
      "Epoch: 306 Train loss: 6.123 Aux train loss: 3.748 Val loss: 3.897 Aux val loss: 2.339 Train MAE: 6.817 Val MAE: 17.023 Epoch time: 198.639 seconds \n",
      "Epoch: 307 Train loss: 6.320 Aux train loss: 3.862 Val loss: 3.909 Aux val loss: 2.351 Train MAE: 6.398 Val MAE: 18.246 Epoch time: 198.836 seconds \n",
      "Epoch: 308 Train loss: 6.531 Aux train loss: 3.989 Val loss: 3.894 Aux val loss: 2.341 Train MAE: 6.745 Val MAE: 17.479 Epoch time: 198.776 seconds \n",
      "Epoch: 309 Train loss: 6.031 Aux train loss: 3.693 Val loss: 3.894 Aux val loss: 2.341 Train MAE: 6.390 Val MAE: 16.023 Epoch time: 198.112 seconds \n",
      "Epoch: 310 Train loss: 6.099 Aux train loss: 3.732 Val loss: 3.908 Aux val loss: 2.345 Train MAE: 6.189 Val MAE: 16.099 Epoch time: 198.446 seconds \n",
      "Epoch: 311 Train loss: 5.804 Aux train loss: 3.556 Val loss: 3.900 Aux val loss: 2.340 Train MAE: 6.342 Val MAE: 16.708 Epoch time: 198.842 seconds \n",
      "Epoch: 312 Train loss: 6.317 Aux train loss: 3.863 Val loss: 3.875 Aux val loss: 2.339 Train MAE: 6.208 Val MAE: 13.972 Epoch time: 198.940 seconds \n",
      "Epoch: 313 Train loss: 6.398 Aux train loss: 3.902 Val loss: 3.916 Aux val loss: 2.342 Train MAE: 6.639 Val MAE: 13.772 Epoch time: 198.854 seconds \n",
      "Epoch: 314 Train loss: 6.460 Aux train loss: 3.954 Val loss: 3.921 Aux val loss: 2.333 Train MAE: 6.669 Val MAE: 16.850 Epoch time: 198.307 seconds \n",
      "Epoch: 315 Train loss: 6.328 Aux train loss: 3.880 Val loss: 3.937 Aux val loss: 2.355 Train MAE: 6.413 Val MAE: 15.784 Epoch time: 198.657 seconds \n",
      "Epoch: 316 Train loss: 6.420 Aux train loss: 3.926 Val loss: 3.910 Aux val loss: 2.335 Train MAE: 6.602 Val MAE: 16.494 Epoch time: 198.440 seconds \n",
      "Epoch: 317 Train loss: 6.068 Aux train loss: 3.718 Val loss: 3.918 Aux val loss: 2.377 Train MAE: 6.428 Val MAE: 17.904 Epoch time: 198.912 seconds \n",
      "Epoch: 318 Train loss: 6.428 Aux train loss: 3.924 Val loss: 3.905 Aux val loss: 2.352 Train MAE: 6.460 Val MAE: 16.347 Epoch time: 198.404 seconds \n",
      "Epoch: 319 Train loss: 6.224 Aux train loss: 3.805 Val loss: 3.855 Aux val loss: 2.334 Train MAE: 6.295 Val MAE: 15.292 Epoch time: 198.443 seconds \n",
      "Epoch: 320 Train loss: 6.095 Aux train loss: 3.736 Val loss: 3.917 Aux val loss: 2.358 Train MAE: 6.196 Val MAE: 15.773 Epoch time: 198.969 seconds \n",
      "Epoch: 321 Train loss: 6.895 Aux train loss: 4.218 Val loss: 3.877 Aux val loss: 2.335 Train MAE: 6.541 Val MAE: 14.408 Epoch time: 198.844 seconds \n",
      "Epoch: 322 Train loss: 6.219 Aux train loss: 3.799 Val loss: 3.886 Aux val loss: 2.322 Train MAE: 6.113 Val MAE: 15.752 Epoch time: 198.741 seconds \n",
      "Epoch: 323 Train loss: 5.839 Aux train loss: 3.573 Val loss: 3.930 Aux val loss: 2.357 Train MAE: 6.188 Val MAE: 16.806 Epoch time: 198.730 seconds \n",
      "Epoch: 324 Train loss: 6.461 Aux train loss: 3.943 Val loss: 3.949 Aux val loss: 2.344 Train MAE: 6.214 Val MAE: 17.233 Epoch time: 198.535 seconds \n",
      "Epoch: 325 Train loss: 6.824 Aux train loss: 4.157 Val loss: 3.898 Aux val loss: 2.349 Train MAE: 6.833 Val MAE: 15.728 Epoch time: 198.462 seconds \n",
      "Epoch: 326 Train loss: 6.576 Aux train loss: 4.018 Val loss: 3.913 Aux val loss: 2.343 Train MAE: 6.270 Val MAE: 15.284 Epoch time: 198.142 seconds \n",
      "Epoch: 327 Train loss: 6.185 Aux train loss: 3.791 Val loss: 3.852 Aux val loss: 2.311 Train MAE: 5.716 Val MAE: 15.607 Epoch time: 198.306 seconds \n",
      "Epoch: 328 Train loss: 6.381 Aux train loss: 3.901 Val loss: 3.898 Aux val loss: 2.332 Train MAE: 6.449 Val MAE: 15.378 Epoch time: 198.902 seconds \n",
      "Epoch: 329 Train loss: 6.413 Aux train loss: 3.921 Val loss: 3.902 Aux val loss: 2.322 Train MAE: 6.498 Val MAE: 15.786 Epoch time: 199.348 seconds \n",
      "Epoch: 330 Train loss: 6.504 Aux train loss: 3.984 Val loss: 3.866 Aux val loss: 2.330 Train MAE: 6.406 Val MAE: 14.406 Epoch time: 198.457 seconds \n",
      "Epoch: 331 Train loss: 6.132 Aux train loss: 3.750 Val loss: 3.913 Aux val loss: 2.334 Train MAE: 6.415 Val MAE: 17.248 Epoch time: 198.074 seconds \n",
      "Epoch: 332 Train loss: 6.706 Aux train loss: 4.104 Val loss: 3.879 Aux val loss: 2.325 Train MAE: 6.928 Val MAE: 17.509 Epoch time: 198.072 seconds \n",
      "Epoch: 333 Train loss: 6.486 Aux train loss: 3.973 Val loss: 3.879 Aux val loss: 2.332 Train MAE: 6.245 Val MAE: 14.667 Epoch time: 197.781 seconds \n",
      "Epoch: 334 Train loss: 6.307 Aux train loss: 3.853 Val loss: 3.920 Aux val loss: 2.349 Train MAE: 6.172 Val MAE: 16.553 Epoch time: 198.449 seconds \n",
      "Epoch: 335 Train loss: 5.986 Aux train loss: 3.673 Val loss: 3.919 Aux val loss: 2.348 Train MAE: 6.543 Val MAE: 16.586 Epoch time: 198.017 seconds \n",
      "Epoch: 336 Train loss: 5.834 Aux train loss: 3.571 Val loss: 3.881 Aux val loss: 2.325 Train MAE: 6.265 Val MAE: 19.285 Epoch time: 198.676 seconds \n",
      "Epoch: 337 Train loss: 6.398 Aux train loss: 3.910 Val loss: 3.885 Aux val loss: 2.333 Train MAE: 7.178 Val MAE: 14.412 Epoch time: 198.069 seconds \n",
      "Epoch: 338 Train loss: 6.666 Aux train loss: 4.076 Val loss: 3.906 Aux val loss: 2.345 Train MAE: 6.346 Val MAE: 16.397 Epoch time: 198.229 seconds \n",
      "Epoch: 339 Train loss: 6.237 Aux train loss: 3.816 Val loss: 3.920 Aux val loss: 2.348 Train MAE: 6.426 Val MAE: 16.082 Epoch time: 197.698 seconds \n",
      "Epoch: 340 Train loss: 6.992 Aux train loss: 4.275 Val loss: 3.877 Aux val loss: 2.323 Train MAE: 6.651 Val MAE: 14.687 Epoch time: 198.269 seconds \n",
      "Epoch: 341 Train loss: 6.248 Aux train loss: 3.814 Val loss: 3.925 Aux val loss: 2.341 Train MAE: 6.200 Val MAE: 18.217 Epoch time: 198.956 seconds \n",
      "Epoch: 342 Train loss: 6.211 Aux train loss: 3.788 Val loss: 3.908 Aux val loss: 2.333 Train MAE: 6.401 Val MAE: 15.715 Epoch time: 198.235 seconds \n",
      "Epoch: 343 Train loss: 6.106 Aux train loss: 3.745 Val loss: 3.910 Aux val loss: 2.340 Train MAE: 6.436 Val MAE: 18.231 Epoch time: 197.842 seconds \n",
      "Epoch: 344 Train loss: 5.916 Aux train loss: 3.622 Val loss: 3.926 Aux val loss: 2.346 Train MAE: 5.891 Val MAE: 18.125 Epoch time: 197.866 seconds \n",
      "Epoch: 345 Train loss: 6.105 Aux train loss: 3.736 Val loss: 3.869 Aux val loss: 2.323 Train MAE: 6.704 Val MAE: 17.689 Epoch time: 197.732 seconds \n",
      "Epoch: 346 Train loss: 5.943 Aux train loss: 3.637 Val loss: 3.945 Aux val loss: 2.365 Train MAE: 6.469 Val MAE: 18.532 Epoch time: 198.092 seconds \n",
      "Epoch: 347 Train loss: 6.140 Aux train loss: 3.758 Val loss: 3.894 Aux val loss: 2.341 Train MAE: 6.391 Val MAE: 14.976 Epoch time: 197.703 seconds \n",
      "Epoch: 348 Train loss: 6.516 Aux train loss: 3.981 Val loss: 3.919 Aux val loss: 2.352 Train MAE: 6.353 Val MAE: 15.857 Epoch time: 198.077 seconds \n",
      "Epoch: 349 Train loss: 5.692 Aux train loss: 3.485 Val loss: 3.891 Aux val loss: 2.339 Train MAE: 5.985 Val MAE: 15.266 Epoch time: 197.669 seconds \n",
      "Epoch: 350 Train loss: 6.348 Aux train loss: 3.866 Val loss: 3.911 Aux val loss: 2.341 Train MAE: 6.554 Val MAE: 16.469 Epoch time: 197.707 seconds \n",
      "Epoch: 351 Train loss: 5.843 Aux train loss: 3.588 Val loss: 3.893 Aux val loss: 2.336 Train MAE: 6.276 Val MAE: 15.084 Epoch time: 197.953 seconds \n",
      "Epoch: 352 Train loss: 6.471 Aux train loss: 3.957 Val loss: 3.956 Aux val loss: 2.375 Train MAE: 6.204 Val MAE: 17.816 Epoch time: 198.064 seconds \n",
      "Epoch: 353 Train loss: 6.375 Aux train loss: 3.902 Val loss: 3.880 Aux val loss: 2.330 Train MAE: 6.492 Val MAE: 14.287 Epoch time: 197.496 seconds \n",
      "Epoch: 354 Train loss: 6.192 Aux train loss: 3.786 Val loss: 3.883 Aux val loss: 2.341 Train MAE: 6.839 Val MAE: 15.841 Epoch time: 197.661 seconds \n",
      "Epoch: 355 Train loss: 5.906 Aux train loss: 3.618 Val loss: 3.876 Aux val loss: 2.344 Train MAE: 6.315 Val MAE: 18.111 Epoch time: 197.704 seconds \n",
      "Epoch: 356 Train loss: 5.920 Aux train loss: 3.626 Val loss: 3.877 Aux val loss: 2.337 Train MAE: 6.346 Val MAE: 16.221 Epoch time: 198.375 seconds \n",
      "Epoch: 357 Train loss: 6.624 Aux train loss: 4.059 Val loss: 3.857 Aux val loss: 2.316 Train MAE: 6.492 Val MAE: 14.472 Epoch time: 197.487 seconds \n",
      "Epoch: 358 Train loss: 6.001 Aux train loss: 3.669 Val loss: 3.842 Aux val loss: 2.325 Train MAE: 6.210 Val MAE: 14.707 Epoch time: 198.215 seconds \n",
      "Epoch: 359 Train loss: 6.292 Aux train loss: 3.866 Val loss: 3.898 Aux val loss: 2.387 Train MAE: 6.747 Val MAE: 16.047 Epoch time: 197.906 seconds \n",
      "Epoch: 360 Train loss: 6.328 Aux train loss: 3.874 Val loss: 3.901 Aux val loss: 2.350 Train MAE: 6.132 Val MAE: 15.607 Epoch time: 198.617 seconds \n",
      "Epoch: 361 Train loss: 6.080 Aux train loss: 3.718 Val loss: 3.859 Aux val loss: 2.332 Train MAE: 6.007 Val MAE: 14.573 Epoch time: 197.669 seconds \n",
      "Epoch: 362 Train loss: 6.742 Aux train loss: 4.116 Val loss: 3.900 Aux val loss: 2.340 Train MAE: 6.529 Val MAE: 17.058 Epoch time: 197.821 seconds \n",
      "Epoch: 363 Train loss: 5.718 Aux train loss: 3.505 Val loss: 3.869 Aux val loss: 2.324 Train MAE: 6.245 Val MAE: 18.395 Epoch time: 197.690 seconds \n",
      "Epoch: 364 Train loss: 5.875 Aux train loss: 3.594 Val loss: 3.918 Aux val loss: 2.334 Train MAE: 6.692 Val MAE: 15.131 Epoch time: 198.050 seconds \n",
      "Epoch: 365 Train loss: 6.018 Aux train loss: 3.687 Val loss: 3.892 Aux val loss: 2.340 Train MAE: 6.185 Val MAE: 15.503 Epoch time: 197.914 seconds \n",
      "Epoch: 366 Train loss: 5.659 Aux train loss: 3.469 Val loss: 3.870 Aux val loss: 2.333 Train MAE: 6.221 Val MAE: 17.252 Epoch time: 198.585 seconds \n",
      "Epoch: 367 Train loss: 6.239 Aux train loss: 3.826 Val loss: 3.880 Aux val loss: 2.337 Train MAE: 6.497 Val MAE: 14.419 Epoch time: 198.347 seconds \n",
      "Epoch: 368 Train loss: 6.386 Aux train loss: 3.906 Val loss: 3.881 Aux val loss: 2.325 Train MAE: 6.391 Val MAE: 15.311 Epoch time: 197.892 seconds \n",
      "Epoch: 369 Train loss: 5.721 Aux train loss: 3.496 Val loss: 3.851 Aux val loss: 2.311 Train MAE: 6.266 Val MAE: 14.602 Epoch time: 197.909 seconds \n",
      "Epoch: 370 Train loss: 6.026 Aux train loss: 3.692 Val loss: 3.856 Aux val loss: 2.321 Train MAE: 6.633 Val MAE: 17.167 Epoch time: 197.940 seconds \n",
      "Epoch: 371 Train loss: 6.342 Aux train loss: 3.881 Val loss: 3.851 Aux val loss: 2.318 Train MAE: 6.570 Val MAE: 14.928 Epoch time: 197.672 seconds \n",
      "Epoch: 372 Train loss: 6.057 Aux train loss: 3.713 Val loss: 3.859 Aux val loss: 2.327 Train MAE: 6.510 Val MAE: 16.564 Epoch time: 198.136 seconds \n",
      "Epoch: 373 Train loss: 6.252 Aux train loss: 3.825 Val loss: 3.899 Aux val loss: 2.349 Train MAE: 6.075 Val MAE: 17.305 Epoch time: 198.202 seconds \n",
      "Epoch: 374 Train loss: 6.002 Aux train loss: 3.670 Val loss: 3.939 Aux val loss: 2.344 Train MAE: 6.403 Val MAE: 16.266 Epoch time: 197.908 seconds \n",
      "Epoch: 375 Train loss: 6.431 Aux train loss: 3.926 Val loss: 3.895 Aux val loss: 2.339 Train MAE: 6.249 Val MAE: 18.275 Epoch time: 197.850 seconds \n",
      "Epoch: 376 Train loss: 6.074 Aux train loss: 3.716 Val loss: 3.910 Aux val loss: 2.341 Train MAE: 5.732 Val MAE: 16.157 Epoch time: 197.824 seconds \n",
      "Epoch: 377 Train loss: 6.511 Aux train loss: 3.986 Val loss: 3.838 Aux val loss: 2.309 Train MAE: 6.226 Val MAE: 13.940 Epoch time: 198.100 seconds \n",
      "Epoch: 378 Train loss: 6.106 Aux train loss: 3.742 Val loss: 3.846 Aux val loss: 2.315 Train MAE: 5.934 Val MAE: 15.595 Epoch time: 197.925 seconds \n",
      "Epoch: 379 Train loss: 6.385 Aux train loss: 3.906 Val loss: 3.857 Aux val loss: 2.317 Train MAE: 6.393 Val MAE: 14.699 Epoch time: 198.283 seconds \n",
      "Epoch: 380 Train loss: 5.933 Aux train loss: 3.640 Val loss: 3.887 Aux val loss: 2.318 Train MAE: 6.071 Val MAE: 15.092 Epoch time: 198.273 seconds \n",
      "Epoch: 381 Train loss: 5.903 Aux train loss: 3.611 Val loss: 3.892 Aux val loss: 2.335 Train MAE: 6.322 Val MAE: 17.305 Epoch time: 197.932 seconds \n",
      "Epoch: 382 Train loss: 6.282 Aux train loss: 3.848 Val loss: 3.883 Aux val loss: 2.331 Train MAE: 6.725 Val MAE: 15.753 Epoch time: 197.698 seconds \n",
      "Epoch: 383 Train loss: 6.610 Aux train loss: 4.043 Val loss: 3.882 Aux val loss: 2.327 Train MAE: 6.214 Val MAE: 15.253 Epoch time: 198.293 seconds \n",
      "Epoch: 384 Train loss: 6.486 Aux train loss: 3.971 Val loss: 3.885 Aux val loss: 2.326 Train MAE: 6.340 Val MAE: 16.496 Epoch time: 198.866 seconds \n",
      "Epoch: 385 Train loss: 6.142 Aux train loss: 3.765 Val loss: 3.851 Aux val loss: 2.305 Train MAE: 6.250 Val MAE: 14.374 Epoch time: 198.269 seconds \n",
      "Epoch: 386 Train loss: 6.354 Aux train loss: 3.893 Val loss: 3.855 Aux val loss: 2.303 Train MAE: 6.220 Val MAE: 15.751 Epoch time: 198.595 seconds \n",
      "Epoch: 387 Train loss: 5.764 Aux train loss: 3.523 Val loss: 3.864 Aux val loss: 2.319 Train MAE: 6.095 Val MAE: 15.438 Epoch time: 199.114 seconds \n",
      "Epoch: 388 Train loss: 6.288 Aux train loss: 3.850 Val loss: 3.892 Aux val loss: 2.325 Train MAE: 6.494 Val MAE: 17.690 Epoch time: 199.296 seconds \n",
      "Epoch: 389 Train loss: 6.533 Aux train loss: 3.992 Val loss: 3.878 Aux val loss: 2.318 Train MAE: 6.291 Val MAE: 14.691 Epoch time: 199.515 seconds \n",
      "Epoch: 390 Train loss: 6.258 Aux train loss: 3.823 Val loss: 3.901 Aux val loss: 2.344 Train MAE: 6.309 Val MAE: 16.235 Epoch time: 199.122 seconds \n",
      "Epoch: 391 Train loss: 5.795 Aux train loss: 3.556 Val loss: 3.884 Aux val loss: 2.326 Train MAE: 6.064 Val MAE: 16.842 Epoch time: 199.972 seconds \n",
      "Epoch: 392 Train loss: 6.338 Aux train loss: 3.882 Val loss: 3.909 Aux val loss: 2.348 Train MAE: 6.328 Val MAE: 16.419 Epoch time: 198.232 seconds \n",
      "Epoch: 393 Train loss: 5.478 Aux train loss: 3.366 Val loss: 3.903 Aux val loss: 2.331 Train MAE: 6.122 Val MAE: 15.189 Epoch time: 198.566 seconds \n",
      "Epoch: 394 Train loss: 5.964 Aux train loss: 3.661 Val loss: 3.892 Aux val loss: 2.322 Train MAE: 6.119 Val MAE: 14.989 Epoch time: 198.702 seconds \n",
      "Epoch: 395 Train loss: 6.046 Aux train loss: 3.708 Val loss: 3.889 Aux val loss: 2.324 Train MAE: 5.882 Val MAE: 16.686 Epoch time: 197.952 seconds \n",
      "Epoch: 396 Train loss: 6.543 Aux train loss: 4.002 Val loss: 3.882 Aux val loss: 2.346 Train MAE: 5.940 Val MAE: 18.312 Epoch time: 198.314 seconds \n",
      "Epoch: 397 Train loss: 6.225 Aux train loss: 3.808 Val loss: 3.839 Aux val loss: 2.314 Train MAE: 6.567 Val MAE: 15.284 Epoch time: 198.154 seconds \n",
      "Epoch: 398 Train loss: 6.124 Aux train loss: 3.745 Val loss: 3.856 Aux val loss: 2.317 Train MAE: 6.538 Val MAE: 14.941 Epoch time: 198.570 seconds \n",
      "Epoch: 399 Train loss: 6.008 Aux train loss: 3.679 Val loss: 3.890 Aux val loss: 2.337 Train MAE: 6.029 Val MAE: 17.335 Epoch time: 197.861 seconds \n",
      "Epoch: 400 Train loss: 6.024 Aux train loss: 3.693 Val loss: 3.864 Aux val loss: 2.323 Train MAE: 6.283 Val MAE: 14.480 Epoch time: 197.716 seconds \n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc_per_node=2 module11/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient17_freeze_gd --epochs=400 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193df64b-94c7-415f-ae8a-3f2ccefcac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Val set MAE: 13.40 RMSE: 38.84\n",
      "GT count: tensor([30.,  8.], device='cuda:0')\n",
      "Predicted count: tensor([30.8235, 11.1631], device='cuda:0')\n",
      "Test set MAE: 15.03 RMSE: 93.09\n",
      "GT count: tensor([17., 34.], device='cuda:0')\n",
      "Predicted count: tensor([25.6420, 40.8864], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 torchrun --nproc_per_node=1 module11/evaluate.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient17_freeze_gd --epochs=400 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4dbaacf-ca19-4b50-ac9d-b6b4ed42c654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 Best MAE:\n",
      "==================================================\n",
      "Train MAE:\n",
      "Epoch     MAE            \n",
      "-------------------------\n",
      "327       5.7160         \n",
      "376       5.7324         \n",
      "395       5.8820         \n",
      "344       5.8913         \n",
      "378       5.9341         \n",
      "\n",
      "Validation MAE:\n",
      "Epoch     MAE            \n",
      "-------------------------\n",
      "180       13.3998        \n",
      "243       13.5289        \n",
      "149       13.7703        \n",
      "313       13.7715        \n",
      "173       13.8944        \n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Baca CSV\n",
    "df = pd.read_csv('checkpoints/efficient17_freeze_gd.csv')\n",
    "\n",
    "# df['Validation MAE'] = df['Validation MAE'] / 8\n",
    "\n",
    "best_train = df.nsmallest(5, 'Train MAE')[['Epoch', 'Train MAE']]\n",
    "best_val = df.nsmallest(5, 'Validation MAE')[['Epoch', 'Validation MAE']]\n",
    "\n",
    "\n",
    "# Print tabel\n",
    "print(\"\\n5 Best MAE:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Train MAE:\")\n",
    "print(f\"{'Epoch':<10}{'MAE':<15}\")\n",
    "print(\"-\" * 25)\n",
    "for _, row in best_train.iterrows():\n",
    "    print(f\"{int(row['Epoch']):<10}{row['Train MAE']:<15.4f}\")\n",
    "\n",
    "print(\"\\nValidation MAE:\")\n",
    "print(f\"{'Epoch':<10}{'MAE':<15}\")\n",
    "print(\"-\" * 25)\n",
    "for _, row in best_val.iterrows():\n",
    "    print(f\"{int(row['Epoch']):<10}{row['Validation MAE']:<15.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['Epoch'], df['Training Loss'], label='Training Loss')\n",
    "plt.plot(df['Epoch'], df['Validation Loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Progress')\n",
    "plt.show()\n",
    "\n",
    "# Plot MAE dengan best points\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['Epoch'], df['Train MAE'], label='Train MAE')\n",
    "plt.plot(df['Epoch'], df['Validation MAE'], label='Validation MAE')\n",
    "\n",
    "# Tambahkan titik merah untuk Best Epochs\n",
    "best_epochs = df[df['Best Epoch'] == 'Yes']\n",
    "plt.scatter(best_epochs['Epoch'], best_epochs['Validation MAE'], \n",
    "           color='red', s=100, label='Best Epochs', zorder=5)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('Validation MAE over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1ff31-61a0-4389-9a95-206b27554c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3facfb8-9343-42f1-9b38-d429107d836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1ab6be-87d9-48f0-a5a5-a6b9ff0d0f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0321 14:15:54.529000 997549 site-packages/torch/distributed/run.py:793] \n",
      "W0321 14:15:54.529000 997549 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0321 14:15:54.529000 997549 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0321 14:15:54.529000 997549 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "Epoch: 1 Train loss: 10.323 Aux train loss: 6.041 Val loss: 3.937 Aux val loss: 2.354 Train MAE: 73.580 Val MAE: 51.443 Epoch time: 89.518 seconds best\n",
      "Epoch: 2 Train loss: 8.641 Aux train loss: 5.183 Val loss: 3.782 Aux val loss: 2.266 Train MAE: 43.439 Val MAE: 32.260 Epoch time: 86.652 seconds best\n",
      "Epoch: 3 Train loss: 8.623 Aux train loss: 5.160 Val loss: 3.676 Aux val loss: 2.213 Train MAE: 38.141 Val MAE: 34.408 Epoch time: 88.726 seconds \n",
      "Epoch: 4 Train loss: 8.213 Aux train loss: 4.965 Val loss: 3.652 Aux val loss: 2.201 Train MAE: 37.706 Val MAE: 29.985 Epoch time: 89.778 seconds best\n",
      "Epoch: 5 Train loss: 7.760 Aux train loss: 4.630 Val loss: 3.638 Aux val loss: 2.187 Train MAE: 35.451 Val MAE: 28.982 Epoch time: 90.317 seconds best\n",
      "Epoch: 6 Train loss: 8.116 Aux train loss: 4.897 Val loss: 3.542 Aux val loss: 2.134 Train MAE: 35.658 Val MAE: 38.787 Epoch time: 90.990 seconds \n",
      "Epoch: 7 Train loss: 7.585 Aux train loss: 4.564 Val loss: 3.609 Aux val loss: 2.166 Train MAE: 30.777 Val MAE: 34.084 Epoch time: 91.589 seconds \n",
      "Epoch: 8 Train loss: 7.229 Aux train loss: 4.370 Val loss: 3.504 Aux val loss: 2.140 Train MAE: 27.797 Val MAE: 40.606 Epoch time: 92.665 seconds \n",
      "Epoch: 9 Train loss: 6.865 Aux train loss: 4.165 Val loss: 3.437 Aux val loss: 2.065 Train MAE: 28.252 Val MAE: 29.260 Epoch time: 96.748 seconds \n",
      "Epoch: 10 Train loss: 6.729 Aux train loss: 4.098 Val loss: 3.389 Aux val loss: 2.030 Train MAE: 27.526 Val MAE: 26.337 Epoch time: 94.231 seconds best\n",
      "Epoch: 11 Train loss: 6.403 Aux train loss: 3.876 Val loss: 3.316 Aux val loss: 2.007 Train MAE: 27.861 Val MAE: 28.615 Epoch time: 92.928 seconds \n",
      "Epoch: 12 Train loss: 7.081 Aux train loss: 4.285 Val loss: 3.290 Aux val loss: 1.985 Train MAE: 27.197 Val MAE: 23.717 Epoch time: 90.036 seconds best\n",
      "Epoch: 13 Train loss: 6.815 Aux train loss: 4.139 Val loss: 3.191 Aux val loss: 1.960 Train MAE: 26.272 Val MAE: 21.907 Epoch time: 90.190 seconds best\n",
      "Epoch: 14 Train loss: 5.728 Aux train loss: 3.480 Val loss: 3.202 Aux val loss: 1.915 Train MAE: 24.533 Val MAE: 23.332 Epoch time: 90.478 seconds \n",
      "Epoch: 15 Train loss: 6.254 Aux train loss: 3.792 Val loss: 3.131 Aux val loss: 1.914 Train MAE: 22.904 Val MAE: 17.261 Epoch time: 90.912 seconds best\n",
      "Epoch: 16 Train loss: 5.951 Aux train loss: 3.620 Val loss: 3.188 Aux val loss: 1.959 Train MAE: 24.867 Val MAE: 31.293 Epoch time: 88.347 seconds \n",
      "Epoch: 17 Train loss: 6.791 Aux train loss: 4.125 Val loss: 3.140 Aux val loss: 1.885 Train MAE: 22.884 Val MAE: 20.470 Epoch time: 89.969 seconds \n",
      "Epoch: 18 Train loss: 6.192 Aux train loss: 3.761 Val loss: 3.095 Aux val loss: 1.884 Train MAE: 22.002 Val MAE: 19.038 Epoch time: 92.637 seconds \n",
      "Epoch: 19 Train loss: 5.836 Aux train loss: 3.542 Val loss: 3.007 Aux val loss: 1.832 Train MAE: 23.283 Val MAE: 24.082 Epoch time: 91.860 seconds \n",
      "Epoch: 20 Train loss: 6.741 Aux train loss: 4.060 Val loss: 3.031 Aux val loss: 1.827 Train MAE: 25.294 Val MAE: 18.444 Epoch time: 93.409 seconds \n",
      "Epoch: 21 Train loss: 6.169 Aux train loss: 3.734 Val loss: 3.001 Aux val loss: 1.845 Train MAE: 24.887 Val MAE: 19.480 Epoch time: 91.853 seconds \n",
      "Epoch: 22 Train loss: 5.545 Aux train loss: 3.352 Val loss: 3.089 Aux val loss: 1.850 Train MAE: 21.278 Val MAE: 33.243 Epoch time: 92.028 seconds \n",
      "Epoch: 23 Train loss: 5.624 Aux train loss: 3.410 Val loss: 3.100 Aux val loss: 1.849 Train MAE: 23.552 Val MAE: 26.673 Epoch time: 92.157 seconds \n",
      "Epoch: 24 Train loss: 5.780 Aux train loss: 3.497 Val loss: 2.979 Aux val loss: 1.801 Train MAE: 19.184 Val MAE: 17.678 Epoch time: 92.291 seconds \n",
      "Epoch: 25 Train loss: 5.653 Aux train loss: 3.430 Val loss: 2.958 Aux val loss: 1.785 Train MAE: 20.292 Val MAE: 20.798 Epoch time: 91.501 seconds \n",
      "Epoch: 26 Train loss: 5.661 Aux train loss: 3.412 Val loss: 2.907 Aux val loss: 1.769 Train MAE: 24.991 Val MAE: 21.252 Epoch time: 92.278 seconds \n",
      "Epoch: 27 Train loss: 5.627 Aux train loss: 3.373 Val loss: 2.879 Aux val loss: 1.743 Train MAE: 21.654 Val MAE: 20.074 Epoch time: 94.283 seconds \n",
      "Epoch: 28 Train loss: 5.854 Aux train loss: 3.525 Val loss: 2.907 Aux val loss: 1.759 Train MAE: 22.543 Val MAE: 19.047 Epoch time: 95.812 seconds \n",
      "Epoch: 29 Train loss: 5.814 Aux train loss: 3.492 Val loss: 3.022 Aux val loss: 1.833 Train MAE: 20.768 Val MAE: 19.985 Epoch time: 93.341 seconds \n",
      "Epoch: 30 Train loss: 5.177 Aux train loss: 3.116 Val loss: 2.954 Aux val loss: 1.755 Train MAE: 19.549 Val MAE: 18.930 Epoch time: 93.508 seconds \n",
      "Epoch: 31 Train loss: 5.173 Aux train loss: 3.128 Val loss: 2.985 Aux val loss: 1.784 Train MAE: 18.191 Val MAE: 29.754 Epoch time: 93.099 seconds \n",
      "Epoch: 32 Train loss: 5.563 Aux train loss: 3.361 Val loss: 2.907 Aux val loss: 1.758 Train MAE: 19.805 Val MAE: 24.439 Epoch time: 94.918 seconds \n",
      "Epoch: 33 Train loss: 5.723 Aux train loss: 3.458 Val loss: 2.908 Aux val loss: 1.760 Train MAE: 18.691 Val MAE: 16.278 Epoch time: 93.747 seconds best\n",
      "Epoch: 34 Train loss: 5.585 Aux train loss: 3.405 Val loss: 2.941 Aux val loss: 1.796 Train MAE: 20.417 Val MAE: 26.098 Epoch time: 101.656 seconds \n",
      "Epoch: 35 Train loss: 5.522 Aux train loss: 3.361 Val loss: 2.879 Aux val loss: 1.747 Train MAE: 19.097 Val MAE: 19.113 Epoch time: 98.059 seconds \n",
      "Epoch: 36 Train loss: 5.341 Aux train loss: 3.240 Val loss: 2.911 Aux val loss: 1.765 Train MAE: 21.173 Val MAE: 19.708 Epoch time: 97.335 seconds \n",
      "Epoch: 37 Train loss: 5.565 Aux train loss: 3.351 Val loss: 3.057 Aux val loss: 1.816 Train MAE: 18.615 Val MAE: 30.937 Epoch time: 95.636 seconds \n",
      "Epoch: 38 Train loss: 5.658 Aux train loss: 3.436 Val loss: 2.811 Aux val loss: 1.704 Train MAE: 19.142 Val MAE: 20.339 Epoch time: 94.161 seconds \n",
      "Epoch: 39 Train loss: 5.126 Aux train loss: 3.103 Val loss: 2.826 Aux val loss: 1.709 Train MAE: 16.724 Val MAE: 20.048 Epoch time: 94.814 seconds \n",
      "Epoch: 40 Train loss: 5.619 Aux train loss: 3.400 Val loss: 2.875 Aux val loss: 1.731 Train MAE: 18.106 Val MAE: 25.326 Epoch time: 91.918 seconds \n",
      "Epoch: 41 Train loss: 5.454 Aux train loss: 3.299 Val loss: 2.862 Aux val loss: 1.723 Train MAE: 18.032 Val MAE: 20.743 Epoch time: 91.163 seconds \n",
      "Epoch: 42 Train loss: 5.092 Aux train loss: 3.069 Val loss: 2.888 Aux val loss: 1.743 Train MAE: 17.753 Val MAE: 25.559 Epoch time: 90.546 seconds \n",
      "Epoch: 43 Train loss: 5.402 Aux train loss: 3.273 Val loss: 2.873 Aux val loss: 1.720 Train MAE: 17.538 Val MAE: 25.188 Epoch time: 93.321 seconds \n",
      "Epoch: 44 Train loss: 5.489 Aux train loss: 3.336 Val loss: 2.846 Aux val loss: 1.707 Train MAE: 18.618 Val MAE: 23.310 Epoch time: 90.758 seconds \n",
      "Epoch: 45 Train loss: 4.855 Aux train loss: 2.962 Val loss: 2.840 Aux val loss: 1.704 Train MAE: 16.403 Val MAE: 17.535 Epoch time: 89.446 seconds \n",
      "Epoch: 46 Train loss: 5.554 Aux train loss: 3.356 Val loss: 2.946 Aux val loss: 1.710 Train MAE: 18.242 Val MAE: 24.562 Epoch time: 91.770 seconds \n",
      "Epoch: 47 Train loss: 5.165 Aux train loss: 3.124 Val loss: 2.837 Aux val loss: 1.707 Train MAE: 19.303 Val MAE: 22.067 Epoch time: 90.997 seconds \n",
      "Epoch: 48 Train loss: 5.043 Aux train loss: 3.040 Val loss: 2.817 Aux val loss: 1.686 Train MAE: 16.618 Val MAE: 30.606 Epoch time: 93.569 seconds \n",
      "Epoch: 49 Train loss: 4.846 Aux train loss: 2.951 Val loss: 2.780 Aux val loss: 1.683 Train MAE: 16.719 Val MAE: 17.171 Epoch time: 95.131 seconds \n",
      "Epoch: 50 Train loss: 5.527 Aux train loss: 3.348 Val loss: 2.852 Aux val loss: 1.706 Train MAE: 16.156 Val MAE: 18.872 Epoch time: 95.018 seconds \n",
      "Epoch: 51 Train loss: 5.212 Aux train loss: 3.160 Val loss: 2.897 Aux val loss: 1.748 Train MAE: 15.695 Val MAE: 18.638 Epoch time: 94.836 seconds \n",
      "Epoch: 52 Train loss: 4.817 Aux train loss: 2.935 Val loss: 2.847 Aux val loss: 1.703 Train MAE: 16.353 Val MAE: 29.429 Epoch time: 94.577 seconds \n",
      "Epoch: 53 Train loss: 5.224 Aux train loss: 3.170 Val loss: 2.930 Aux val loss: 1.745 Train MAE: 16.768 Val MAE: 33.384 Epoch time: 100.060 seconds \n",
      "Epoch: 54 Train loss: 5.019 Aux train loss: 3.017 Val loss: 2.773 Aux val loss: 1.674 Train MAE: 18.456 Val MAE: 22.924 Epoch time: 95.194 seconds \n",
      "Epoch: 55 Train loss: 5.182 Aux train loss: 3.139 Val loss: 2.758 Aux val loss: 1.660 Train MAE: 15.318 Val MAE: 20.270 Epoch time: 99.184 seconds \n",
      "Epoch: 56 Train loss: 5.344 Aux train loss: 3.244 Val loss: 2.815 Aux val loss: 1.721 Train MAE: 14.899 Val MAE: 15.772 Epoch time: 90.985 seconds best\n",
      "Epoch: 57 Train loss: 5.011 Aux train loss: 3.038 Val loss: 2.771 Aux val loss: 1.666 Train MAE: 16.375 Val MAE: 22.658 Epoch time: 96.237 seconds \n",
      "Epoch: 58 Train loss: 5.167 Aux train loss: 3.124 Val loss: 2.867 Aux val loss: 1.681 Train MAE: 16.797 Val MAE: 25.421 Epoch time: 95.207 seconds \n",
      "Epoch: 59 Train loss: 4.887 Aux train loss: 2.949 Val loss: 2.769 Aux val loss: 1.672 Train MAE: 13.783 Val MAE: 17.018 Epoch time: 95.847 seconds \n",
      "Epoch: 60 Train loss: 4.904 Aux train loss: 2.967 Val loss: 2.935 Aux val loss: 1.726 Train MAE: 15.361 Val MAE: 26.828 Epoch time: 106.976 seconds \n",
      "Epoch: 61 Train loss: 4.779 Aux train loss: 2.889 Val loss: 2.761 Aux val loss: 1.668 Train MAE: 15.561 Val MAE: 22.814 Epoch time: 99.528 seconds \n",
      "Epoch: 62 Train loss: 4.584 Aux train loss: 2.790 Val loss: 2.825 Aux val loss: 1.690 Train MAE: 14.246 Val MAE: 21.972 Epoch time: 101.636 seconds \n",
      "Epoch: 63 Train loss: 4.970 Aux train loss: 3.009 Val loss: 2.758 Aux val loss: 1.656 Train MAE: 15.532 Val MAE: 16.672 Epoch time: 102.598 seconds \n",
      "Epoch: 64 Train loss: 4.870 Aux train loss: 2.945 Val loss: 2.821 Aux val loss: 1.703 Train MAE: 15.958 Val MAE: 29.923 Epoch time: 94.414 seconds \n",
      "Epoch: 65 Train loss: 5.053 Aux train loss: 3.069 Val loss: 2.718 Aux val loss: 1.652 Train MAE: 18.916 Val MAE: 18.763 Epoch time: 100.795 seconds \n",
      "Epoch: 66 Train loss: 4.690 Aux train loss: 2.850 Val loss: 2.814 Aux val loss: 1.677 Train MAE: 15.883 Val MAE: 20.270 Epoch time: 93.029 seconds \n",
      "Epoch: 67 Train loss: 4.636 Aux train loss: 2.811 Val loss: 2.794 Aux val loss: 1.675 Train MAE: 13.753 Val MAE: 19.476 Epoch time: 94.992 seconds \n",
      "Epoch: 68 Train loss: 4.815 Aux train loss: 2.914 Val loss: 2.784 Aux val loss: 1.651 Train MAE: 15.201 Val MAE: 19.390 Epoch time: 99.546 seconds \n",
      "Epoch: 69 Train loss: 4.820 Aux train loss: 2.927 Val loss: 2.826 Aux val loss: 1.678 Train MAE: 14.249 Val MAE: 20.598 Epoch time: 101.976 seconds \n",
      "Epoch: 70 Train loss: 4.676 Aux train loss: 2.844 Val loss: 2.832 Aux val loss: 1.681 Train MAE: 14.586 Val MAE: 18.299 Epoch time: 97.183 seconds \n",
      "Epoch: 71 Train loss: 4.990 Aux train loss: 3.044 Val loss: 2.674 Aux val loss: 1.610 Train MAE: 15.554 Val MAE: 14.843 Epoch time: 100.613 seconds best\n",
      "Epoch: 72 Train loss: 4.983 Aux train loss: 3.025 Val loss: 2.711 Aux val loss: 1.651 Train MAE: 16.632 Val MAE: 16.818 Epoch time: 95.620 seconds \n",
      "Epoch: 73 Train loss: 5.167 Aux train loss: 3.129 Val loss: 2.797 Aux val loss: 1.699 Train MAE: 17.819 Val MAE: 16.804 Epoch time: 101.431 seconds \n",
      "Epoch: 74 Train loss: 5.037 Aux train loss: 3.043 Val loss: 2.738 Aux val loss: 1.655 Train MAE: 16.035 Val MAE: 23.485 Epoch time: 104.127 seconds \n",
      "Epoch: 75 Train loss: 5.180 Aux train loss: 3.131 Val loss: 2.747 Aux val loss: 1.648 Train MAE: 17.156 Val MAE: 17.217 Epoch time: 95.686 seconds \n",
      "Epoch: 76 Train loss: 4.546 Aux train loss: 2.781 Val loss: 2.719 Aux val loss: 1.643 Train MAE: 13.795 Val MAE: 17.261 Epoch time: 95.218 seconds \n",
      "Epoch: 77 Train loss: 5.393 Aux train loss: 3.285 Val loss: 2.675 Aux val loss: 1.623 Train MAE: 15.177 Val MAE: 16.778 Epoch time: 105.863 seconds \n",
      "Epoch: 78 Train loss: 5.248 Aux train loss: 3.178 Val loss: 2.636 Aux val loss: 1.604 Train MAE: 14.387 Val MAE: 17.305 Epoch time: 96.196 seconds \n",
      "Epoch: 79 Train loss: 4.752 Aux train loss: 2.894 Val loss: 2.684 Aux val loss: 1.621 Train MAE: 13.772 Val MAE: 15.517 Epoch time: 97.567 seconds \n",
      "Epoch: 80 Train loss: 4.925 Aux train loss: 2.990 Val loss: 2.701 Aux val loss: 1.649 Train MAE: 14.361 Val MAE: 17.939 Epoch time: 101.051 seconds \n",
      "Epoch: 81 Train loss: 5.136 Aux train loss: 3.124 Val loss: 2.761 Aux val loss: 1.646 Train MAE: 15.708 Val MAE: 17.793 Epoch time: 100.034 seconds \n",
      "Epoch: 82 Train loss: 5.037 Aux train loss: 3.058 Val loss: 2.732 Aux val loss: 1.648 Train MAE: 16.017 Val MAE: 18.361 Epoch time: 94.452 seconds \n",
      "Epoch: 83 Train loss: 4.677 Aux train loss: 2.838 Val loss: 2.722 Aux val loss: 1.667 Train MAE: 14.569 Val MAE: 18.148 Epoch time: 99.509 seconds \n",
      "Epoch: 84 Train loss: 4.804 Aux train loss: 2.914 Val loss: 2.715 Aux val loss: 1.658 Train MAE: 15.281 Val MAE: 23.721 Epoch time: 103.064 seconds \n",
      "Epoch: 85 Train loss: 4.864 Aux train loss: 2.964 Val loss: 2.643 Aux val loss: 1.605 Train MAE: 13.722 Val MAE: 18.334 Epoch time: 90.670 seconds \n",
      "Epoch: 86 Train loss: 4.869 Aux train loss: 2.986 Val loss: 2.606 Aux val loss: 1.576 Train MAE: 15.270 Val MAE: 17.347 Epoch time: 94.333 seconds \n",
      "Epoch: 87 Train loss: 5.209 Aux train loss: 3.166 Val loss: 2.663 Aux val loss: 1.624 Train MAE: 15.178 Val MAE: 21.438 Epoch time: 102.652 seconds \n",
      "Epoch: 88 Train loss: 4.785 Aux train loss: 2.902 Val loss: 2.792 Aux val loss: 1.672 Train MAE: 14.883 Val MAE: 26.052 Epoch time: 88.595 seconds \n",
      "Epoch: 89 Train loss: 4.817 Aux train loss: 2.926 Val loss: 2.679 Aux val loss: 1.614 Train MAE: 14.866 Val MAE: 16.126 Epoch time: 92.795 seconds \n",
      "Epoch: 90 Train loss: 4.509 Aux train loss: 2.738 Val loss: 2.691 Aux val loss: 1.637 Train MAE: 13.467 Val MAE: 23.046 Epoch time: 97.150 seconds \n",
      "Epoch: 91 Train loss: 4.672 Aux train loss: 2.832 Val loss: 2.699 Aux val loss: 1.626 Train MAE: 15.169 Val MAE: 18.702 Epoch time: 90.653 seconds \n",
      "Epoch: 92 Train loss: 4.350 Aux train loss: 2.640 Val loss: 2.708 Aux val loss: 1.613 Train MAE: 14.635 Val MAE: 17.559 Epoch time: 89.410 seconds \n",
      "Epoch: 93 Train loss: 4.823 Aux train loss: 2.889 Val loss: 2.665 Aux val loss: 1.611 Train MAE: 14.958 Val MAE: 21.868 Epoch time: 90.068 seconds \n",
      "Epoch: 94 Train loss: 4.902 Aux train loss: 2.963 Val loss: 2.743 Aux val loss: 1.628 Train MAE: 15.962 Val MAE: 27.019 Epoch time: 101.566 seconds \n",
      "Epoch: 95 Train loss: 4.969 Aux train loss: 3.007 Val loss: 2.787 Aux val loss: 1.647 Train MAE: 17.284 Val MAE: 26.301 Epoch time: 94.543 seconds \n",
      "Epoch: 96 Train loss: 4.997 Aux train loss: 3.014 Val loss: 2.712 Aux val loss: 1.625 Train MAE: 15.767 Val MAE: 16.130 Epoch time: 92.708 seconds \n",
      "Epoch: 97 Train loss: 4.292 Aux train loss: 2.624 Val loss: 2.705 Aux val loss: 1.637 Train MAE: 12.978 Val MAE: 18.505 Epoch time: 91.809 seconds \n",
      "Epoch: 98 Train loss: 4.867 Aux train loss: 2.951 Val loss: 2.656 Aux val loss: 1.592 Train MAE: 13.152 Val MAE: 14.468 Epoch time: 91.232 seconds best\n",
      "Epoch: 99 Train loss: 4.401 Aux train loss: 2.673 Val loss: 2.689 Aux val loss: 1.620 Train MAE: 14.427 Val MAE: 22.038 Epoch time: 86.995 seconds \n",
      "Epoch: 100 Train loss: 4.470 Aux train loss: 2.698 Val loss: 2.760 Aux val loss: 1.653 Train MAE: 13.673 Val MAE: 20.427 Epoch time: 95.243 seconds \n",
      "Epoch: 101 Train loss: 4.575 Aux train loss: 2.786 Val loss: 2.731 Aux val loss: 1.627 Train MAE: 13.684 Val MAE: 16.736 Epoch time: 101.912 seconds \n",
      "Epoch: 102 Train loss: 4.514 Aux train loss: 2.742 Val loss: 2.698 Aux val loss: 1.615 Train MAE: 13.138 Val MAE: 21.010 Epoch time: 93.207 seconds \n",
      "Epoch: 103 Train loss: 4.843 Aux train loss: 2.935 Val loss: 2.694 Aux val loss: 1.614 Train MAE: 14.046 Val MAE: 18.547 Epoch time: 91.835 seconds \n",
      "Epoch: 104 Train loss: 4.382 Aux train loss: 2.662 Val loss: 2.679 Aux val loss: 1.625 Train MAE: 13.089 Val MAE: 18.023 Epoch time: 85.712 seconds \n",
      "Epoch: 105 Train loss: 4.354 Aux train loss: 2.649 Val loss: 2.656 Aux val loss: 1.608 Train MAE: 16.166 Val MAE: 16.706 Epoch time: 89.131 seconds \n",
      "Epoch: 106 Train loss: 4.532 Aux train loss: 2.757 Val loss: 2.745 Aux val loss: 1.676 Train MAE: 14.095 Val MAE: 28.394 Epoch time: 89.063 seconds \n",
      "Epoch: 107 Train loss: 4.468 Aux train loss: 2.721 Val loss: 2.600 Aux val loss: 1.593 Train MAE: 13.877 Val MAE: 15.615 Epoch time: 88.311 seconds \n",
      "Epoch: 108 Train loss: 4.344 Aux train loss: 2.673 Val loss: 2.627 Aux val loss: 1.618 Train MAE: 12.932 Val MAE: 18.370 Epoch time: 91.542 seconds \n",
      "Epoch: 109 Train loss: 4.448 Aux train loss: 2.690 Val loss: 2.705 Aux val loss: 1.625 Train MAE: 13.766 Val MAE: 16.145 Epoch time: 92.344 seconds \n",
      "Epoch: 110 Train loss: 4.474 Aux train loss: 2.714 Val loss: 2.692 Aux val loss: 1.625 Train MAE: 13.178 Val MAE: 23.220 Epoch time: 104.850 seconds \n",
      "Epoch: 111 Train loss: 4.455 Aux train loss: 2.711 Val loss: 2.589 Aux val loss: 1.625 Train MAE: 13.499 Val MAE: 20.246 Epoch time: 97.065 seconds \n",
      "Epoch: 112 Train loss: 4.296 Aux train loss: 2.604 Val loss: 2.619 Aux val loss: 1.593 Train MAE: 14.213 Val MAE: 15.833 Epoch time: 101.747 seconds \n",
      "Epoch: 113 Train loss: 4.468 Aux train loss: 2.718 Val loss: 2.612 Aux val loss: 1.596 Train MAE: 13.279 Val MAE: 17.889 Epoch time: 109.491 seconds \n",
      "Epoch: 114 Train loss: 4.831 Aux train loss: 2.947 Val loss: 2.743 Aux val loss: 1.635 Train MAE: 13.570 Val MAE: 18.603 Epoch time: 94.370 seconds \n",
      "Epoch: 115 Train loss: 4.285 Aux train loss: 2.613 Val loss: 2.673 Aux val loss: 1.644 Train MAE: 13.196 Val MAE: 15.119 Epoch time: 90.131 seconds \n",
      "Epoch: 116 Train loss: 4.464 Aux train loss: 2.718 Val loss: 2.608 Aux val loss: 1.576 Train MAE: 13.402 Val MAE: 16.795 Epoch time: 100.996 seconds \n",
      "Epoch: 117 Train loss: 4.566 Aux train loss: 2.768 Val loss: 2.617 Aux val loss: 1.593 Train MAE: 13.662 Val MAE: 17.575 Epoch time: 101.943 seconds \n",
      "Epoch: 118 Train loss: 4.650 Aux train loss: 2.826 Val loss: 2.697 Aux val loss: 1.643 Train MAE: 13.840 Val MAE: 20.626 Epoch time: 95.333 seconds \n",
      "Epoch: 119 Train loss: 4.474 Aux train loss: 2.711 Val loss: 2.624 Aux val loss: 1.579 Train MAE: 13.367 Val MAE: 15.041 Epoch time: 102.483 seconds \n",
      "Epoch: 120 Train loss: 4.156 Aux train loss: 2.525 Val loss: 2.619 Aux val loss: 1.593 Train MAE: 11.294 Val MAE: 15.517 Epoch time: 104.214 seconds \n",
      "Epoch: 121 Train loss: 4.291 Aux train loss: 2.610 Val loss: 2.668 Aux val loss: 1.591 Train MAE: 12.537 Val MAE: 21.461 Epoch time: 119.223 seconds \n",
      "Epoch: 122 Train loss: 4.661 Aux train loss: 2.826 Val loss: 2.663 Aux val loss: 1.626 Train MAE: 12.785 Val MAE: 22.428 Epoch time: 95.806 seconds \n",
      "Epoch: 123 Train loss: 4.419 Aux train loss: 2.679 Val loss: 2.564 Aux val loss: 1.586 Train MAE: 11.977 Val MAE: 18.134 Epoch time: 94.539 seconds \n",
      "Epoch: 124 Train loss: 4.413 Aux train loss: 2.698 Val loss: 2.638 Aux val loss: 1.589 Train MAE: 13.746 Val MAE: 17.131 Epoch time: 110.933 seconds \n",
      "Epoch: 125 Train loss: 4.278 Aux train loss: 2.602 Val loss: 2.614 Aux val loss: 1.575 Train MAE: 11.869 Val MAE: 21.120 Epoch time: 97.341 seconds \n",
      "Epoch: 126 Train loss: 4.118 Aux train loss: 2.494 Val loss: 2.659 Aux val loss: 1.587 Train MAE: 11.283 Val MAE: 16.524 Epoch time: 113.368 seconds \n",
      "Epoch: 127 Train loss: 4.428 Aux train loss: 2.689 Val loss: 2.671 Aux val loss: 1.591 Train MAE: 12.906 Val MAE: 19.628 Epoch time: 112.600 seconds \n",
      "Epoch: 128 Train loss: 4.123 Aux train loss: 2.495 Val loss: 2.662 Aux val loss: 1.608 Train MAE: 12.718 Val MAE: 29.061 Epoch time: 106.367 seconds \n",
      "Epoch: 129 Train loss: 4.591 Aux train loss: 2.785 Val loss: 2.665 Aux val loss: 1.600 Train MAE: 14.016 Val MAE: 16.515 Epoch time: 107.718 seconds \n",
      "Epoch: 130 Train loss: 4.197 Aux train loss: 2.555 Val loss: 2.568 Aux val loss: 1.535 Train MAE: 12.339 Val MAE: 19.609 Epoch time: 100.345 seconds \n",
      "Epoch: 131 Train loss: 4.003 Aux train loss: 2.427 Val loss: 2.648 Aux val loss: 1.591 Train MAE: 12.231 Val MAE: 19.080 Epoch time: 105.451 seconds \n",
      "Epoch: 132 Train loss: 4.038 Aux train loss: 2.455 Val loss: 2.628 Aux val loss: 1.605 Train MAE: 11.787 Val MAE: 18.058 Epoch time: 111.960 seconds \n",
      "Epoch: 133 Train loss: 4.247 Aux train loss: 2.597 Val loss: 2.591 Aux val loss: 1.567 Train MAE: 12.563 Val MAE: 16.378 Epoch time: 111.906 seconds \n",
      "Epoch: 134 Train loss: 4.578 Aux train loss: 2.772 Val loss: 2.659 Aux val loss: 1.574 Train MAE: 14.371 Val MAE: 18.307 Epoch time: 106.763 seconds \n",
      "Epoch: 135 Train loss: 4.268 Aux train loss: 2.596 Val loss: 2.789 Aux val loss: 1.635 Train MAE: 12.245 Val MAE: 18.446 Epoch time: 108.503 seconds \n",
      "Epoch: 136 Train loss: 4.342 Aux train loss: 2.646 Val loss: 2.718 Aux val loss: 1.663 Train MAE: 13.862 Val MAE: 20.202 Epoch time: 107.994 seconds \n",
      "Epoch: 137 Train loss: 4.276 Aux train loss: 2.599 Val loss: 2.632 Aux val loss: 1.591 Train MAE: 11.857 Val MAE: 14.826 Epoch time: 110.417 seconds \n",
      "Epoch: 138 Train loss: 3.827 Aux train loss: 2.318 Val loss: 2.635 Aux val loss: 1.624 Train MAE: 12.757 Val MAE: 16.926 Epoch time: 101.743 seconds \n",
      "Epoch: 139 Train loss: 4.544 Aux train loss: 2.750 Val loss: 2.611 Aux val loss: 1.559 Train MAE: 12.415 Val MAE: 18.835 Epoch time: 100.509 seconds \n",
      "Epoch: 140 Train loss: 4.146 Aux train loss: 2.512 Val loss: 2.596 Aux val loss: 1.588 Train MAE: 14.031 Val MAE: 15.775 Epoch time: 104.669 seconds \n",
      "Epoch: 141 Train loss: 4.559 Aux train loss: 2.782 Val loss: 2.674 Aux val loss: 1.612 Train MAE: 12.310 Val MAE: 18.210 Epoch time: 105.826 seconds \n",
      "Epoch: 142 Train loss: 4.698 Aux train loss: 2.869 Val loss: 2.691 Aux val loss: 1.605 Train MAE: 13.537 Val MAE: 21.020 Epoch time: 108.499 seconds \n",
      "Epoch: 143 Train loss: 4.596 Aux train loss: 2.785 Val loss: 2.658 Aux val loss: 1.576 Train MAE: 13.707 Val MAE: 27.618 Epoch time: 104.200 seconds \n",
      "Epoch: 144 Train loss: 4.013 Aux train loss: 2.449 Val loss: 2.663 Aux val loss: 1.608 Train MAE: 12.345 Val MAE: 16.029 Epoch time: 100.266 seconds \n",
      "Epoch: 145 Train loss: 4.437 Aux train loss: 2.701 Val loss: 2.587 Aux val loss: 1.557 Train MAE: 11.863 Val MAE: 16.162 Epoch time: 103.298 seconds \n",
      "Epoch: 146 Train loss: 4.413 Aux train loss: 2.682 Val loss: 2.612 Aux val loss: 1.561 Train MAE: 13.381 Val MAE: 16.315 Epoch time: 100.688 seconds \n",
      "Epoch: 147 Train loss: 4.435 Aux train loss: 2.701 Val loss: 2.649 Aux val loss: 1.599 Train MAE: 12.691 Val MAE: 17.488 Epoch time: 100.988 seconds \n",
      "Epoch: 148 Train loss: 4.382 Aux train loss: 2.648 Val loss: 2.590 Aux val loss: 1.577 Train MAE: 13.371 Val MAE: 16.401 Epoch time: 111.706 seconds \n",
      "Epoch: 149 Train loss: 4.598 Aux train loss: 2.790 Val loss: 2.633 Aux val loss: 1.604 Train MAE: 12.890 Val MAE: 15.577 Epoch time: 102.933 seconds \n",
      "Epoch: 150 Train loss: 4.154 Aux train loss: 2.521 Val loss: 2.605 Aux val loss: 1.599 Train MAE: 11.912 Val MAE: 14.082 Epoch time: 107.287 seconds best\n",
      "Epoch: 151 Train loss: 4.311 Aux train loss: 2.628 Val loss: 2.546 Aux val loss: 1.557 Train MAE: 12.564 Val MAE: 14.500 Epoch time: 103.712 seconds \n",
      "Epoch: 152 Train loss: 4.332 Aux train loss: 2.639 Val loss: 2.578 Aux val loss: 1.549 Train MAE: 11.810 Val MAE: 26.324 Epoch time: 106.420 seconds \n",
      "Epoch: 153 Train loss: 4.702 Aux train loss: 2.830 Val loss: 2.623 Aux val loss: 1.736 Train MAE: 13.060 Val MAE: 15.882 Epoch time: 105.917 seconds \n",
      "Epoch: 154 Train loss: 4.413 Aux train loss: 2.682 Val loss: 2.618 Aux val loss: 1.585 Train MAE: 14.544 Val MAE: 18.114 Epoch time: 111.000 seconds \n",
      "Epoch: 155 Train loss: 4.558 Aux train loss: 2.767 Val loss: 2.634 Aux val loss: 1.587 Train MAE: 13.617 Val MAE: 17.250 Epoch time: 104.202 seconds \n",
      "Epoch: 156 Train loss: 4.080 Aux train loss: 2.493 Val loss: 2.636 Aux val loss: 1.559 Train MAE: 12.077 Val MAE: 20.347 Epoch time: 99.581 seconds \n",
      "Epoch: 157 Train loss: 4.265 Aux train loss: 2.592 Val loss: 2.581 Aux val loss: 1.553 Train MAE: 11.764 Val MAE: 16.116 Epoch time: 101.647 seconds \n",
      "Epoch: 158 Train loss: 4.276 Aux train loss: 2.571 Val loss: 2.638 Aux val loss: 1.571 Train MAE: 11.800 Val MAE: 14.615 Epoch time: 106.983 seconds \n",
      "Epoch: 159 Train loss: 4.158 Aux train loss: 2.516 Val loss: 2.643 Aux val loss: 1.614 Train MAE: 12.719 Val MAE: 18.715 Epoch time: 102.141 seconds \n",
      "Epoch: 160 Train loss: 4.461 Aux train loss: 2.725 Val loss: 2.686 Aux val loss: 1.628 Train MAE: 13.007 Val MAE: 21.368 Epoch time: 109.229 seconds \n",
      "Epoch: 161 Train loss: 4.456 Aux train loss: 2.711 Val loss: 2.589 Aux val loss: 1.556 Train MAE: 12.439 Val MAE: 18.828 Epoch time: 113.095 seconds \n",
      "Epoch: 162 Train loss: 4.072 Aux train loss: 2.480 Val loss: 2.539 Aux val loss: 1.559 Train MAE: 11.905 Val MAE: 18.894 Epoch time: 111.399 seconds \n",
      "Epoch: 163 Train loss: 4.244 Aux train loss: 2.582 Val loss: 2.658 Aux val loss: 1.570 Train MAE: 11.519 Val MAE: 17.194 Epoch time: 110.101 seconds \n",
      "Epoch: 164 Train loss: 4.025 Aux train loss: 2.443 Val loss: 2.589 Aux val loss: 1.569 Train MAE: 11.501 Val MAE: 16.579 Epoch time: 98.159 seconds \n",
      "Epoch: 165 Train loss: 4.232 Aux train loss: 2.574 Val loss: 2.647 Aux val loss: 1.604 Train MAE: 11.987 Val MAE: 18.014 Epoch time: 108.303 seconds \n",
      "Epoch: 166 Train loss: 4.277 Aux train loss: 2.601 Val loss: 2.565 Aux val loss: 1.549 Train MAE: 11.934 Val MAE: 14.742 Epoch time: 111.596 seconds \n",
      "Epoch: 167 Train loss: 4.367 Aux train loss: 2.665 Val loss: 2.616 Aux val loss: 1.603 Train MAE: 11.726 Val MAE: 14.530 Epoch time: 108.241 seconds \n",
      "Epoch: 168 Train loss: 4.855 Aux train loss: 2.981 Val loss: 2.600 Aux val loss: 1.577 Train MAE: 12.506 Val MAE: 17.066 Epoch time: 105.347 seconds \n",
      "Epoch: 169 Train loss: 4.628 Aux train loss: 2.816 Val loss: 2.581 Aux val loss: 1.539 Train MAE: 13.614 Val MAE: 15.693 Epoch time: 101.303 seconds \n",
      "Epoch: 170 Train loss: 3.698 Aux train loss: 2.255 Val loss: 2.622 Aux val loss: 1.567 Train MAE: 11.515 Val MAE: 20.747 Epoch time: 88.140 seconds \n",
      "Epoch: 171 Train loss: 4.332 Aux train loss: 2.683 Val loss: 2.712 Aux val loss: 1.585 Train MAE: 11.511 Val MAE: 17.727 Epoch time: 89.865 seconds \n",
      "Epoch: 172 Train loss: 4.720 Aux train loss: 2.879 Val loss: 2.681 Aux val loss: 1.595 Train MAE: 11.467 Val MAE: 19.446 Epoch time: 88.213 seconds \n",
      "Epoch: 173 Train loss: 3.740 Aux train loss: 2.283 Val loss: 2.565 Aux val loss: 1.551 Train MAE: 11.411 Val MAE: 15.936 Epoch time: 89.337 seconds \n",
      "Epoch: 174 Train loss: 4.151 Aux train loss: 2.529 Val loss: 2.578 Aux val loss: 1.541 Train MAE: 12.597 Val MAE: 15.261 Epoch time: 88.653 seconds \n",
      "Epoch: 175 Train loss: 4.135 Aux train loss: 2.524 Val loss: 2.531 Aux val loss: 1.511 Train MAE: 10.827 Val MAE: 18.692 Epoch time: 89.887 seconds \n",
      "Epoch: 176 Train loss: 4.029 Aux train loss: 2.449 Val loss: 2.555 Aux val loss: 1.536 Train MAE: 10.986 Val MAE: 13.527 Epoch time: 87.485 seconds best\n",
      "Epoch: 177 Train loss: 4.220 Aux train loss: 2.561 Val loss: 2.490 Aux val loss: 1.503 Train MAE: 13.563 Val MAE: 15.897 Epoch time: 89.999 seconds \n",
      "Epoch: 178 Train loss: 4.272 Aux train loss: 2.589 Val loss: 2.595 Aux val loss: 1.540 Train MAE: 12.888 Val MAE: 14.338 Epoch time: 88.863 seconds \n",
      "Epoch: 179 Train loss: 4.260 Aux train loss: 2.593 Val loss: 2.542 Aux val loss: 1.536 Train MAE: 12.807 Val MAE: 14.875 Epoch time: 85.815 seconds \n",
      "Epoch: 180 Train loss: 4.297 Aux train loss: 2.606 Val loss: 2.649 Aux val loss: 1.583 Train MAE: 12.127 Val MAE: 18.052 Epoch time: 88.969 seconds \n",
      "Epoch: 181 Train loss: 4.040 Aux train loss: 2.459 Val loss: 2.471 Aux val loss: 1.499 Train MAE: 11.672 Val MAE: 15.539 Epoch time: 87.535 seconds \n",
      "Epoch: 182 Train loss: 4.098 Aux train loss: 2.508 Val loss: 2.578 Aux val loss: 1.546 Train MAE: 12.281 Val MAE: 15.650 Epoch time: 87.661 seconds \n",
      "Epoch: 183 Train loss: 4.236 Aux train loss: 2.577 Val loss: 2.624 Aux val loss: 1.557 Train MAE: 11.800 Val MAE: 20.286 Epoch time: 87.465 seconds \n",
      "Epoch: 184 Train loss: 4.140 Aux train loss: 2.516 Val loss: 2.508 Aux val loss: 1.509 Train MAE: 12.115 Val MAE: 18.095 Epoch time: 89.766 seconds \n",
      "Epoch: 185 Train loss: 4.398 Aux train loss: 2.683 Val loss: 2.575 Aux val loss: 1.567 Train MAE: 11.234 Val MAE: 18.153 Epoch time: 91.622 seconds \n",
      "Epoch: 186 Train loss: 4.079 Aux train loss: 2.485 Val loss: 2.477 Aux val loss: 1.501 Train MAE: 11.237 Val MAE: 20.044 Epoch time: 88.897 seconds \n",
      "Epoch: 187 Train loss: 4.174 Aux train loss: 2.544 Val loss: 2.605 Aux val loss: 1.553 Train MAE: 11.159 Val MAE: 23.950 Epoch time: 87.603 seconds \n",
      "Epoch: 188 Train loss: 3.785 Aux train loss: 2.298 Val loss: 2.503 Aux val loss: 1.505 Train MAE: 13.141 Val MAE: 15.441 Epoch time: 89.058 seconds \n",
      "Epoch: 189 Train loss: 4.028 Aux train loss: 2.447 Val loss: 2.614 Aux val loss: 1.563 Train MAE: 12.422 Val MAE: 16.035 Epoch time: 87.021 seconds \n",
      "Epoch: 190 Train loss: 3.983 Aux train loss: 2.414 Val loss: 2.568 Aux val loss: 1.549 Train MAE: 11.641 Val MAE: 15.987 Epoch time: 87.723 seconds \n",
      "Epoch: 191 Train loss: 4.100 Aux train loss: 2.489 Val loss: 2.479 Aux val loss: 1.497 Train MAE: 11.903 Val MAE: 16.916 Epoch time: 90.036 seconds \n",
      "Epoch: 192 Train loss: 3.909 Aux train loss: 2.374 Val loss: 2.507 Aux val loss: 1.504 Train MAE: 11.145 Val MAE: 15.027 Epoch time: 87.035 seconds \n",
      "Epoch: 193 Train loss: 4.298 Aux train loss: 2.607 Val loss: 2.642 Aux val loss: 1.539 Train MAE: 12.540 Val MAE: 16.694 Epoch time: 88.139 seconds \n",
      "Epoch: 194 Train loss: 4.206 Aux train loss: 2.558 Val loss: 2.593 Aux val loss: 1.591 Train MAE: 12.062 Val MAE: 21.155 Epoch time: 89.758 seconds \n",
      "Epoch: 195 Train loss: 4.279 Aux train loss: 2.610 Val loss: 2.531 Aux val loss: 1.515 Train MAE: 12.555 Val MAE: 17.032 Epoch time: 85.835 seconds \n",
      "Epoch: 196 Train loss: 4.183 Aux train loss: 2.540 Val loss: 2.535 Aux val loss: 1.526 Train MAE: 11.977 Val MAE: 15.833 Epoch time: 92.485 seconds \n",
      "Epoch: 197 Train loss: 4.224 Aux train loss: 2.569 Val loss: 2.527 Aux val loss: 1.870 Train MAE: 11.178 Val MAE: 21.535 Epoch time: 97.732 seconds \n",
      "Epoch: 198 Train loss: 4.106 Aux train loss: 2.497 Val loss: 2.619 Aux val loss: 1.568 Train MAE: 11.398 Val MAE: 18.242 Epoch time: 110.207 seconds \n",
      "Epoch: 199 Train loss: 4.315 Aux train loss: 2.629 Val loss: 2.493 Aux val loss: 1.628 Train MAE: 11.109 Val MAE: 13.978 Epoch time: 108.294 seconds \n",
      "Epoch: 200 Train loss: 4.031 Aux train loss: 2.460 Val loss: 2.561 Aux val loss: 2.083 Train MAE: 10.120 Val MAE: 16.618 Epoch time: 109.886 seconds \n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=4,5,6,7 torchrun --nproc_per_node=4 module12/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient18_freeze_gd --epochs=200 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b316ae1b-8dcd-46b1-ade4-a72b8c3fbb14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Val set MAE: 13.41 RMSE: 40.09\n",
      "GT count: tensor([30.,  8.], device='cuda:0')\n",
      "Predicted count: tensor([27.0593,  6.1285], device='cuda:0')\n",
      "Test set MAE: 13.40 RMSE: 83.51\n",
      "GT count: tensor([17., 34.], device='cuda:0')\n",
      "Predicted count: tensor([16.6480, 32.9708], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 torchrun --nproc_per_node=1 module12/evaluate.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient18_freeze_gd --epochs=200 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c94dc03-f640-419f-8f24-294d0f62b553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485fa16d-f7dd-4f3c-89c3-ae6ed4573423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b517b9c-7711-42e0-b30a-02c5aa02d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f893bf-bbb5-4013-88b7-b6d900ecbd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0322 10:01:28.195000 1526082 site-packages/torch/distributed/run.py:793] \n",
      "W0322 10:01:28.195000 1526082 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0322 10:01:28.195000 1526082 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0322 10:01:28.195000 1526082 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "0\n",
      "1\n",
      "Epoch: 1 Train loss: 18.022 Aux train loss: 10.329 Val loss: 7.044 Aux val loss: 3.942 Train MAE: 74.681 Val MAE: 78.525 Epoch time: 124.007 seconds best\n",
      "Epoch: 2 Train loss: 17.143 Aux train loss: 9.185 Val loss: 6.922 Aux val loss: 3.718 Train MAE: 69.973 Val MAE: 57.877 Epoch time: 124.184 seconds best\n",
      "Epoch: 3 Train loss: 17.394 Aux train loss: 9.018 Val loss: 6.971 Aux val loss: 3.740 Train MAE: 70.036 Val MAE: 68.212 Epoch time: 123.793 seconds \n",
      "Epoch: 4 Train loss: 17.115 Aux train loss: 8.720 Val loss: 6.931 Aux val loss: 3.614 Train MAE: 68.477 Val MAE: 60.160 Epoch time: 124.426 seconds \n",
      "Epoch: 5 Train loss: 17.680 Aux train loss: 8.949 Val loss: 7.120 Aux val loss: 3.556 Train MAE: 71.924 Val MAE: 87.103 Epoch time: 124.208 seconds \n",
      "Epoch: 6 Train loss: 17.870 Aux train loss: 8.565 Val loss: 6.986 Aux val loss: 3.431 Train MAE: 71.664 Val MAE: 70.641 Epoch time: 124.576 seconds \n",
      "Epoch: 7 Train loss: 17.751 Aux train loss: 8.272 Val loss: 7.080 Aux val loss: 3.402 Train MAE: 71.521 Val MAE: 82.864 Epoch time: 124.544 seconds \n",
      "Epoch: 8 Train loss: 17.855 Aux train loss: 8.075 Val loss: 6.930 Aux val loss: 3.392 Train MAE: 70.822 Val MAE: 60.153 Epoch time: 124.131 seconds \n",
      "Epoch: 9 Train loss: 61.919 Aux train loss: 8.238 Val loss: 6.989 Aux val loss: 3.301 Train MAE: 85.778 Val MAE: 70.952 Epoch time: 124.397 seconds \n",
      "Epoch: 10 Train loss: 17.449 Aux train loss: 7.537 Val loss: 6.925 Aux val loss: 3.417 Train MAE: 69.960 Val MAE: 58.665 Epoch time: 124.025 seconds \n",
      "Epoch: 11 Train loss: 17.617 Aux train loss: 7.675 Val loss: 7.112 Aux val loss: 3.306 Train MAE: 73.090 Val MAE: 86.221 Epoch time: 123.991 seconds \n",
      "Epoch: 12 Train loss: 17.821 Aux train loss: 7.558 Val loss: 7.016 Aux val loss: 3.296 Train MAE: 71.078 Val MAE: 74.880 Epoch time: 124.428 seconds \n",
      "Epoch: 13 Train loss: 17.593 Aux train loss: 7.306 Val loss: 6.910 Aux val loss: 3.287 Train MAE: 70.171 Val MAE: 52.359 Epoch time: 124.624 seconds best\n",
      "Epoch: 14 Train loss: 17.414 Aux train loss: 6.983 Val loss: 7.001 Aux val loss: 3.218 Train MAE: 71.415 Val MAE: 72.842 Epoch time: 124.049 seconds \n",
      "Epoch: 15 Train loss: 18.538 Aux train loss: 7.600 Val loss: 7.032 Aux val loss: 3.160 Train MAE: 72.508 Val MAE: 77.032 Epoch time: 124.407 seconds \n",
      "Epoch: 16 Train loss: 17.623 Aux train loss: 7.273 Val loss: 6.926 Aux val loss: 3.155 Train MAE: 71.195 Val MAE: 59.115 Epoch time: 123.700 seconds \n",
      "Epoch: 17 Train loss: 18.055 Aux train loss: 7.212 Val loss: 7.907 Aux val loss: 3.224 Train MAE: 72.055 Val MAE: 145.869 Epoch time: 123.895 seconds \n",
      "Epoch: 18 Train loss: 18.073 Aux train loss: 7.224 Val loss: 6.991 Aux val loss: 3.243 Train MAE: 70.676 Val MAE: 71.309 Epoch time: 124.284 seconds \n",
      "Epoch: 19 Train loss: 16.969 Aux train loss: 6.508 Val loss: 6.918 Aux val loss: 3.083 Train MAE: 71.516 Val MAE: 56.773 Epoch time: 123.824 seconds \n",
      "Epoch: 20 Train loss: 18.277 Aux train loss: 7.034 Val loss: 7.052 Aux val loss: 3.249 Train MAE: 71.890 Val MAE: 79.540 Epoch time: 124.078 seconds \n",
      "Epoch: 21 Train loss: 18.108 Aux train loss: 6.981 Val loss: 6.968 Aux val loss: 3.041 Train MAE: 71.914 Val MAE: 67.687 Epoch time: 123.814 seconds \n",
      "Epoch: 22 Train loss: 17.343 Aux train loss: 6.565 Val loss: 6.953 Aux val loss: 3.061 Train MAE: 73.926 Val MAE: 65.033 Epoch time: 123.604 seconds \n",
      "Epoch: 23 Train loss: 18.768 Aux train loss: 7.170 Val loss: 7.084 Aux val loss: 3.043 Train MAE: 74.418 Val MAE: 83.269 Epoch time: 124.125 seconds \n",
      "Epoch: 24 Train loss: 17.335 Aux train loss: 6.397 Val loss: 6.922 Aux val loss: 3.122 Train MAE: 69.643 Val MAE: 58.123 Epoch time: 123.650 seconds \n",
      "Epoch: 25 Train loss: 17.109 Aux train loss: 6.287 Val loss: 6.915 Aux val loss: 3.000 Train MAE: 69.942 Val MAE: 55.455 Epoch time: 123.890 seconds \n",
      "Epoch: 26 Train loss: 17.571 Aux train loss: 6.534 Val loss: 7.178 Aux val loss: 2.983 Train MAE: 70.693 Val MAE: 92.932 Epoch time: 123.903 seconds \n",
      "Epoch: 27 Train loss: 17.537 Aux train loss: 6.400 Val loss: 6.926 Aux val loss: 2.948 Train MAE: 70.475 Val MAE: 59.200 Epoch time: 124.115 seconds \n",
      "Epoch: 28 Train loss: 18.179 Aux train loss: 6.710 Val loss: 6.912 Aux val loss: 3.023 Train MAE: 75.056 Val MAE: 54.263 Epoch time: 125.051 seconds \n",
      "Epoch: 29 Train loss: 17.821 Aux train loss: 6.387 Val loss: 6.911 Aux val loss: 2.952 Train MAE: 72.188 Val MAE: 53.487 Epoch time: 124.728 seconds \n",
      "Epoch: 30 Train loss: 17.893 Aux train loss: 6.420 Val loss: 6.909 Aux val loss: 3.178 Train MAE: 69.891 Val MAE: 51.651 Epoch time: 124.146 seconds best\n",
      "Epoch: 31 Train loss: 18.151 Aux train loss: 6.581 Val loss: 7.022 Aux val loss: 2.887 Train MAE: 70.409 Val MAE: 75.857 Epoch time: 123.653 seconds \n",
      "Epoch: 32 Train loss: 17.818 Aux train loss: 6.360 Val loss: 6.929 Aux val loss: 2.974 Train MAE: 70.901 Val MAE: 59.814 Epoch time: 123.891 seconds \n",
      "Epoch: 33 Train loss: 18.448 Aux train loss: 6.770 Val loss: 6.981 Aux val loss: 3.007 Train MAE: 69.907 Val MAE: 69.810 Epoch time: 124.194 seconds \n",
      "Epoch: 34 Train loss: 17.397 Aux train loss: 6.094 Val loss: 6.964 Aux val loss: 3.050 Train MAE: 71.761 Val MAE: 67.008 Epoch time: 123.928 seconds \n",
      "Epoch: 35 Train loss: 17.660 Aux train loss: 6.405 Val loss: 6.910 Aux val loss: 2.963 Train MAE: 70.226 Val MAE: 50.295 Epoch time: 123.808 seconds best\n",
      "Epoch: 36 Train loss: 16.599 Aux train loss: 5.673 Val loss: 7.178 Aux val loss: 2.872 Train MAE: 67.481 Val MAE: 92.959 Epoch time: 123.726 seconds \n",
      "Epoch: 37 Train loss: 17.405 Aux train loss: 6.015 Val loss: 6.910 Aux val loss: 2.876 Train MAE: 69.298 Val MAE: 53.070 Epoch time: 123.741 seconds \n",
      "Epoch: 38 Train loss: 17.708 Aux train loss: 6.145 Val loss: 6.971 Aux val loss: 3.059 Train MAE: 69.408 Val MAE: 68.407 Epoch time: 124.334 seconds \n",
      "Epoch: 39 Train loss: 17.099 Aux train loss: 5.889 Val loss: 6.936 Aux val loss: 2.848 Train MAE: 69.375 Val MAE: 48.849 Epoch time: 124.910 seconds best\n",
      "Epoch: 40 Train loss: 17.697 Aux train loss: 6.025 Val loss: 6.964 Aux val loss: 2.880 Train MAE: 71.252 Val MAE: 67.038 Epoch time: 123.733 seconds \n",
      "Epoch: 41 Train loss: 17.349 Aux train loss: 5.831 Val loss: 6.962 Aux val loss: 2.909 Train MAE: 68.624 Val MAE: 66.728 Epoch time: 124.017 seconds \n",
      "Epoch: 42 Train loss: 17.660 Aux train loss: 5.979 Val loss: 6.910 Aux val loss: 3.088 Train MAE: 69.083 Val MAE: 52.831 Epoch time: 124.253 seconds \n",
      "Epoch: 43 Train loss: 17.843 Aux train loss: 6.232 Val loss: 6.918 Aux val loss: 2.866 Train MAE: 70.382 Val MAE: 56.992 Epoch time: 124.164 seconds \n",
      "Epoch: 44 Train loss: 18.145 Aux train loss: 6.305 Val loss: 7.135 Aux val loss: 2.809 Train MAE: 73.061 Val MAE: 88.669 Epoch time: 124.180 seconds \n",
      "Epoch: 45 Train loss: 17.541 Aux train loss: 5.877 Val loss: 6.934 Aux val loss: 2.877 Train MAE: 68.061 Val MAE: 61.158 Epoch time: 123.808 seconds \n",
      "Epoch: 46 Train loss: 16.815 Aux train loss: 5.488 Val loss: 6.911 Aux val loss: 2.880 Train MAE: 71.974 Val MAE: 54.067 Epoch time: 124.011 seconds \n",
      "Epoch: 47 Train loss: 18.119 Aux train loss: 6.166 Val loss: 6.957 Aux val loss: 2.863 Train MAE: 71.596 Val MAE: 65.852 Epoch time: 123.649 seconds \n",
      "Epoch: 48 Train loss: 17.757 Aux train loss: 5.912 Val loss: 7.053 Aux val loss: 2.870 Train MAE: 70.785 Val MAE: 79.733 Epoch time: 124.432 seconds \n",
      "Epoch: 49 Train loss: 17.368 Aux train loss: 5.715 Val loss: 6.932 Aux val loss: 2.752 Train MAE: 67.856 Val MAE: 60.769 Epoch time: 124.166 seconds \n",
      "Epoch: 50 Train loss: 18.379 Aux train loss: 6.207 Val loss: 7.153 Aux val loss: 2.773 Train MAE: 73.055 Val MAE: 90.486 Epoch time: 124.640 seconds \n",
      "Epoch: 51 Train loss: 18.173 Aux train loss: 6.110 Val loss: 6.964 Aux val loss: 2.809 Train MAE: 73.908 Val MAE: 67.135 Epoch time: 124.101 seconds \n",
      "Epoch: 52 Train loss: 17.757 Aux train loss: 5.868 Val loss: 6.946 Aux val loss: 2.837 Train MAE: 70.870 Val MAE: 49.590 Epoch time: 124.126 seconds \n",
      "Epoch: 53 Train loss: 17.544 Aux train loss: 5.762 Val loss: 7.316 Aux val loss: 2.756 Train MAE: 69.459 Val MAE: 105.270 Epoch time: 124.281 seconds \n",
      "Epoch: 54 Train loss: 17.729 Aux train loss: 5.729 Val loss: 6.951 Aux val loss: 2.751 Train MAE: 71.676 Val MAE: 64.695 Epoch time: 124.612 seconds \n",
      "Epoch: 55 Train loss: 17.974 Aux train loss: 5.918 Val loss: 6.909 Aux val loss: 2.830 Train MAE: 68.954 Val MAE: 51.671 Epoch time: 124.231 seconds \n",
      "Epoch: 56 Train loss: 17.721 Aux train loss: 5.705 Val loss: 7.068 Aux val loss: 2.874 Train MAE: 72.527 Val MAE: 81.402 Epoch time: 124.934 seconds \n",
      "Epoch: 57 Train loss: 17.161 Aux train loss: 5.412 Val loss: 6.909 Aux val loss: 2.950 Train MAE: 70.193 Val MAE: 52.465 Epoch time: 123.918 seconds \n",
      "Epoch: 58 Train loss: 17.512 Aux train loss: 5.594 Val loss: 7.023 Aux val loss: 2.743 Train MAE: 69.445 Val MAE: 75.944 Epoch time: 124.108 seconds \n",
      "Epoch: 59 Train loss: 17.884 Aux train loss: 5.753 Val loss: 6.978 Aux val loss: 2.757 Train MAE: 69.213 Val MAE: 69.507 Epoch time: 124.411 seconds \n",
      "Epoch: 60 Train loss: 17.958 Aux train loss: 5.691 Val loss: 6.960 Aux val loss: 2.743 Train MAE: 70.307 Val MAE: 66.446 Epoch time: 124.109 seconds \n",
      "Epoch: 61 Train loss: 17.828 Aux train loss: 5.732 Val loss: 7.075 Aux val loss: 2.802 Train MAE: 71.770 Val MAE: 82.288 Epoch time: 123.530 seconds \n",
      "Epoch: 62 Train loss: 17.303 Aux train loss: 5.426 Val loss: 6.956 Aux val loss: 2.788 Train MAE: 71.630 Val MAE: 65.705 Epoch time: 124.035 seconds \n",
      "Epoch: 63 Train loss: 17.700 Aux train loss: 5.577 Val loss: 7.065 Aux val loss: 2.821 Train MAE: 69.330 Val MAE: 81.186 Epoch time: 124.063 seconds \n",
      "Epoch: 64 Train loss: 18.942 Aux train loss: 6.193 Val loss: 7.019 Aux val loss: 2.774 Train MAE: 70.294 Val MAE: 75.401 Epoch time: 124.013 seconds \n",
      "Epoch: 65 Train loss: 18.930 Aux train loss: 6.069 Val loss: 6.920 Aux val loss: 2.709 Train MAE: 75.871 Val MAE: 57.392 Epoch time: 124.092 seconds \n",
      "Epoch: 66 Train loss: 17.401 Aux train loss: 5.560 Val loss: 6.910 Aux val loss: 2.670 Train MAE: 70.405 Val MAE: 53.350 Epoch time: 123.838 seconds \n",
      "Epoch: 67 Train loss: 17.178 Aux train loss: 5.293 Val loss: 6.920 Aux val loss: 2.675 Train MAE: 69.321 Val MAE: 48.759 Epoch time: 123.983 seconds best\n",
      "Epoch: 68 Train loss: 17.627 Aux train loss: 5.496 Val loss: 6.910 Aux val loss: 2.710 Train MAE: 67.300 Val MAE: 53.329 Epoch time: 123.842 seconds \n",
      "Epoch: 69 Train loss: 17.934 Aux train loss: 5.644 Val loss: 6.910 Aux val loss: 2.715 Train MAE: 69.608 Val MAE: 50.633 Epoch time: 124.598 seconds \n",
      "Epoch: 70 Train loss: 17.450 Aux train loss: 5.330 Val loss: 6.986 Aux val loss: 2.817 Train MAE: 70.783 Val MAE: 70.711 Epoch time: 123.963 seconds \n",
      "Epoch: 71 Train loss: 17.792 Aux train loss: 5.644 Val loss: 6.932 Aux val loss: 2.751 Train MAE: 68.166 Val MAE: 60.778 Epoch time: 124.110 seconds \n",
      "Epoch: 72 Train loss: 17.621 Aux train loss: 5.460 Val loss: 7.093 Aux val loss: 2.788 Train MAE: 71.850 Val MAE: 84.339 Epoch time: 123.901 seconds \n",
      "Epoch: 73 Train loss: 17.748 Aux train loss: 5.516 Val loss: 6.915 Aux val loss: 2.735 Train MAE: 70.376 Val MAE: 55.809 Epoch time: 124.339 seconds \n",
      "Epoch: 74 Train loss: 17.318 Aux train loss: 5.324 Val loss: 6.912 Aux val loss: 2.708 Train MAE: 66.849 Val MAE: 54.319 Epoch time: 124.059 seconds \n",
      "Epoch: 75 Train loss: 17.251 Aux train loss: 5.391 Val loss: 6.922 Aux val loss: 2.744 Train MAE: 69.047 Val MAE: 48.715 Epoch time: 124.151 seconds best\n",
      "Epoch: 76 Train loss: 16.997 Aux train loss: 5.195 Val loss: 6.968 Aux val loss: 2.693 Train MAE: 66.608 Val MAE: 67.824 Epoch time: 124.476 seconds \n",
      "Epoch: 77 Train loss: 17.534 Aux train loss: 5.377 Val loss: 6.915 Aux val loss: 2.735 Train MAE: 69.322 Val MAE: 55.833 Epoch time: 123.850 seconds \n",
      "Epoch: 78 Train loss: 17.462 Aux train loss: 5.391 Val loss: 6.976 Aux val loss: 2.671 Train MAE: 69.713 Val MAE: 69.304 Epoch time: 125.329 seconds \n",
      "Epoch: 79 Train loss: 17.242 Aux train loss: 5.146 Val loss: 6.994 Aux val loss: 2.673 Train MAE: 69.531 Val MAE: 72.002 Epoch time: 124.924 seconds \n",
      "Epoch: 80 Train loss: 17.717 Aux train loss: 5.306 Val loss: 6.911 Aux val loss: 2.637 Train MAE: 73.559 Val MAE: 53.204 Epoch time: 124.805 seconds \n",
      "Epoch: 81 Train loss: 17.625 Aux train loss: 5.409 Val loss: 6.920 Aux val loss: 2.738 Train MAE: 71.239 Val MAE: 57.552 Epoch time: 125.111 seconds \n",
      "Epoch: 82 Train loss: 17.669 Aux train loss: 5.344 Val loss: 6.947 Aux val loss: 2.730 Train MAE: 69.001 Val MAE: 63.873 Epoch time: 124.651 seconds \n",
      "Epoch: 83 Train loss: 17.138 Aux train loss: 5.237 Val loss: 6.935 Aux val loss: 2.710 Train MAE: 70.967 Val MAE: 61.326 Epoch time: 124.869 seconds \n",
      "Epoch: 84 Train loss: 17.645 Aux train loss: 5.377 Val loss: 6.993 Aux val loss: 2.672 Train MAE: 69.413 Val MAE: 71.824 Epoch time: 124.584 seconds \n",
      "Epoch: 85 Train loss: 18.048 Aux train loss: 5.512 Val loss: 7.049 Aux val loss: 2.640 Train MAE: 70.361 Val MAE: 79.215 Epoch time: 125.133 seconds \n",
      "Epoch: 86 Train loss: 18.897 Aux train loss: 5.805 Val loss: 7.028 Aux val loss: 2.655 Train MAE: 73.003 Val MAE: 76.681 Epoch time: 125.327 seconds \n",
      "Epoch: 87 Train loss: 17.896 Aux train loss: 5.415 Val loss: 6.926 Aux val loss: 2.727 Train MAE: 68.576 Val MAE: 59.227 Epoch time: 125.461 seconds \n",
      "Epoch: 88 Train loss: 17.812 Aux train loss: 5.445 Val loss: 7.148 Aux val loss: 2.682 Train MAE: 68.119 Val MAE: 89.998 Epoch time: 124.422 seconds \n",
      "Epoch: 89 Train loss: 17.688 Aux train loss: 5.294 Val loss: 6.971 Aux val loss: 2.827 Train MAE: 66.985 Val MAE: 68.276 Epoch time: 125.184 seconds \n",
      "Epoch: 90 Train loss: 17.595 Aux train loss: 5.286 Val loss: 6.919 Aux val loss: 2.667 Train MAE: 68.738 Val MAE: 57.358 Epoch time: 124.809 seconds \n",
      "Epoch: 91 Train loss: 17.299 Aux train loss: 5.142 Val loss: 7.032 Aux val loss: 2.694 Train MAE: 71.453 Val MAE: 77.109 Epoch time: 125.347 seconds \n",
      "Epoch: 92 Train loss: 17.997 Aux train loss: 5.433 Val loss: 7.070 Aux val loss: 2.757 Train MAE: 73.018 Val MAE: 81.749 Epoch time: 124.510 seconds \n",
      "Epoch: 93 Train loss: 17.470 Aux train loss: 5.330 Val loss: 6.916 Aux val loss: 2.842 Train MAE: 68.121 Val MAE: 49.114 Epoch time: 124.982 seconds \n",
      "Epoch: 94 Train loss: 17.847 Aux train loss: 5.324 Val loss: 6.972 Aux val loss: 2.714 Train MAE: 69.994 Val MAE: 68.530 Epoch time: 124.225 seconds \n",
      "Epoch: 95 Train loss: 17.957 Aux train loss: 5.265 Val loss: 6.917 Aux val loss: 2.645 Train MAE: 72.545 Val MAE: 48.989 Epoch time: 124.336 seconds \n",
      "Epoch: 96 Train loss: 18.324 Aux train loss: 5.490 Val loss: 6.917 Aux val loss: 2.720 Train MAE: 70.047 Val MAE: 48.953 Epoch time: 124.850 seconds \n",
      "Epoch: 97 Train loss: 17.272 Aux train loss: 5.069 Val loss: 6.913 Aux val loss: 2.749 Train MAE: 69.057 Val MAE: 54.982 Epoch time: 124.792 seconds \n",
      "Epoch: 98 Train loss: 17.650 Aux train loss: 5.209 Val loss: 6.940 Aux val loss: 2.634 Train MAE: 69.385 Val MAE: 62.473 Epoch time: 124.889 seconds \n",
      "Epoch: 99 Train loss: 17.857 Aux train loss: 5.269 Val loss: 7.209 Aux val loss: 2.694 Train MAE: 70.158 Val MAE: 95.839 Epoch time: 124.216 seconds \n",
      "Epoch: 100 Train loss: 18.248 Aux train loss: 5.505 Val loss: 6.959 Aux val loss: 2.684 Train MAE: 75.419 Val MAE: 66.226 Epoch time: 124.964 seconds \n",
      "Epoch: 101 Train loss: 18.011 Aux train loss: 5.364 Val loss: 6.909 Aux val loss: 2.624 Train MAE: 73.080 Val MAE: 51.774 Epoch time: 124.424 seconds \n",
      "Epoch: 102 Train loss: 18.086 Aux train loss: 5.409 Val loss: 6.915 Aux val loss: 2.677 Train MAE: 72.066 Val MAE: 55.980 Epoch time: 125.111 seconds \n",
      "Epoch: 103 Train loss: 17.353 Aux train loss: 5.064 Val loss: 6.913 Aux val loss: 2.674 Train MAE: 68.351 Val MAE: 54.922 Epoch time: 125.350 seconds \n",
      "Epoch: 104 Train loss: 17.486 Aux train loss: 5.025 Val loss: 6.928 Aux val loss: 2.662 Train MAE: 68.712 Val MAE: 59.601 Epoch time: 124.294 seconds \n",
      "Epoch: 105 Train loss: 20.962 Aux train loss: 4.918 Val loss: 6.927 Aux val loss: 2.823 Train MAE: 73.054 Val MAE: 59.298 Epoch time: 124.934 seconds \n",
      "Epoch: 106 Train loss: 18.119 Aux train loss: 5.334 Val loss: 6.912 Aux val loss: 2.779 Train MAE: 70.597 Val MAE: 54.284 Epoch time: 124.893 seconds \n",
      "Epoch: 107 Train loss: 17.846 Aux train loss: 5.249 Val loss: 6.987 Aux val loss: 2.643 Train MAE: 70.022 Val MAE: 70.853 Epoch time: 124.789 seconds \n",
      "Epoch: 108 Train loss: 17.617 Aux train loss: 5.134 Val loss: 6.912 Aux val loss: 2.631 Train MAE: 67.641 Val MAE: 54.586 Epoch time: 124.473 seconds \n",
      "Epoch: 109 Train loss: 17.294 Aux train loss: 4.961 Val loss: 6.938 Aux val loss: 2.617 Train MAE: 69.164 Val MAE: 61.983 Epoch time: 124.739 seconds \n",
      "Epoch: 110 Train loss: 17.675 Aux train loss: 5.096 Val loss: 7.015 Aux val loss: 2.731 Train MAE: 71.749 Val MAE: 74.872 Epoch time: 124.741 seconds \n",
      "Epoch: 111 Train loss: 17.752 Aux train loss: 5.168 Val loss: 6.951 Aux val loss: 2.660 Train MAE: 68.774 Val MAE: 64.756 Epoch time: 124.050 seconds \n",
      "Epoch: 112 Train loss: 17.945 Aux train loss: 5.197 Val loss: 7.055 Aux val loss: 2.635 Train MAE: 72.822 Val MAE: 79.918 Epoch time: 124.481 seconds \n",
      "Epoch: 113 Train loss: 17.706 Aux train loss: 5.102 Val loss: 7.004 Aux val loss: 2.638 Train MAE: 69.986 Val MAE: 73.346 Epoch time: 125.534 seconds \n",
      "Epoch: 114 Train loss: 17.275 Aux train loss: 4.904 Val loss: 6.912 Aux val loss: 2.699 Train MAE: 69.180 Val MAE: 54.653 Epoch time: 124.123 seconds \n",
      "Epoch: 115 Train loss: 17.404 Aux train loss: 4.934 Val loss: 6.981 Aux val loss: 2.615 Train MAE: 72.152 Val MAE: 69.913 Epoch time: 124.236 seconds \n",
      "Epoch: 116 Train loss: 17.265 Aux train loss: 5.558 Val loss: 5.816 Aux val loss: 2.759 Train MAE: 58.453 Val MAE: 30.195 Epoch time: 124.643 seconds best\n",
      "Epoch: 117 Train loss: 13.037 Aux train loss: 5.460 Val loss: 5.429 Aux val loss: 2.668 Train MAE: 31.719 Val MAE: 20.767 Epoch time: 125.089 seconds best\n",
      "Epoch: 118 Train loss: 11.332 Aux train loss: 5.164 Val loss: 5.134 Aux val loss: 2.598 Train MAE: 25.879 Val MAE: 26.644 Epoch time: 124.021 seconds \n",
      "Epoch: 119 Train loss: 11.286 Aux train loss: 5.365 Val loss: 5.158 Aux val loss: 2.615 Train MAE: 21.538 Val MAE: 32.001 Epoch time: 124.966 seconds \n",
      "Epoch: 120 Train loss: 10.238 Aux train loss: 5.040 Val loss: 4.937 Aux val loss: 2.579 Train MAE: 22.115 Val MAE: 29.217 Epoch time: 125.101 seconds \n",
      "Epoch: 121 Train loss: 10.104 Aux train loss: 5.167 Val loss: 4.817 Aux val loss: 2.634 Train MAE: 21.937 Val MAE: 22.716 Epoch time: 123.974 seconds \n",
      "Epoch: 122 Train loss: 9.524 Aux train loss: 4.977 Val loss: 4.738 Aux val loss: 2.543 Train MAE: 18.139 Val MAE: 21.398 Epoch time: 124.122 seconds \n",
      "Epoch: 123 Train loss: 10.452 Aux train loss: 5.471 Val loss: 4.767 Aux val loss: 2.625 Train MAE: 19.460 Val MAE: 17.639 Epoch time: 124.277 seconds best\n",
      "Epoch: 124 Train loss: 9.362 Aux train loss: 4.926 Val loss: 4.617 Aux val loss: 2.564 Train MAE: 18.293 Val MAE: 16.104 Epoch time: 124.309 seconds best\n",
      "Epoch: 125 Train loss: 9.065 Aux train loss: 4.899 Val loss: 4.824 Aux val loss: 2.652 Train MAE: 15.565 Val MAE: 18.355 Epoch time: 124.190 seconds \n",
      "Epoch: 126 Train loss: 8.893 Aux train loss: 4.844 Val loss: 4.736 Aux val loss: 2.638 Train MAE: 15.584 Val MAE: 17.890 Epoch time: 124.799 seconds \n",
      "Epoch: 127 Train loss: 8.766 Aux train loss: 4.725 Val loss: 4.588 Aux val loss: 2.579 Train MAE: 16.913 Val MAE: 25.463 Epoch time: 125.154 seconds \n",
      "Epoch: 128 Train loss: 8.549 Aux train loss: 4.691 Val loss: 4.685 Aux val loss: 2.674 Train MAE: 15.817 Val MAE: 17.075 Epoch time: 124.942 seconds \n",
      "Epoch: 129 Train loss: 8.735 Aux train loss: 4.750 Val loss: 4.630 Aux val loss: 2.625 Train MAE: 15.646 Val MAE: 15.167 Epoch time: 124.298 seconds best\n",
      "Epoch: 130 Train loss: 8.473 Aux train loss: 4.713 Val loss: 4.528 Aux val loss: 2.633 Train MAE: 16.838 Val MAE: 16.019 Epoch time: 124.373 seconds \n",
      "Epoch: 131 Train loss: 9.122 Aux train loss: 5.056 Val loss: 4.663 Aux val loss: 2.674 Train MAE: 15.515 Val MAE: 16.911 Epoch time: 124.397 seconds \n",
      "Epoch: 132 Train loss: 9.240 Aux train loss: 5.129 Val loss: 4.514 Aux val loss: 2.595 Train MAE: 16.857 Val MAE: 19.994 Epoch time: 125.086 seconds \n",
      "Epoch: 133 Train loss: 9.585 Aux train loss: 5.313 Val loss: 4.682 Aux val loss: 2.621 Train MAE: 15.416 Val MAE: 28.603 Epoch time: 124.993 seconds \n",
      "Epoch: 134 Train loss: 8.963 Aux train loss: 4.980 Val loss: 4.629 Aux val loss: 2.645 Train MAE: 15.389 Val MAE: 15.790 Epoch time: 125.074 seconds \n",
      "Epoch: 135 Train loss: 8.333 Aux train loss: 4.674 Val loss: 4.467 Aux val loss: 2.571 Train MAE: 14.773 Val MAE: 15.162 Epoch time: 124.844 seconds best\n",
      "Epoch: 136 Train loss: 9.101 Aux train loss: 5.124 Val loss: 4.416 Aux val loss: 2.552 Train MAE: 16.539 Val MAE: 13.295 Epoch time: 124.091 seconds best\n",
      "Epoch: 137 Train loss: 8.972 Aux train loss: 5.047 Val loss: 4.351 Aux val loss: 2.595 Train MAE: 15.277 Val MAE: 16.245 Epoch time: 124.919 seconds \n",
      "Epoch: 138 Train loss: 8.693 Aux train loss: 4.932 Val loss: 4.445 Aux val loss: 2.596 Train MAE: 15.196 Val MAE: 15.685 Epoch time: 124.174 seconds \n",
      "Epoch: 139 Train loss: 8.478 Aux train loss: 4.829 Val loss: 4.459 Aux val loss: 2.598 Train MAE: 13.149 Val MAE: 25.476 Epoch time: 124.024 seconds \n",
      "Epoch: 140 Train loss: 8.503 Aux train loss: 4.897 Val loss: 4.441 Aux val loss: 2.686 Train MAE: 15.661 Val MAE: 15.513 Epoch time: 124.806 seconds \n",
      "Epoch: 141 Train loss: 8.783 Aux train loss: 5.001 Val loss: 4.514 Aux val loss: 2.576 Train MAE: 14.969 Val MAE: 28.349 Epoch time: 124.754 seconds \n",
      "Epoch: 142 Train loss: 8.927 Aux train loss: 5.074 Val loss: 4.410 Aux val loss: 2.556 Train MAE: 15.085 Val MAE: 13.096 Epoch time: 125.405 seconds best\n",
      "Epoch: 143 Train loss: 8.081 Aux train loss: 4.625 Val loss: 4.358 Aux val loss: 2.532 Train MAE: 15.066 Val MAE: 24.223 Epoch time: 125.859 seconds \n",
      "Epoch: 144 Train loss: 8.600 Aux train loss: 4.933 Val loss: 4.562 Aux val loss: 2.638 Train MAE: 13.932 Val MAE: 17.368 Epoch time: 125.266 seconds \n",
      "Epoch: 145 Train loss: 8.616 Aux train loss: 4.913 Val loss: 4.435 Aux val loss: 2.635 Train MAE: 13.883 Val MAE: 18.664 Epoch time: 124.641 seconds \n",
      "Epoch: 146 Train loss: 7.919 Aux train loss: 4.583 Val loss: 4.485 Aux val loss: 2.609 Train MAE: 13.440 Val MAE: 22.084 Epoch time: 125.303 seconds \n",
      "Epoch: 147 Train loss: 8.235 Aux train loss: 4.782 Val loss: 4.525 Aux val loss: 2.682 Train MAE: 13.273 Val MAE: 22.395 Epoch time: 123.943 seconds \n",
      "Epoch: 148 Train loss: 7.777 Aux train loss: 4.474 Val loss: 4.349 Aux val loss: 2.603 Train MAE: 14.150 Val MAE: 21.291 Epoch time: 124.552 seconds \n",
      "Epoch: 149 Train loss: 8.335 Aux train loss: 4.852 Val loss: 4.433 Aux val loss: 2.588 Train MAE: 13.088 Val MAE: 15.964 Epoch time: 124.391 seconds \n",
      "Epoch: 150 Train loss: 8.305 Aux train loss: 4.810 Val loss: 4.544 Aux val loss: 2.617 Train MAE: 14.197 Val MAE: 14.300 Epoch time: 124.281 seconds \n",
      "Epoch: 151 Train loss: 8.427 Aux train loss: 4.905 Val loss: 4.428 Aux val loss: 2.591 Train MAE: 12.459 Val MAE: 14.455 Epoch time: 124.354 seconds \n",
      "Epoch: 152 Train loss: 8.345 Aux train loss: 4.789 Val loss: 4.458 Aux val loss: 2.644 Train MAE: 12.857 Val MAE: 17.413 Epoch time: 124.883 seconds \n",
      "Epoch: 153 Train loss: 8.407 Aux train loss: 4.891 Val loss: 4.286 Aux val loss: 2.569 Train MAE: 13.832 Val MAE: 23.933 Epoch time: 124.855 seconds \n",
      "Epoch: 154 Train loss: 8.124 Aux train loss: 4.767 Val loss: 4.428 Aux val loss: 2.582 Train MAE: 12.930 Val MAE: 18.371 Epoch time: 124.576 seconds \n",
      "Epoch: 155 Train loss: 8.228 Aux train loss: 4.814 Val loss: 4.334 Aux val loss: 2.585 Train MAE: 13.030 Val MAE: 16.732 Epoch time: 125.153 seconds \n",
      "Epoch: 156 Train loss: 8.328 Aux train loss: 4.840 Val loss: 4.302 Aux val loss: 2.559 Train MAE: 14.035 Val MAE: 15.884 Epoch time: 124.030 seconds \n",
      "Epoch: 157 Train loss: 7.862 Aux train loss: 4.591 Val loss: 4.311 Aux val loss: 2.532 Train MAE: 13.106 Val MAE: 17.854 Epoch time: 124.356 seconds \n",
      "Epoch: 158 Train loss: 8.420 Aux train loss: 4.965 Val loss: 4.271 Aux val loss: 2.559 Train MAE: 12.733 Val MAE: 13.619 Epoch time: 124.721 seconds \n",
      "Epoch: 159 Train loss: 8.251 Aux train loss: 4.831 Val loss: 4.339 Aux val loss: 2.566 Train MAE: 12.520 Val MAE: 13.893 Epoch time: 125.533 seconds \n",
      "Epoch: 160 Train loss: 7.989 Aux train loss: 4.726 Val loss: 4.346 Aux val loss: 2.614 Train MAE: 12.781 Val MAE: 15.625 Epoch time: 124.778 seconds \n",
      "Epoch: 161 Train loss: 7.694 Aux train loss: 4.530 Val loss: 4.425 Aux val loss: 2.588 Train MAE: 11.774 Val MAE: 18.698 Epoch time: 124.345 seconds \n",
      "Epoch: 162 Train loss: 7.773 Aux train loss: 4.603 Val loss: 4.333 Aux val loss: 2.584 Train MAE: 12.756 Val MAE: 23.233 Epoch time: 124.302 seconds \n",
      "Epoch: 163 Train loss: 7.805 Aux train loss: 4.610 Val loss: 4.217 Aux val loss: 2.527 Train MAE: 13.219 Val MAE: 17.964 Epoch time: 124.420 seconds \n",
      "Epoch: 164 Train loss: 8.205 Aux train loss: 4.842 Val loss: 4.288 Aux val loss: 2.557 Train MAE: 12.668 Val MAE: 14.187 Epoch time: 124.120 seconds \n",
      "Epoch: 165 Train loss: 7.722 Aux train loss: 4.576 Val loss: 4.224 Aux val loss: 2.522 Train MAE: 11.782 Val MAE: 13.646 Epoch time: 124.410 seconds \n",
      "Epoch: 166 Train loss: 7.604 Aux train loss: 4.501 Val loss: 4.254 Aux val loss: 2.570 Train MAE: 11.930 Val MAE: 14.831 Epoch time: 124.477 seconds \n",
      "Epoch: 167 Train loss: 8.005 Aux train loss: 4.736 Val loss: 4.334 Aux val loss: 2.541 Train MAE: 10.616 Val MAE: 15.859 Epoch time: 125.169 seconds \n",
      "Epoch: 168 Train loss: 7.901 Aux train loss: 4.686 Val loss: 4.321 Aux val loss: 2.583 Train MAE: 12.805 Val MAE: 18.007 Epoch time: 123.455 seconds \n",
      "Epoch: 169 Train loss: 7.484 Aux train loss: 4.425 Val loss: 4.321 Aux val loss: 2.590 Train MAE: 12.602 Val MAE: 21.327 Epoch time: 124.452 seconds \n",
      "Epoch: 170 Train loss: 8.342 Aux train loss: 4.938 Val loss: 4.330 Aux val loss: 2.547 Train MAE: 13.213 Val MAE: 16.324 Epoch time: 124.233 seconds \n",
      "Epoch: 171 Train loss: 8.255 Aux train loss: 4.881 Val loss: 4.350 Aux val loss: 2.649 Train MAE: 13.250 Val MAE: 18.246 Epoch time: 124.408 seconds \n",
      "Epoch: 172 Train loss: 8.172 Aux train loss: 4.896 Val loss: 4.212 Aux val loss: 2.495 Train MAE: 12.924 Val MAE: 12.079 Epoch time: 124.219 seconds best\n",
      "Epoch: 173 Train loss: 7.849 Aux train loss: 4.691 Val loss: 4.347 Aux val loss: 2.564 Train MAE: 12.481 Val MAE: 15.293 Epoch time: 124.598 seconds \n",
      "Epoch: 174 Train loss: 8.086 Aux train loss: 4.825 Val loss: 4.240 Aux val loss: 2.536 Train MAE: 11.867 Val MAE: 13.921 Epoch time: 124.409 seconds \n",
      "Epoch: 175 Train loss: 8.009 Aux train loss: 4.801 Val loss: 4.340 Aux val loss: 2.614 Train MAE: 11.513 Val MAE: 24.780 Epoch time: 124.543 seconds \n",
      "Epoch: 176 Train loss: 7.350 Aux train loss: 4.420 Val loss: 4.259 Aux val loss: 2.506 Train MAE: 12.722 Val MAE: 18.384 Epoch time: 124.205 seconds \n",
      "Epoch: 177 Train loss: 7.597 Aux train loss: 4.558 Val loss: 4.351 Aux val loss: 2.595 Train MAE: 11.351 Val MAE: 15.908 Epoch time: 124.188 seconds \n",
      "Epoch: 178 Train loss: 8.166 Aux train loss: 4.865 Val loss: 4.269 Aux val loss: 2.553 Train MAE: 11.927 Val MAE: 14.845 Epoch time: 124.769 seconds \n",
      "Epoch: 179 Train loss: 8.198 Aux train loss: 4.910 Val loss: 4.350 Aux val loss: 2.598 Train MAE: 12.444 Val MAE: 25.841 Epoch time: 125.470 seconds \n",
      "Epoch: 180 Train loss: 7.482 Aux train loss: 4.484 Val loss: 4.271 Aux val loss: 2.518 Train MAE: 11.668 Val MAE: 23.488 Epoch time: 124.454 seconds \n",
      "Epoch: 181 Train loss: 7.686 Aux train loss: 4.581 Val loss: 4.240 Aux val loss: 2.656 Train MAE: 11.853 Val MAE: 14.802 Epoch time: 124.888 seconds \n",
      "Epoch: 182 Train loss: 7.622 Aux train loss: 4.582 Val loss: 4.304 Aux val loss: 2.553 Train MAE: 11.103 Val MAE: 16.320 Epoch time: 124.221 seconds \n",
      "Epoch: 183 Train loss: 7.512 Aux train loss: 4.533 Val loss: 4.345 Aux val loss: 2.550 Train MAE: 11.142 Val MAE: 15.493 Epoch time: 124.837 seconds \n",
      "Epoch: 184 Train loss: 8.070 Aux train loss: 4.846 Val loss: 4.198 Aux val loss: 2.489 Train MAE: 11.363 Val MAE: 13.162 Epoch time: 124.796 seconds \n",
      "Epoch: 185 Train loss: 7.524 Aux train loss: 4.517 Val loss: 4.142 Aux val loss: 2.511 Train MAE: 11.329 Val MAE: 13.479 Epoch time: 124.307 seconds \n",
      "Epoch: 186 Train loss: 7.524 Aux train loss: 4.476 Val loss: 4.331 Aux val loss: 2.576 Train MAE: 11.727 Val MAE: 16.309 Epoch time: 124.520 seconds \n",
      "Epoch: 187 Train loss: 8.071 Aux train loss: 4.806 Val loss: 4.180 Aux val loss: 2.525 Train MAE: 12.220 Val MAE: 17.113 Epoch time: 124.612 seconds \n",
      "Epoch: 188 Train loss: 7.688 Aux train loss: 4.612 Val loss: 4.193 Aux val loss: 2.697 Train MAE: 11.753 Val MAE: 16.972 Epoch time: 124.107 seconds \n",
      "Epoch: 189 Train loss: 7.678 Aux train loss: 4.626 Val loss: 4.129 Aux val loss: 2.571 Train MAE: 10.600 Val MAE: 14.084 Epoch time: 123.736 seconds \n",
      "Epoch: 190 Train loss: 8.161 Aux train loss: 4.928 Val loss: 4.173 Aux val loss: 2.549 Train MAE: 11.885 Val MAE: 13.587 Epoch time: 124.934 seconds \n",
      "Epoch: 191 Train loss: 7.468 Aux train loss: 4.501 Val loss: 4.232 Aux val loss: 2.617 Train MAE: 10.765 Val MAE: 13.162 Epoch time: 124.059 seconds \n",
      "Epoch: 192 Train loss: 8.419 Aux train loss: 5.029 Val loss: 4.155 Aux val loss: 2.500 Train MAE: 12.545 Val MAE: 14.387 Epoch time: 124.379 seconds \n",
      "Epoch: 193 Train loss: 7.345 Aux train loss: 4.403 Val loss: 4.162 Aux val loss: 2.540 Train MAE: 11.019 Val MAE: 12.198 Epoch time: 124.319 seconds \n",
      "Epoch: 194 Train loss: 7.645 Aux train loss: 4.618 Val loss: 4.147 Aux val loss: 2.514 Train MAE: 10.763 Val MAE: 14.816 Epoch time: 124.257 seconds \n",
      "Epoch: 195 Train loss: 7.398 Aux train loss: 4.471 Val loss: 4.389 Aux val loss: 2.616 Train MAE: 11.643 Val MAE: 15.714 Epoch time: 124.158 seconds \n",
      "Epoch: 196 Train loss: 7.054 Aux train loss: 4.277 Val loss: 4.260 Aux val loss: 2.589 Train MAE: 10.071 Val MAE: 17.636 Epoch time: 124.224 seconds \n",
      "Epoch: 197 Train loss: 6.947 Aux train loss: 4.216 Val loss: 4.371 Aux val loss: 2.620 Train MAE: 11.901 Val MAE: 14.580 Epoch time: 124.494 seconds \n",
      "Epoch: 198 Train loss: 8.884 Aux train loss: 5.310 Val loss: 4.282 Aux val loss: 2.570 Train MAE: 12.669 Val MAE: 14.718 Epoch time: 125.420 seconds \n",
      "Epoch: 199 Train loss: 7.797 Aux train loss: 4.682 Val loss: 4.399 Aux val loss: 2.567 Train MAE: 11.138 Val MAE: 14.075 Epoch time: 123.880 seconds \n",
      "Epoch: 200 Train loss: 7.527 Aux train loss: 4.555 Val loss: 4.232 Aux val loss: 2.580 Train MAE: 10.818 Val MAE: 16.359 Epoch time: 124.638 seconds \n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc_per_node=2 module12/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient19_freeze_gd --epochs=200 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319cca7b-9f8f-4e26-b40e-cc68f229a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Val set MAE: 12.08 RMSE: 37.84\n",
      "GT count: tensor([30.,  8.], device='cuda:0')\n",
      "Predicted count: tensor([25.6116,  7.3657], device='cuda:0')\n",
      "Test set MAE: 14.26 RMSE: 92.49\n",
      "GT count: tensor([17., 34.], device='cuda:0')\n",
      "Predicted count: tensor([ 8.1130, 30.2387], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 torchrun --nproc_per_node=1 module12/evaluate.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient19_freeze_gd --epochs=200 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fded64-e7ab-4ab1-987b-58a84d4f3b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4979b6d-43d4-490c-97bf-c73cea7faa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afdc6aa-a8b5-480a-82dc-f48e233b9578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0322 20:01:05.471000 1768660 site-packages/torch/distributed/run.py:793] \n",
      "W0322 20:01:05.471000 1768660 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0322 20:01:05.471000 1768660 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0322 20:01:05.471000 1768660 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "Epoch: 1 Train loss: 9.770 Aux train loss: 5.671 Val loss: 4.206 Aux val loss: 2.376 Train MAE: 71.689 Val MAE: 75.139 Epoch time: 95.654 seconds best\n",
      "Epoch: 2 Train loss: 9.467 Aux train loss: 5.365 Val loss: 3.886 Aux val loss: 2.306 Train MAE: 59.796 Val MAE: 41.011 Epoch time: 94.409 seconds best\n",
      "Epoch: 3 Train loss: 8.917 Aux train loss: 5.344 Val loss: 3.727 Aux val loss: 2.253 Train MAE: 41.432 Val MAE: 35.669 Epoch time: 94.106 seconds best\n",
      "Epoch: 4 Train loss: 8.005 Aux train loss: 4.927 Val loss: 3.553 Aux val loss: 2.171 Train MAE: 33.612 Val MAE: 29.349 Epoch time: 96.987 seconds best\n",
      "Epoch: 5 Train loss: 18.485 Aux train loss: 4.829 Val loss: 3.400 Aux val loss: 2.104 Train MAE: 45.641 Val MAE: 20.685 Epoch time: 96.203 seconds best\n",
      "Epoch: 6 Train loss: 7.070 Aux train loss: 4.321 Val loss: 3.505 Aux val loss: 2.065 Train MAE: 29.054 Val MAE: 37.774 Epoch time: 91.416 seconds \n",
      "Epoch: 7 Train loss: 6.726 Aux train loss: 4.182 Val loss: 3.316 Aux val loss: 2.067 Train MAE: 27.637 Val MAE: 22.980 Epoch time: 90.203 seconds \n",
      "Epoch: 8 Train loss: 6.555 Aux train loss: 4.118 Val loss: 3.185 Aux val loss: 1.967 Train MAE: 24.936 Val MAE: 21.069 Epoch time: 94.094 seconds \n",
      "Epoch: 9 Train loss: 5.830 Aux train loss: 3.664 Val loss: 3.142 Aux val loss: 1.965 Train MAE: 22.226 Val MAE: 30.440 Epoch time: 94.232 seconds \n",
      "Epoch: 10 Train loss: 7.079 Aux train loss: 4.446 Val loss: 3.188 Aux val loss: 1.964 Train MAE: 24.695 Val MAE: 42.767 Epoch time: 94.829 seconds \n",
      "Epoch: 11 Train loss: 6.600 Aux train loss: 4.081 Val loss: 3.074 Aux val loss: 1.916 Train MAE: 24.934 Val MAE: 18.374 Epoch time: 96.722 seconds best\n",
      "Epoch: 12 Train loss: 6.317 Aux train loss: 3.968 Val loss: 3.012 Aux val loss: 1.866 Train MAE: 23.633 Val MAE: 21.314 Epoch time: 96.624 seconds \n",
      "Epoch: 13 Train loss: 5.766 Aux train loss: 3.615 Val loss: 2.966 Aux val loss: 1.861 Train MAE: 21.345 Val MAE: 19.806 Epoch time: 95.245 seconds \n",
      "Epoch: 14 Train loss: 5.888 Aux train loss: 3.716 Val loss: 3.198 Aux val loss: 2.113 Train MAE: 20.317 Val MAE: 21.210 Epoch time: 93.980 seconds \n",
      "Epoch: 15 Train loss: 5.586 Aux train loss: 3.533 Val loss: 2.957 Aux val loss: 1.865 Train MAE: 20.859 Val MAE: 17.688 Epoch time: 95.636 seconds best\n",
      "Epoch: 16 Train loss: 6.063 Aux train loss: 3.781 Val loss: 2.943 Aux val loss: 1.855 Train MAE: 21.768 Val MAE: 19.890 Epoch time: 93.894 seconds \n",
      "Epoch: 17 Train loss: 5.190 Aux train loss: 3.266 Val loss: 2.996 Aux val loss: 1.830 Train MAE: 20.603 Val MAE: 30.228 Epoch time: 92.878 seconds \n",
      "Epoch: 18 Train loss: 6.062 Aux train loss: 3.799 Val loss: 2.862 Aux val loss: 1.799 Train MAE: 21.775 Val MAE: 15.876 Epoch time: 95.071 seconds best\n",
      "Epoch: 19 Train loss: 5.411 Aux train loss: 3.397 Val loss: 2.909 Aux val loss: 1.807 Train MAE: 18.300 Val MAE: 22.255 Epoch time: 89.123 seconds \n",
      "Epoch: 20 Train loss: 5.818 Aux train loss: 3.677 Val loss: 2.894 Aux val loss: 1.819 Train MAE: 19.153 Val MAE: 23.374 Epoch time: 90.962 seconds \n",
      "Epoch: 21 Train loss: 5.742 Aux train loss: 3.584 Val loss: 2.827 Aux val loss: 1.764 Train MAE: 20.035 Val MAE: 18.709 Epoch time: 92.300 seconds \n",
      "Epoch: 22 Train loss: 5.315 Aux train loss: 3.529 Val loss: 2.870 Aux val loss: 1.753 Train MAE: 18.721 Val MAE: 20.445 Epoch time: 92.770 seconds \n",
      "Epoch: 23 Train loss: 5.175 Aux train loss: 3.234 Val loss: 2.805 Aux val loss: 1.745 Train MAE: 18.686 Val MAE: 13.853 Epoch time: 91.864 seconds best\n",
      "Epoch: 24 Train loss: 4.877 Aux train loss: 3.089 Val loss: 2.897 Aux val loss: 1.850 Train MAE: 16.828 Val MAE: 19.476 Epoch time: 89.005 seconds \n",
      "Epoch: 25 Train loss: 5.372 Aux train loss: 3.403 Val loss: 2.913 Aux val loss: 1.797 Train MAE: 15.911 Val MAE: 16.792 Epoch time: 89.424 seconds \n",
      "Epoch: 26 Train loss: 5.718 Aux train loss: 3.594 Val loss: 2.764 Aux val loss: 1.727 Train MAE: 18.148 Val MAE: 14.564 Epoch time: 89.563 seconds \n",
      "Epoch: 27 Train loss: 5.409 Aux train loss: 3.458 Val loss: 2.791 Aux val loss: 1.729 Train MAE: 17.714 Val MAE: 14.217 Epoch time: 91.179 seconds \n",
      "Epoch: 28 Train loss: 5.199 Aux train loss: 3.270 Val loss: 2.779 Aux val loss: 1.725 Train MAE: 17.281 Val MAE: 16.366 Epoch time: 97.356 seconds \n",
      "Epoch: 29 Train loss: 5.225 Aux train loss: 3.296 Val loss: 2.764 Aux val loss: 1.733 Train MAE: 17.877 Val MAE: 15.459 Epoch time: 94.752 seconds \n",
      "Epoch: 30 Train loss: 5.059 Aux train loss: 3.212 Val loss: 2.757 Aux val loss: 1.749 Train MAE: 15.001 Val MAE: 14.310 Epoch time: 93.695 seconds \n",
      "Epoch: 31 Train loss: 5.153 Aux train loss: 3.235 Val loss: 2.754 Aux val loss: 1.736 Train MAE: 15.963 Val MAE: 14.557 Epoch time: 95.145 seconds \n",
      "Epoch: 32 Train loss: 5.066 Aux train loss: 3.186 Val loss: 2.829 Aux val loss: 1.768 Train MAE: 16.009 Val MAE: 22.019 Epoch time: 97.682 seconds \n",
      "Epoch: 33 Train loss: 5.219 Aux train loss: 3.313 Val loss: 2.793 Aux val loss: 1.725 Train MAE: 16.066 Val MAE: 18.481 Epoch time: 95.272 seconds \n",
      "Epoch: 34 Train loss: 4.973 Aux train loss: 3.142 Val loss: 2.841 Aux val loss: 1.732 Train MAE: 16.100 Val MAE: 16.938 Epoch time: 95.829 seconds \n",
      "Epoch: 35 Train loss: 4.735 Aux train loss: 2.990 Val loss: 2.808 Aux val loss: 1.742 Train MAE: 14.833 Val MAE: 18.533 Epoch time: 90.486 seconds \n",
      "Epoch: 36 Train loss: 5.028 Aux train loss: 3.182 Val loss: 2.731 Aux val loss: 1.689 Train MAE: 15.932 Val MAE: 16.945 Epoch time: 96.382 seconds \n",
      "Epoch: 37 Train loss: 5.642 Aux train loss: 3.519 Val loss: 2.747 Aux val loss: 1.690 Train MAE: 17.108 Val MAE: 16.543 Epoch time: 92.699 seconds \n",
      "Epoch: 38 Train loss: 4.766 Aux train loss: 3.031 Val loss: 2.772 Aux val loss: 1.702 Train MAE: 14.072 Val MAE: 17.645 Epoch time: 93.834 seconds \n",
      "Epoch: 39 Train loss: 5.451 Aux train loss: 3.425 Val loss: 2.853 Aux val loss: 1.800 Train MAE: 16.442 Val MAE: 16.895 Epoch time: 96.276 seconds \n",
      "Epoch: 40 Train loss: 5.022 Aux train loss: 3.158 Val loss: 2.766 Aux val loss: 1.690 Train MAE: 14.909 Val MAE: 19.936 Epoch time: 94.484 seconds \n",
      "Epoch: 41 Train loss: 4.999 Aux train loss: 3.125 Val loss: 2.689 Aux val loss: 1.661 Train MAE: 15.238 Val MAE: 13.952 Epoch time: 93.225 seconds \n",
      "Epoch: 42 Train loss: 5.094 Aux train loss: 3.220 Val loss: 2.765 Aux val loss: 1.704 Train MAE: 15.732 Val MAE: 19.087 Epoch time: 93.451 seconds \n",
      "Epoch: 43 Train loss: 4.733 Aux train loss: 2.990 Val loss: 2.674 Aux val loss: 1.659 Train MAE: 14.577 Val MAE: 15.412 Epoch time: 93.388 seconds \n",
      "Epoch: 44 Train loss: 5.251 Aux train loss: 3.336 Val loss: 2.705 Aux val loss: 1.675 Train MAE: 14.582 Val MAE: 20.174 Epoch time: 94.956 seconds \n",
      "Epoch: 45 Train loss: 4.969 Aux train loss: 3.104 Val loss: 2.724 Aux val loss: 1.674 Train MAE: 14.403 Val MAE: 15.143 Epoch time: 93.635 seconds \n",
      "Epoch: 46 Train loss: 4.967 Aux train loss: 3.135 Val loss: 2.848 Aux val loss: 1.734 Train MAE: 13.587 Val MAE: 17.763 Epoch time: 97.341 seconds \n",
      "Epoch: 47 Train loss: 4.736 Aux train loss: 3.013 Val loss: 2.684 Aux val loss: 1.671 Train MAE: 14.003 Val MAE: 23.963 Epoch time: 94.293 seconds \n",
      "Epoch: 48 Train loss: 5.034 Aux train loss: 3.192 Val loss: 2.724 Aux val loss: 1.670 Train MAE: 15.744 Val MAE: 18.726 Epoch time: 91.678 seconds \n",
      "Epoch: 49 Train loss: 5.005 Aux train loss: 3.168 Val loss: 2.840 Aux val loss: 1.762 Train MAE: 16.971 Val MAE: 30.101 Epoch time: 94.353 seconds \n",
      "Epoch: 50 Train loss: 4.903 Aux train loss: 3.083 Val loss: 2.654 Aux val loss: 1.661 Train MAE: 14.645 Val MAE: 14.387 Epoch time: 105.792 seconds \n",
      "Epoch: 51 Train loss: 4.704 Aux train loss: 2.992 Val loss: 2.775 Aux val loss: 1.701 Train MAE: 14.881 Val MAE: 17.239 Epoch time: 99.922 seconds \n",
      "Epoch: 52 Train loss: 5.164 Aux train loss: 3.279 Val loss: 2.723 Aux val loss: 1.691 Train MAE: 14.808 Val MAE: 16.237 Epoch time: 95.610 seconds \n",
      "Epoch: 53 Train loss: 5.584 Aux train loss: 3.490 Val loss: 2.796 Aux val loss: 1.733 Train MAE: 16.065 Val MAE: 18.517 Epoch time: 92.830 seconds \n",
      "Epoch: 54 Train loss: 4.330 Aux train loss: 2.737 Val loss: 2.696 Aux val loss: 1.685 Train MAE: 12.408 Val MAE: 17.549 Epoch time: 95.628 seconds \n",
      "Epoch: 55 Train loss: 4.988 Aux train loss: 3.164 Val loss: 2.725 Aux val loss: 1.680 Train MAE: 14.755 Val MAE: 18.206 Epoch time: 97.115 seconds \n",
      "Epoch: 56 Train loss: 4.840 Aux train loss: 3.045 Val loss: 2.781 Aux val loss: 1.734 Train MAE: 13.977 Val MAE: 25.763 Epoch time: 99.858 seconds \n",
      "Epoch: 57 Train loss: 4.599 Aux train loss: 2.902 Val loss: 2.678 Aux val loss: 1.661 Train MAE: 13.440 Val MAE: 22.345 Epoch time: 98.088 seconds \n",
      "Epoch: 58 Train loss: 5.155 Aux train loss: 3.228 Val loss: 2.728 Aux val loss: 1.654 Train MAE: 13.851 Val MAE: 18.063 Epoch time: 97.979 seconds \n",
      "Epoch: 59 Train loss: 4.275 Aux train loss: 2.702 Val loss: 2.594 Aux val loss: 1.625 Train MAE: 12.539 Val MAE: 13.228 Epoch time: 98.199 seconds best\n",
      "Epoch: 60 Train loss: 4.433 Aux train loss: 2.772 Val loss: 2.700 Aux val loss: 1.660 Train MAE: 13.087 Val MAE: 15.128 Epoch time: 95.117 seconds \n",
      "Epoch: 61 Train loss: 4.718 Aux train loss: 2.976 Val loss: 2.723 Aux val loss: 1.705 Train MAE: 14.194 Val MAE: 14.734 Epoch time: 94.796 seconds \n",
      "Epoch: 62 Train loss: 4.875 Aux train loss: 3.050 Val loss: 2.725 Aux val loss: 1.693 Train MAE: 13.289 Val MAE: 15.200 Epoch time: 94.121 seconds \n",
      "Epoch: 63 Train loss: 4.569 Aux train loss: 2.881 Val loss: 2.673 Aux val loss: 1.657 Train MAE: 13.207 Val MAE: 21.237 Epoch time: 94.419 seconds \n",
      "Epoch: 64 Train loss: 4.629 Aux train loss: 2.997 Val loss: 2.635 Aux val loss: 1.631 Train MAE: 12.945 Val MAE: 16.892 Epoch time: 95.537 seconds \n",
      "Epoch: 65 Train loss: 5.039 Aux train loss: 3.169 Val loss: 2.669 Aux val loss: 1.633 Train MAE: 14.823 Val MAE: 19.415 Epoch time: 99.528 seconds \n",
      "Epoch: 66 Train loss: 4.904 Aux train loss: 3.136 Val loss: 2.727 Aux val loss: 1.650 Train MAE: 12.840 Val MAE: 17.607 Epoch time: 95.845 seconds \n",
      "Epoch: 67 Train loss: 4.832 Aux train loss: 3.062 Val loss: 2.616 Aux val loss: 1.615 Train MAE: 15.012 Val MAE: 15.908 Epoch time: 96.826 seconds \n",
      "Epoch: 68 Train loss: 4.787 Aux train loss: 3.032 Val loss: 2.652 Aux val loss: 1.629 Train MAE: 12.669 Val MAE: 14.138 Epoch time: 94.401 seconds \n",
      "Epoch: 69 Train loss: 4.596 Aux train loss: 2.896 Val loss: 2.580 Aux val loss: 1.595 Train MAE: 11.554 Val MAE: 16.996 Epoch time: 94.655 seconds \n",
      "Epoch: 70 Train loss: 4.626 Aux train loss: 2.887 Val loss: 2.592 Aux val loss: 1.600 Train MAE: 12.366 Val MAE: 14.398 Epoch time: 94.351 seconds \n",
      "Epoch: 71 Train loss: 4.800 Aux train loss: 3.032 Val loss: 2.617 Aux val loss: 1.635 Train MAE: 12.870 Val MAE: 14.569 Epoch time: 94.129 seconds \n",
      "Epoch: 72 Train loss: 5.059 Aux train loss: 3.109 Val loss: 2.673 Aux val loss: 1.668 Train MAE: 15.101 Val MAE: 18.183 Epoch time: 95.708 seconds \n",
      "Epoch: 73 Train loss: 4.867 Aux train loss: 3.043 Val loss: 2.730 Aux val loss: 1.664 Train MAE: 13.181 Val MAE: 16.429 Epoch time: 98.114 seconds \n",
      "Epoch: 74 Train loss: 4.841 Aux train loss: 3.035 Val loss: 2.601 Aux val loss: 1.619 Train MAE: 13.325 Val MAE: 20.087 Epoch time: 94.338 seconds \n",
      "Epoch: 75 Train loss: 4.875 Aux train loss: 3.025 Val loss: 2.608 Aux val loss: 1.615 Train MAE: 13.817 Val MAE: 14.273 Epoch time: 97.130 seconds \n",
      "Epoch: 76 Train loss: 4.503 Aux train loss: 2.838 Val loss: 2.583 Aux val loss: 1.604 Train MAE: 12.026 Val MAE: 14.520 Epoch time: 94.836 seconds \n",
      "Epoch: 77 Train loss: 4.270 Aux train loss: 2.683 Val loss: 2.599 Aux val loss: 1.608 Train MAE: 11.319 Val MAE: 14.151 Epoch time: 96.117 seconds \n",
      "Epoch: 78 Train loss: 4.609 Aux train loss: 2.908 Val loss: 2.574 Aux val loss: 1.610 Train MAE: 12.858 Val MAE: 12.735 Epoch time: 94.563 seconds best\n",
      "Epoch: 79 Train loss: 4.313 Aux train loss: 2.732 Val loss: 2.569 Aux val loss: 1.599 Train MAE: 10.948 Val MAE: 15.168 Epoch time: 96.201 seconds \n",
      "Epoch: 80 Train loss: 4.398 Aux train loss: 2.744 Val loss: 2.618 Aux val loss: 1.615 Train MAE: 12.897 Val MAE: 14.578 Epoch time: 96.380 seconds \n",
      "Epoch: 81 Train loss: 4.686 Aux train loss: 2.965 Val loss: 2.588 Aux val loss: 1.614 Train MAE: 12.478 Val MAE: 12.487 Epoch time: 98.079 seconds best\n",
      "Epoch: 82 Train loss: 4.821 Aux train loss: 3.033 Val loss: 2.628 Aux val loss: 1.649 Train MAE: 14.815 Val MAE: 16.759 Epoch time: 93.728 seconds \n",
      "Epoch: 83 Train loss: 4.325 Aux train loss: 2.739 Val loss: 2.603 Aux val loss: 1.603 Train MAE: 12.911 Val MAE: 13.877 Epoch time: 94.002 seconds \n",
      "Epoch: 84 Train loss: 4.274 Aux train loss: 2.694 Val loss: 2.687 Aux val loss: 1.636 Train MAE: 11.494 Val MAE: 13.112 Epoch time: 93.775 seconds \n",
      "Epoch: 85 Train loss: 4.618 Aux train loss: 2.899 Val loss: 2.624 Aux val loss: 1.647 Train MAE: 12.008 Val MAE: 17.128 Epoch time: 98.675 seconds \n",
      "Epoch: 86 Train loss: 4.837 Aux train loss: 2.804 Val loss: 2.618 Aux val loss: 1.616 Train MAE: 13.288 Val MAE: 14.680 Epoch time: 94.880 seconds \n",
      "Epoch: 87 Train loss: 4.916 Aux train loss: 3.083 Val loss: 2.598 Aux val loss: 1.649 Train MAE: 13.342 Val MAE: 18.566 Epoch time: 96.217 seconds \n",
      "Epoch: 88 Train loss: 4.348 Aux train loss: 2.753 Val loss: 2.569 Aux val loss: 1.600 Train MAE: 12.878 Val MAE: 14.418 Epoch time: 94.792 seconds \n",
      "Epoch: 89 Train loss: 4.489 Aux train loss: 2.808 Val loss: 2.497 Aux val loss: 1.549 Train MAE: 12.376 Val MAE: 13.139 Epoch time: 96.731 seconds \n",
      "Epoch: 90 Train loss: 4.494 Aux train loss: 2.829 Val loss: 2.825 Aux val loss: 1.788 Train MAE: 11.205 Val MAE: 19.287 Epoch time: 93.715 seconds \n",
      "Epoch: 91 Train loss: 4.012 Aux train loss: 2.510 Val loss: 2.673 Aux val loss: 1.641 Train MAE: 11.883 Val MAE: 20.628 Epoch time: 94.737 seconds \n",
      "Epoch: 92 Train loss: 4.377 Aux train loss: 2.741 Val loss: 2.600 Aux val loss: 1.592 Train MAE: 11.522 Val MAE: 21.723 Epoch time: 95.215 seconds \n",
      "Epoch: 93 Train loss: 4.704 Aux train loss: 2.912 Val loss: 2.555 Aux val loss: 1.596 Train MAE: 12.820 Val MAE: 15.548 Epoch time: 97.006 seconds \n",
      "Epoch: 94 Train loss: 4.557 Aux train loss: 2.881 Val loss: 2.604 Aux val loss: 1.613 Train MAE: 12.038 Val MAE: 13.467 Epoch time: 99.341 seconds \n",
      "Epoch: 95 Train loss: 4.670 Aux train loss: 2.901 Val loss: 2.689 Aux val loss: 1.636 Train MAE: 15.127 Val MAE: 18.065 Epoch time: 89.158 seconds \n",
      "Epoch: 96 Train loss: 4.551 Aux train loss: 2.835 Val loss: 2.546 Aux val loss: 1.580 Train MAE: 13.147 Val MAE: 21.519 Epoch time: 91.512 seconds \n",
      "Epoch: 97 Train loss: 4.307 Aux train loss: 2.699 Val loss: 2.553 Aux val loss: 1.576 Train MAE: 11.508 Val MAE: 14.168 Epoch time: 90.103 seconds \n",
      "Epoch: 98 Train loss: 4.374 Aux train loss: 2.738 Val loss: 2.594 Aux val loss: 1.592 Train MAE: 10.191 Val MAE: 14.744 Epoch time: 92.636 seconds \n",
      "Epoch: 99 Train loss: 4.549 Aux train loss: 2.846 Val loss: 2.475 Aux val loss: 1.557 Train MAE: 11.756 Val MAE: 12.349 Epoch time: 90.818 seconds best\n",
      "Epoch: 100 Train loss: 4.398 Aux train loss: 2.779 Val loss: 2.476 Aux val loss: 1.547 Train MAE: 10.923 Val MAE: 13.285 Epoch time: 90.068 seconds \n",
      "Epoch: 101 Train loss: 4.112 Aux train loss: 2.584 Val loss: 2.552 Aux val loss: 1.565 Train MAE: 12.300 Val MAE: 14.555 Epoch time: 90.431 seconds \n",
      "Epoch: 102 Train loss: 4.397 Aux train loss: 2.751 Val loss: 2.568 Aux val loss: 1.603 Train MAE: 10.660 Val MAE: 16.374 Epoch time: 92.323 seconds \n",
      "Epoch: 103 Train loss: 4.644 Aux train loss: 2.904 Val loss: 2.518 Aux val loss: 1.584 Train MAE: 13.060 Val MAE: 14.012 Epoch time: 92.030 seconds \n",
      "Epoch: 104 Train loss: 4.124 Aux train loss: 2.574 Val loss: 2.535 Aux val loss: 1.581 Train MAE: 10.843 Val MAE: 14.016 Epoch time: 93.243 seconds \n",
      "Epoch: 105 Train loss: 4.051 Aux train loss: 2.535 Val loss: 2.640 Aux val loss: 1.629 Train MAE: 10.154 Val MAE: 16.364 Epoch time: 89.616 seconds \n",
      "Epoch: 106 Train loss: 4.353 Aux train loss: 2.735 Val loss: 2.555 Aux val loss: 1.580 Train MAE: 11.141 Val MAE: 24.490 Epoch time: 91.296 seconds \n",
      "Epoch: 107 Train loss: 4.386 Aux train loss: 2.767 Val loss: 2.499 Aux val loss: 1.570 Train MAE: 11.043 Val MAE: 13.525 Epoch time: 92.659 seconds \n",
      "Epoch: 108 Train loss: 4.477 Aux train loss: 2.818 Val loss: 2.583 Aux val loss: 1.612 Train MAE: 12.738 Val MAE: 13.807 Epoch time: 91.404 seconds \n",
      "Epoch: 109 Train loss: 4.582 Aux train loss: 2.881 Val loss: 2.584 Aux val loss: 1.598 Train MAE: 11.290 Val MAE: 17.264 Epoch time: 90.773 seconds \n",
      "Epoch: 110 Train loss: 4.473 Aux train loss: 2.808 Val loss: 2.505 Aux val loss: 1.572 Train MAE: 11.575 Val MAE: 14.643 Epoch time: 90.534 seconds \n",
      "Epoch: 111 Train loss: 4.302 Aux train loss: 2.670 Val loss: 2.523 Aux val loss: 1.555 Train MAE: 12.136 Val MAE: 14.953 Epoch time: 91.148 seconds \n",
      "Epoch: 112 Train loss: 4.524 Aux train loss: 2.884 Val loss: 2.537 Aux val loss: 1.568 Train MAE: 13.423 Val MAE: 15.712 Epoch time: 92.697 seconds \n",
      "Epoch: 113 Train loss: 4.809 Aux train loss: 2.960 Val loss: 2.516 Aux val loss: 1.542 Train MAE: 12.008 Val MAE: 12.833 Epoch time: 91.145 seconds \n",
      "Epoch: 114 Train loss: 3.906 Aux train loss: 2.456 Val loss: 2.466 Aux val loss: 1.545 Train MAE: 10.453 Val MAE: 12.363 Epoch time: 91.660 seconds \n",
      "Epoch: 115 Train loss: 4.313 Aux train loss: 2.702 Val loss: 2.513 Aux val loss: 1.590 Train MAE: 10.840 Val MAE: 15.077 Epoch time: 90.361 seconds \n",
      "Epoch: 116 Train loss: 4.014 Aux train loss: 2.516 Val loss: 2.489 Aux val loss: 1.548 Train MAE: 11.587 Val MAE: 14.964 Epoch time: 88.363 seconds \n",
      "Epoch: 117 Train loss: 4.501 Aux train loss: 2.843 Val loss: 2.619 Aux val loss: 1.586 Train MAE: 11.693 Val MAE: 14.428 Epoch time: 93.074 seconds \n",
      "Epoch: 118 Train loss: 4.207 Aux train loss: 2.658 Val loss: 2.520 Aux val loss: 1.554 Train MAE: 10.750 Val MAE: 17.277 Epoch time: 90.304 seconds \n",
      "Epoch: 119 Train loss: 4.435 Aux train loss: 2.793 Val loss: 2.527 Aux val loss: 1.557 Train MAE: 12.379 Val MAE: 15.807 Epoch time: 92.353 seconds \n",
      "Epoch: 120 Train loss: 4.547 Aux train loss: 2.855 Val loss: 2.521 Aux val loss: 1.562 Train MAE: 11.203 Val MAE: 15.373 Epoch time: 91.049 seconds \n",
      "Epoch: 121 Train loss: 4.388 Aux train loss: 2.698 Val loss: 2.543 Aux val loss: 1.568 Train MAE: 11.250 Val MAE: 14.301 Epoch time: 92.489 seconds \n",
      "Epoch: 122 Train loss: 4.393 Aux train loss: 2.771 Val loss: 2.510 Aux val loss: 1.557 Train MAE: 10.843 Val MAE: 17.214 Epoch time: 92.107 seconds \n",
      "Epoch: 123 Train loss: 3.774 Aux train loss: 2.374 Val loss: 2.510 Aux val loss: 1.547 Train MAE: 10.428 Val MAE: 16.327 Epoch time: 89.857 seconds \n",
      "Epoch: 124 Train loss: 3.951 Aux train loss: 2.478 Val loss: 2.526 Aux val loss: 1.553 Train MAE: 10.402 Val MAE: 15.041 Epoch time: 90.672 seconds \n",
      "Epoch: 125 Train loss: 4.158 Aux train loss: 2.638 Val loss: 2.505 Aux val loss: 1.538 Train MAE: 10.604 Val MAE: 19.596 Epoch time: 90.669 seconds \n",
      "Epoch: 126 Train loss: 3.871 Aux train loss: 2.445 Val loss: 2.525 Aux val loss: 1.546 Train MAE: 10.529 Val MAE: 18.909 Epoch time: 91.500 seconds \n",
      "Epoch: 127 Train loss: 4.492 Aux train loss: 2.804 Val loss: 2.502 Aux val loss: 1.545 Train MAE: 11.909 Val MAE: 15.051 Epoch time: 91.365 seconds \n",
      "Epoch: 128 Train loss: 4.759 Aux train loss: 2.955 Val loss: 2.539 Aux val loss: 1.559 Train MAE: 13.413 Val MAE: 13.627 Epoch time: 89.025 seconds \n",
      "Epoch: 129 Train loss: 4.149 Aux train loss: 2.597 Val loss: 2.609 Aux val loss: 1.596 Train MAE: 10.342 Val MAE: 14.015 Epoch time: 89.475 seconds \n",
      "Epoch: 130 Train loss: 4.138 Aux train loss: 2.596 Val loss: 2.507 Aux val loss: 1.532 Train MAE: 11.043 Val MAE: 13.878 Epoch time: 89.242 seconds \n",
      "Epoch: 131 Train loss: 3.936 Aux train loss: 2.474 Val loss: 2.541 Aux val loss: 1.564 Train MAE: 9.811 Val MAE: 13.208 Epoch time: 91.652 seconds \n",
      "Epoch: 132 Train loss: 3.970 Aux train loss: 2.481 Val loss: 2.470 Aux val loss: 1.541 Train MAE: 8.803 Val MAE: 16.674 Epoch time: 91.920 seconds \n",
      "Epoch: 133 Train loss: 4.186 Aux train loss: 2.615 Val loss: 2.521 Aux val loss: 1.535 Train MAE: 10.974 Val MAE: 13.132 Epoch time: 90.001 seconds \n",
      "Epoch: 134 Train loss: 4.337 Aux train loss: 2.699 Val loss: 2.490 Aux val loss: 1.548 Train MAE: 11.173 Val MAE: 12.964 Epoch time: 87.406 seconds \n",
      "Epoch: 135 Train loss: 3.947 Aux train loss: 2.464 Val loss: 2.497 Aux val loss: 1.522 Train MAE: 10.175 Val MAE: 15.939 Epoch time: 90.933 seconds \n",
      "Epoch: 136 Train loss: 4.013 Aux train loss: 2.531 Val loss: 2.540 Aux val loss: 1.558 Train MAE: 9.739 Val MAE: 15.475 Epoch time: 90.869 seconds \n",
      "Epoch: 137 Train loss: 3.758 Aux train loss: 2.395 Val loss: 2.514 Aux val loss: 1.552 Train MAE: 9.780 Val MAE: 15.790 Epoch time: 90.845 seconds \n",
      "Epoch: 138 Train loss: 3.805 Aux train loss: 2.387 Val loss: 2.490 Aux val loss: 1.531 Train MAE: 10.967 Val MAE: 14.676 Epoch time: 96.147 seconds \n",
      "Epoch: 139 Train loss: 4.170 Aux train loss: 2.609 Val loss: 2.502 Aux val loss: 1.554 Train MAE: 9.775 Val MAE: 13.674 Epoch time: 90.595 seconds \n",
      "Epoch: 140 Train loss: 4.091 Aux train loss: 2.556 Val loss: 2.512 Aux val loss: 1.548 Train MAE: 9.844 Val MAE: 21.632 Epoch time: 90.833 seconds \n",
      "Epoch: 141 Train loss: 3.805 Aux train loss: 2.392 Val loss: 2.470 Aux val loss: 1.520 Train MAE: 10.318 Val MAE: 13.840 Epoch time: 91.935 seconds \n",
      "Epoch: 142 Train loss: 4.398 Aux train loss: 2.754 Val loss: 2.431 Aux val loss: 1.512 Train MAE: 10.025 Val MAE: 14.159 Epoch time: 89.617 seconds \n",
      "Epoch: 143 Train loss: 4.177 Aux train loss: 2.629 Val loss: 2.450 Aux val loss: 1.511 Train MAE: 10.123 Val MAE: 16.911 Epoch time: 92.400 seconds \n",
      "Epoch: 144 Train loss: 4.016 Aux train loss: 2.497 Val loss: 2.484 Aux val loss: 1.532 Train MAE: 11.051 Val MAE: 15.347 Epoch time: 92.099 seconds \n",
      "Epoch: 145 Train loss: 3.939 Aux train loss: 2.474 Val loss: 2.472 Aux val loss: 1.536 Train MAE: 10.157 Val MAE: 18.505 Epoch time: 93.468 seconds \n",
      "Epoch: 146 Train loss: 3.948 Aux train loss: 2.461 Val loss: 2.477 Aux val loss: 1.524 Train MAE: 10.571 Val MAE: 15.539 Epoch time: 93.567 seconds \n",
      "Epoch: 147 Train loss: 4.270 Aux train loss: 2.685 Val loss: 2.451 Aux val loss: 1.512 Train MAE: 10.983 Val MAE: 13.116 Epoch time: 98.730 seconds \n",
      "Epoch: 148 Train loss: 3.970 Aux train loss: 2.502 Val loss: 2.491 Aux val loss: 1.531 Train MAE: 8.771 Val MAE: 15.648 Epoch time: 96.675 seconds \n",
      "Epoch: 149 Train loss: 4.139 Aux train loss: 2.584 Val loss: 2.464 Aux val loss: 1.518 Train MAE: 9.949 Val MAE: 18.196 Epoch time: 95.872 seconds \n",
      "Epoch: 150 Train loss: 4.125 Aux train loss: 2.572 Val loss: 2.443 Aux val loss: 1.503 Train MAE: 11.019 Val MAE: 13.269 Epoch time: 98.219 seconds \n",
      "Epoch: 151 Train loss: 4.472 Aux train loss: 2.799 Val loss: 2.487 Aux val loss: 1.530 Train MAE: 12.281 Val MAE: 14.462 Epoch time: 94.709 seconds \n",
      "Epoch: 152 Train loss: 3.685 Aux train loss: 2.310 Val loss: 2.485 Aux val loss: 1.525 Train MAE: 9.450 Val MAE: 15.727 Epoch time: 95.893 seconds \n",
      "Epoch: 153 Train loss: 3.985 Aux train loss: 2.489 Val loss: 2.487 Aux val loss: 1.521 Train MAE: 9.971 Val MAE: 17.239 Epoch time: 94.827 seconds \n",
      "Epoch: 154 Train loss: 4.222 Aux train loss: 2.659 Val loss: 2.452 Aux val loss: 1.504 Train MAE: 10.027 Val MAE: 13.770 Epoch time: 94.523 seconds \n",
      "Epoch: 155 Train loss: 3.930 Aux train loss: 2.488 Val loss: 2.523 Aux val loss: 1.559 Train MAE: 9.320 Val MAE: 13.671 Epoch time: 94.460 seconds \n",
      "Epoch: 156 Train loss: 3.974 Aux train loss: 2.507 Val loss: 2.517 Aux val loss: 1.521 Train MAE: 9.590 Val MAE: 15.405 Epoch time: 93.629 seconds \n",
      "Epoch: 157 Train loss: 3.852 Aux train loss: 2.414 Val loss: 2.475 Aux val loss: 1.522 Train MAE: 9.277 Val MAE: 13.084 Epoch time: 94.764 seconds \n",
      "Epoch: 158 Train loss: 3.960 Aux train loss: 2.498 Val loss: 2.507 Aux val loss: 1.532 Train MAE: 11.175 Val MAE: 13.797 Epoch time: 93.869 seconds \n",
      "Epoch: 159 Train loss: 4.526 Aux train loss: 2.854 Val loss: 2.424 Aux val loss: 1.492 Train MAE: 11.818 Val MAE: 13.928 Epoch time: 98.679 seconds \n",
      "Epoch: 160 Train loss: 3.860 Aux train loss: 2.430 Val loss: 2.420 Aux val loss: 1.503 Train MAE: 10.452 Val MAE: 12.009 Epoch time: 95.121 seconds best\n",
      "Epoch: 161 Train loss: 3.921 Aux train loss: 2.461 Val loss: 2.459 Aux val loss: 1.514 Train MAE: 9.393 Val MAE: 17.847 Epoch time: 92.543 seconds \n",
      "Epoch: 162 Train loss: 4.377 Aux train loss: 2.736 Val loss: 2.471 Aux val loss: 1.558 Train MAE: 10.024 Val MAE: 18.833 Epoch time: 96.481 seconds \n",
      "Epoch: 163 Train loss: 4.091 Aux train loss: 2.579 Val loss: 2.436 Aux val loss: 1.536 Train MAE: 9.400 Val MAE: 14.010 Epoch time: 93.863 seconds \n",
      "Epoch: 164 Train loss: 4.059 Aux train loss: 2.543 Val loss: 2.476 Aux val loss: 1.531 Train MAE: 10.447 Val MAE: 14.051 Epoch time: 95.387 seconds \n",
      "Epoch: 165 Train loss: 3.804 Aux train loss: 2.398 Val loss: 2.436 Aux val loss: 1.509 Train MAE: 10.217 Val MAE: 13.505 Epoch time: 95.678 seconds \n",
      "Epoch: 166 Train loss: 4.206 Aux train loss: 2.638 Val loss: 2.458 Aux val loss: 1.500 Train MAE: 10.222 Val MAE: 17.014 Epoch time: 96.396 seconds \n",
      "Epoch: 167 Train loss: 4.404 Aux train loss: 2.749 Val loss: 2.423 Aux val loss: 1.503 Train MAE: 11.254 Val MAE: 13.313 Epoch time: 94.243 seconds \n",
      "Epoch: 168 Train loss: 4.338 Aux train loss: 2.702 Val loss: 2.482 Aux val loss: 1.531 Train MAE: 10.895 Val MAE: 18.593 Epoch time: 94.952 seconds \n",
      "Epoch: 169 Train loss: 3.932 Aux train loss: 2.493 Val loss: 2.433 Aux val loss: 1.494 Train MAE: 9.656 Val MAE: 11.647 Epoch time: 115.705 seconds best\n",
      "Epoch: 170 Train loss: 4.607 Aux train loss: 2.873 Val loss: 2.452 Aux val loss: 1.491 Train MAE: 11.161 Val MAE: 14.099 Epoch time: 95.693 seconds \n",
      "Epoch: 171 Train loss: 3.858 Aux train loss: 2.422 Val loss: 2.494 Aux val loss: 1.537 Train MAE: 9.956 Val MAE: 12.786 Epoch time: 93.602 seconds \n",
      "Epoch: 172 Train loss: 4.187 Aux train loss: 2.632 Val loss: 2.483 Aux val loss: 1.522 Train MAE: 9.713 Val MAE: 12.868 Epoch time: 92.882 seconds \n",
      "Epoch: 173 Train loss: 3.834 Aux train loss: 2.402 Val loss: 2.466 Aux val loss: 1.517 Train MAE: 9.639 Val MAE: 13.116 Epoch time: 97.240 seconds \n",
      "Epoch: 174 Train loss: 4.063 Aux train loss: 2.552 Val loss: 2.510 Aux val loss: 1.516 Train MAE: 9.928 Val MAE: 15.451 Epoch time: 98.812 seconds \n",
      "Epoch: 175 Train loss: 3.819 Aux train loss: 2.401 Val loss: 2.519 Aux val loss: 1.574 Train MAE: 9.735 Val MAE: 22.682 Epoch time: 90.717 seconds \n",
      "Epoch: 176 Train loss: 3.941 Aux train loss: 2.519 Val loss: 2.497 Aux val loss: 1.544 Train MAE: 9.145 Val MAE: 12.515 Epoch time: 93.978 seconds \n",
      "Epoch: 177 Train loss: 3.814 Aux train loss: 2.393 Val loss: 2.470 Aux val loss: 1.519 Train MAE: 8.734 Val MAE: 14.008 Epoch time: 94.188 seconds \n",
      "Epoch: 178 Train loss: 4.041 Aux train loss: 2.535 Val loss: 2.510 Aux val loss: 1.536 Train MAE: 9.709 Val MAE: 17.820 Epoch time: 94.097 seconds \n",
      "Epoch: 179 Train loss: 3.927 Aux train loss: 2.484 Val loss: 2.480 Aux val loss: 1.532 Train MAE: 10.617 Val MAE: 19.043 Epoch time: 95.286 seconds \n",
      "Epoch: 180 Train loss: 3.864 Aux train loss: 2.440 Val loss: 2.426 Aux val loss: 1.485 Train MAE: 8.770 Val MAE: 13.805 Epoch time: 96.973 seconds \n",
      "Epoch: 181 Train loss: 3.653 Aux train loss: 2.281 Val loss: 2.493 Aux val loss: 1.502 Train MAE: 9.474 Val MAE: 16.706 Epoch time: 91.713 seconds \n",
      "Epoch: 182 Train loss: 4.106 Aux train loss: 2.556 Val loss: 2.404 Aux val loss: 1.490 Train MAE: 10.807 Val MAE: 13.440 Epoch time: 94.605 seconds \n",
      "Epoch: 183 Train loss: 4.332 Aux train loss: 2.708 Val loss: 2.435 Aux val loss: 1.493 Train MAE: 10.197 Val MAE: 13.905 Epoch time: 93.947 seconds \n",
      "Epoch: 184 Train loss: 4.119 Aux train loss: 2.586 Val loss: 2.502 Aux val loss: 1.550 Train MAE: 9.943 Val MAE: 14.513 Epoch time: 95.390 seconds \n",
      "Epoch: 185 Train loss: 4.158 Aux train loss: 2.593 Val loss: 2.459 Aux val loss: 1.524 Train MAE: 9.136 Val MAE: 15.929 Epoch time: 93.585 seconds \n",
      "Epoch: 186 Train loss: 4.233 Aux train loss: 2.650 Val loss: 2.407 Aux val loss: 1.495 Train MAE: 9.445 Val MAE: 18.201 Epoch time: 96.737 seconds \n",
      "Epoch: 187 Train loss: 4.413 Aux train loss: 2.739 Val loss: 2.484 Aux val loss: 1.510 Train MAE: 11.911 Val MAE: 15.683 Epoch time: 94.248 seconds \n",
      "Epoch: 188 Train loss: 3.654 Aux train loss: 2.302 Val loss: 2.566 Aux val loss: 1.563 Train MAE: 9.767 Val MAE: 14.111 Epoch time: 93.266 seconds \n",
      "Epoch: 189 Train loss: 4.168 Aux train loss: 2.609 Val loss: 2.462 Aux val loss: 1.508 Train MAE: 9.702 Val MAE: 14.846 Epoch time: 94.537 seconds \n",
      "Epoch: 190 Train loss: 4.241 Aux train loss: 2.669 Val loss: 2.385 Aux val loss: 1.485 Train MAE: 8.920 Val MAE: 19.067 Epoch time: 97.551 seconds \n",
      "Epoch: 191 Train loss: 3.591 Aux train loss: 2.234 Val loss: 2.473 Aux val loss: 1.502 Train MAE: 9.920 Val MAE: 13.259 Epoch time: 93.244 seconds \n",
      "Epoch: 192 Train loss: 4.218 Aux train loss: 2.648 Val loss: 2.471 Aux val loss: 1.510 Train MAE: 9.055 Val MAE: 12.704 Epoch time: 94.420 seconds \n",
      "Epoch: 193 Train loss: 4.285 Aux train loss: 2.692 Val loss: 2.495 Aux val loss: 1.508 Train MAE: 9.624 Val MAE: 14.483 Epoch time: 96.878 seconds \n",
      "Epoch: 194 Train loss: 4.207 Aux train loss: 2.629 Val loss: 2.482 Aux val loss: 1.516 Train MAE: 9.647 Val MAE: 13.327 Epoch time: 95.732 seconds \n",
      "Epoch: 195 Train loss: 4.227 Aux train loss: 2.631 Val loss: 2.489 Aux val loss: 1.496 Train MAE: 11.349 Val MAE: 13.550 Epoch time: 94.329 seconds \n",
      "Epoch: 196 Train loss: 3.940 Aux train loss: 2.458 Val loss: 2.515 Aux val loss: 1.562 Train MAE: 10.263 Val MAE: 14.719 Epoch time: 93.103 seconds \n",
      "Epoch: 197 Train loss: 3.823 Aux train loss: 2.420 Val loss: 2.504 Aux val loss: 1.512 Train MAE: 9.022 Val MAE: 18.170 Epoch time: 93.488 seconds \n",
      "Epoch: 198 Train loss: 4.460 Aux train loss: 2.809 Val loss: 2.446 Aux val loss: 1.499 Train MAE: 10.835 Val MAE: 15.564 Epoch time: 93.671 seconds \n",
      "Epoch: 199 Train loss: 3.793 Aux train loss: 2.378 Val loss: 2.461 Aux val loss: 1.499 Train MAE: 9.351 Val MAE: 15.655 Epoch time: 94.403 seconds \n",
      "Epoch: 200 Train loss: 3.773 Aux train loss: 2.382 Val loss: 2.441 Aux val loss: 1.492 Train MAE: 9.451 Val MAE: 20.336 Epoch time: 91.654 seconds \n",
      "Epoch: 201 Train loss: 3.660 Aux train loss: 2.302 Val loss: 2.392 Aux val loss: 1.467 Train MAE: 7.777 Val MAE: 12.247 Epoch time: 92.648 seconds \n",
      "Epoch: 202 Train loss: 3.431 Aux train loss: 2.149 Val loss: 2.388 Aux val loss: 1.463 Train MAE: 6.402 Val MAE: 12.819 Epoch time: 93.299 seconds \n",
      "Epoch: 203 Train loss: 3.777 Aux train loss: 2.369 Val loss: 2.369 Aux val loss: 1.455 Train MAE: 5.708 Val MAE: 11.937 Epoch time: 94.003 seconds \n",
      "Epoch: 204 Train loss: 3.550 Aux train loss: 2.233 Val loss: 2.396 Aux val loss: 1.469 Train MAE: 6.011 Val MAE: 11.850 Epoch time: 94.616 seconds \n",
      "Epoch: 205 Train loss: 3.649 Aux train loss: 2.292 Val loss: 2.390 Aux val loss: 1.456 Train MAE: 5.830 Val MAE: 12.041 Epoch time: 95.117 seconds \n",
      "Epoch: 206 Train loss: 3.641 Aux train loss: 2.285 Val loss: 2.429 Aux val loss: 1.499 Train MAE: 6.351 Val MAE: 12.499 Epoch time: 97.699 seconds \n",
      "Epoch: 207 Train loss: 3.561 Aux train loss: 2.235 Val loss: 2.407 Aux val loss: 1.462 Train MAE: 6.306 Val MAE: 14.019 Epoch time: 95.755 seconds \n",
      "Epoch: 208 Train loss: 4.139 Aux train loss: 2.594 Val loss: 2.401 Aux val loss: 1.461 Train MAE: 5.843 Val MAE: 11.998 Epoch time: 94.987 seconds \n",
      "Epoch: 209 Train loss: 4.132 Aux train loss: 2.593 Val loss: 2.409 Aux val loss: 1.463 Train MAE: 6.500 Val MAE: 12.488 Epoch time: 94.687 seconds \n",
      "Epoch: 210 Train loss: 3.670 Aux train loss: 2.294 Val loss: 2.387 Aux val loss: 1.462 Train MAE: 5.986 Val MAE: 12.134 Epoch time: 94.884 seconds \n",
      "Epoch: 211 Train loss: 3.967 Aux train loss: 2.490 Val loss: 2.385 Aux val loss: 1.464 Train MAE: 6.108 Val MAE: 12.266 Epoch time: 96.279 seconds \n",
      "Epoch: 212 Train loss: 3.472 Aux train loss: 2.173 Val loss: 2.415 Aux val loss: 1.480 Train MAE: 5.670 Val MAE: 12.643 Epoch time: 94.608 seconds \n",
      "Epoch: 213 Train loss: 3.744 Aux train loss: 2.338 Val loss: 2.424 Aux val loss: 1.477 Train MAE: 5.625 Val MAE: 12.805 Epoch time: 96.322 seconds \n",
      "Epoch: 214 Train loss: 3.514 Aux train loss: 2.218 Val loss: 2.429 Aux val loss: 1.479 Train MAE: 5.865 Val MAE: 13.805 Epoch time: 95.609 seconds \n",
      "Epoch: 215 Train loss: 3.608 Aux train loss: 2.265 Val loss: 2.408 Aux val loss: 1.475 Train MAE: 5.954 Val MAE: 12.409 Epoch time: 96.227 seconds \n",
      "Epoch: 216 Train loss: 3.494 Aux train loss: 2.199 Val loss: 2.412 Aux val loss: 1.485 Train MAE: 5.766 Val MAE: 13.969 Epoch time: 89.149 seconds \n",
      "Epoch: 217 Train loss: 3.741 Aux train loss: 2.359 Val loss: 2.395 Aux val loss: 1.460 Train MAE: 5.985 Val MAE: 12.196 Epoch time: 90.289 seconds \n",
      "Epoch: 218 Train loss: 3.859 Aux train loss: 2.426 Val loss: 2.422 Aux val loss: 1.479 Train MAE: 5.807 Val MAE: 13.208 Epoch time: 90.893 seconds \n",
      "Epoch: 219 Train loss: 3.437 Aux train loss: 2.150 Val loss: 2.426 Aux val loss: 1.477 Train MAE: 5.338 Val MAE: 13.008 Epoch time: 90.251 seconds \n",
      "Epoch: 220 Train loss: 3.503 Aux train loss: 2.199 Val loss: 2.417 Aux val loss: 1.478 Train MAE: 5.639 Val MAE: 12.422 Epoch time: 92.310 seconds \n",
      "Epoch: 221 Train loss: 3.556 Aux train loss: 2.235 Val loss: 2.406 Aux val loss: 1.468 Train MAE: 5.831 Val MAE: 13.000 Epoch time: 92.084 seconds \n",
      "Epoch: 222 Train loss: 3.555 Aux train loss: 2.237 Val loss: 2.401 Aux val loss: 1.470 Train MAE: 5.611 Val MAE: 13.216 Epoch time: 92.991 seconds \n",
      "Epoch: 223 Train loss: 3.583 Aux train loss: 2.245 Val loss: 2.411 Aux val loss: 1.473 Train MAE: 6.074 Val MAE: 13.093 Epoch time: 89.540 seconds \n",
      "Epoch: 224 Train loss: 3.858 Aux train loss: 2.418 Val loss: 2.394 Aux val loss: 1.464 Train MAE: 6.228 Val MAE: 12.228 Epoch time: 92.736 seconds \n",
      "Epoch: 225 Train loss: 3.630 Aux train loss: 2.270 Val loss: 2.419 Aux val loss: 1.473 Train MAE: 5.847 Val MAE: 12.805 Epoch time: 89.802 seconds \n",
      "Epoch: 226 Train loss: 3.661 Aux train loss: 2.315 Val loss: 2.386 Aux val loss: 1.460 Train MAE: 5.721 Val MAE: 13.029 Epoch time: 90.876 seconds \n",
      "Epoch: 227 Train loss: 3.548 Aux train loss: 2.235 Val loss: 2.399 Aux val loss: 1.456 Train MAE: 5.804 Val MAE: 12.570 Epoch time: 90.765 seconds \n",
      "Epoch: 228 Train loss: 3.711 Aux train loss: 2.315 Val loss: 2.385 Aux val loss: 1.459 Train MAE: 5.874 Val MAE: 12.260 Epoch time: 90.717 seconds \n",
      "Epoch: 229 Train loss: 3.783 Aux train loss: 2.363 Val loss: 2.401 Aux val loss: 1.474 Train MAE: 5.472 Val MAE: 12.521 Epoch time: 88.854 seconds \n",
      "Epoch: 230 Train loss: 3.587 Aux train loss: 2.253 Val loss: 2.412 Aux val loss: 1.473 Train MAE: 5.639 Val MAE: 13.629 Epoch time: 90.969 seconds \n",
      "Epoch: 231 Train loss: 3.611 Aux train loss: 2.275 Val loss: 2.418 Aux val loss: 1.471 Train MAE: 5.419 Val MAE: 13.750 Epoch time: 88.511 seconds \n",
      "Epoch: 232 Train loss: 3.446 Aux train loss: 2.159 Val loss: 2.398 Aux val loss: 1.473 Train MAE: 5.968 Val MAE: 12.600 Epoch time: 91.502 seconds \n",
      "Epoch: 233 Train loss: 3.594 Aux train loss: 2.268 Val loss: 2.423 Aux val loss: 1.483 Train MAE: 5.643 Val MAE: 12.918 Epoch time: 90.463 seconds \n",
      "Epoch: 234 Train loss: 3.562 Aux train loss: 2.238 Val loss: 2.418 Aux val loss: 1.491 Train MAE: 5.333 Val MAE: 12.988 Epoch time: 90.465 seconds \n",
      "Epoch: 235 Train loss: 3.759 Aux train loss: 2.348 Val loss: 2.394 Aux val loss: 1.487 Train MAE: 6.063 Val MAE: 13.320 Epoch time: 90.610 seconds \n",
      "Epoch: 236 Train loss: 3.434 Aux train loss: 2.157 Val loss: 2.426 Aux val loss: 1.478 Train MAE: 5.854 Val MAE: 14.393 Epoch time: 89.557 seconds \n",
      "Epoch: 237 Train loss: 3.606 Aux train loss: 2.271 Val loss: 2.422 Aux val loss: 1.479 Train MAE: 5.310 Val MAE: 12.602 Epoch time: 88.565 seconds \n",
      "Epoch: 238 Train loss: 3.587 Aux train loss: 2.250 Val loss: 2.417 Aux val loss: 1.482 Train MAE: 5.690 Val MAE: 13.661 Epoch time: 89.893 seconds \n",
      "Epoch: 239 Train loss: 3.729 Aux train loss: 2.322 Val loss: 2.439 Aux val loss: 1.498 Train MAE: 5.734 Val MAE: 12.765 Epoch time: 90.541 seconds \n",
      "Epoch: 240 Train loss: 3.717 Aux train loss: 2.326 Val loss: 2.433 Aux val loss: 1.483 Train MAE: 6.047 Val MAE: 13.989 Epoch time: 90.062 seconds \n",
      "Epoch: 241 Train loss: 3.576 Aux train loss: 2.240 Val loss: 2.412 Aux val loss: 1.479 Train MAE: 5.944 Val MAE: 13.235 Epoch time: 89.942 seconds \n",
      "Epoch: 242 Train loss: 3.769 Aux train loss: 2.356 Val loss: 2.382 Aux val loss: 1.469 Train MAE: 5.863 Val MAE: 12.745 Epoch time: 91.550 seconds \n",
      "Epoch: 243 Train loss: 3.526 Aux train loss: 2.217 Val loss: 2.397 Aux val loss: 1.465 Train MAE: 5.768 Val MAE: 12.961 Epoch time: 90.415 seconds \n",
      "Epoch: 244 Train loss: 3.676 Aux train loss: 2.300 Val loss: 2.372 Aux val loss: 1.455 Train MAE: 5.677 Val MAE: 13.870 Epoch time: 90.223 seconds \n",
      "Epoch: 245 Train loss: 3.805 Aux train loss: 2.386 Val loss: 2.417 Aux val loss: 1.478 Train MAE: 5.776 Val MAE: 13.092 Epoch time: 89.164 seconds \n",
      "Epoch: 246 Train loss: 3.462 Aux train loss: 2.176 Val loss: 2.422 Aux val loss: 1.485 Train MAE: 5.514 Val MAE: 13.229 Epoch time: 89.743 seconds \n",
      "Epoch: 247 Train loss: 3.360 Aux train loss: 2.108 Val loss: 2.396 Aux val loss: 1.467 Train MAE: 5.576 Val MAE: 12.625 Epoch time: 88.839 seconds \n",
      "Epoch: 248 Train loss: 3.437 Aux train loss: 2.159 Val loss: 2.414 Aux val loss: 1.482 Train MAE: 5.571 Val MAE: 13.009 Epoch time: 90.708 seconds \n",
      "Epoch: 249 Train loss: 3.635 Aux train loss: 2.295 Val loss: 2.431 Aux val loss: 1.485 Train MAE: 5.451 Val MAE: 13.504 Epoch time: 90.955 seconds \n",
      "Epoch: 250 Train loss: 3.452 Aux train loss: 2.175 Val loss: 2.433 Aux val loss: 1.482 Train MAE: 6.022 Val MAE: 12.749 Epoch time: 88.235 seconds \n",
      "Epoch: 251 Train loss: 3.657 Aux train loss: 2.289 Val loss: 2.394 Aux val loss: 1.467 Train MAE: 5.573 Val MAE: 12.238 Epoch time: 91.357 seconds \n",
      "Epoch: 252 Train loss: 3.315 Aux train loss: 2.092 Val loss: 2.404 Aux val loss: 1.462 Train MAE: 5.467 Val MAE: 13.549 Epoch time: 90.609 seconds \n",
      "Epoch: 253 Train loss: 3.923 Aux train loss: 2.461 Val loss: 2.420 Aux val loss: 1.469 Train MAE: 5.529 Val MAE: 13.221 Epoch time: 91.341 seconds \n",
      "Epoch: 254 Train loss: 3.518 Aux train loss: 2.201 Val loss: 2.415 Aux val loss: 1.470 Train MAE: 6.099 Val MAE: 13.073 Epoch time: 92.274 seconds \n",
      "Epoch: 255 Train loss: 3.482 Aux train loss: 2.182 Val loss: 2.432 Aux val loss: 1.484 Train MAE: 5.434 Val MAE: 12.219 Epoch time: 90.187 seconds \n",
      "Epoch: 256 Train loss: 3.678 Aux train loss: 2.294 Val loss: 2.388 Aux val loss: 1.466 Train MAE: 5.612 Val MAE: 12.601 Epoch time: 93.013 seconds \n",
      "Epoch: 257 Train loss: 3.371 Aux train loss: 2.128 Val loss: 2.452 Aux val loss: 1.485 Train MAE: 5.706 Val MAE: 13.346 Epoch time: 88.744 seconds \n",
      "Epoch: 258 Train loss: 3.578 Aux train loss: 2.235 Val loss: 2.461 Aux val loss: 1.472 Train MAE: 5.460 Val MAE: 13.011 Epoch time: 91.630 seconds \n",
      "Epoch: 259 Train loss: 3.667 Aux train loss: 2.302 Val loss: 2.432 Aux val loss: 1.478 Train MAE: 5.504 Val MAE: 12.609 Epoch time: 89.301 seconds \n",
      "Epoch: 260 Train loss: 3.354 Aux train loss: 2.098 Val loss: 2.402 Aux val loss: 1.464 Train MAE: 5.794 Val MAE: 12.904 Epoch time: 90.814 seconds \n",
      "Epoch: 261 Train loss: 3.708 Aux train loss: 2.333 Val loss: 2.452 Aux val loss: 1.490 Train MAE: 5.587 Val MAE: 12.915 Epoch time: 94.323 seconds \n",
      "Epoch: 262 Train loss: 3.648 Aux train loss: 2.284 Val loss: 2.430 Aux val loss: 1.469 Train MAE: 5.710 Val MAE: 13.254 Epoch time: 88.679 seconds \n",
      "Epoch: 263 Train loss: 3.173 Aux train loss: 1.990 Val loss: 2.461 Aux val loss: 1.486 Train MAE: 5.437 Val MAE: 14.491 Epoch time: 90.863 seconds \n",
      "Epoch: 264 Train loss: 3.545 Aux train loss: 2.217 Val loss: 2.408 Aux val loss: 1.462 Train MAE: 5.716 Val MAE: 12.486 Epoch time: 93.850 seconds \n",
      "Epoch: 265 Train loss: 3.023 Aux train loss: 1.908 Val loss: 2.423 Aux val loss: 1.475 Train MAE: 5.027 Val MAE: 13.337 Epoch time: 89.655 seconds \n",
      "Epoch: 266 Train loss: 3.449 Aux train loss: 2.165 Val loss: 2.394 Aux val loss: 1.458 Train MAE: 5.309 Val MAE: 12.315 Epoch time: 91.027 seconds \n",
      "Epoch: 267 Train loss: 3.368 Aux train loss: 2.104 Val loss: 2.399 Aux val loss: 1.459 Train MAE: 5.626 Val MAE: 12.709 Epoch time: 94.863 seconds \n",
      "Epoch: 268 Train loss: 3.676 Aux train loss: 2.300 Val loss: 2.427 Aux val loss: 1.486 Train MAE: 5.490 Val MAE: 13.323 Epoch time: 89.432 seconds \n",
      "Epoch: 269 Train loss: 3.608 Aux train loss: 2.258 Val loss: 2.395 Aux val loss: 1.466 Train MAE: 5.850 Val MAE: 14.020 Epoch time: 87.986 seconds \n",
      "Epoch: 270 Train loss: 3.836 Aux train loss: 2.405 Val loss: 2.421 Aux val loss: 1.487 Train MAE: 6.008 Val MAE: 13.597 Epoch time: 90.431 seconds \n",
      "Epoch: 271 Train loss: 3.658 Aux train loss: 2.298 Val loss: 2.421 Aux val loss: 1.481 Train MAE: 5.584 Val MAE: 12.894 Epoch time: 92.962 seconds \n",
      "Epoch: 272 Train loss: 3.666 Aux train loss: 2.300 Val loss: 2.449 Aux val loss: 1.483 Train MAE: 6.274 Val MAE: 13.531 Epoch time: 90.978 seconds \n",
      "Epoch: 273 Train loss: 3.594 Aux train loss: 2.243 Val loss: 2.412 Aux val loss: 1.466 Train MAE: 5.579 Val MAE: 12.843 Epoch time: 90.419 seconds \n",
      "Epoch: 274 Train loss: 3.735 Aux train loss: 2.333 Val loss: 2.416 Aux val loss: 1.472 Train MAE: 6.206 Val MAE: 13.960 Epoch time: 90.173 seconds \n",
      "Epoch: 275 Train loss: 3.299 Aux train loss: 2.076 Val loss: 2.413 Aux val loss: 1.466 Train MAE: 5.588 Val MAE: 12.734 Epoch time: 90.336 seconds \n",
      "Epoch: 276 Train loss: 3.313 Aux train loss: 2.086 Val loss: 2.410 Aux val loss: 1.471 Train MAE: 5.629 Val MAE: 13.397 Epoch time: 92.364 seconds \n",
      "Epoch: 277 Train loss: 3.796 Aux train loss: 2.373 Val loss: 2.432 Aux val loss: 1.486 Train MAE: 6.113 Val MAE: 14.542 Epoch time: 90.974 seconds \n",
      "Epoch: 278 Train loss: 3.582 Aux train loss: 2.245 Val loss: 2.446 Aux val loss: 1.491 Train MAE: 6.257 Val MAE: 14.109 Epoch time: 90.836 seconds \n",
      "Epoch: 279 Train loss: 3.453 Aux train loss: 2.158 Val loss: 2.431 Aux val loss: 1.495 Train MAE: 5.758 Val MAE: 14.345 Epoch time: 91.116 seconds \n",
      "Epoch: 280 Train loss: 3.304 Aux train loss: 2.070 Val loss: 2.420 Aux val loss: 1.482 Train MAE: 5.398 Val MAE: 12.421 Epoch time: 90.657 seconds \n",
      "Epoch: 281 Train loss: 3.731 Aux train loss: 2.326 Val loss: 2.403 Aux val loss: 1.474 Train MAE: 5.846 Val MAE: 13.219 Epoch time: 91.950 seconds \n",
      "Epoch: 282 Train loss: 3.520 Aux train loss: 2.202 Val loss: 2.398 Aux val loss: 1.476 Train MAE: 5.218 Val MAE: 12.632 Epoch time: 93.084 seconds \n",
      "Epoch: 283 Train loss: 3.339 Aux train loss: 2.114 Val loss: 2.401 Aux val loss: 1.463 Train MAE: 5.486 Val MAE: 12.800 Epoch time: 91.776 seconds \n",
      "Epoch: 284 Train loss: 3.321 Aux train loss: 2.088 Val loss: 2.392 Aux val loss: 1.468 Train MAE: 5.852 Val MAE: 13.833 Epoch time: 88.469 seconds \n",
      "Epoch: 285 Train loss: 3.305 Aux train loss: 2.073 Val loss: 2.375 Aux val loss: 1.449 Train MAE: 5.487 Val MAE: 12.626 Epoch time: 88.469 seconds \n",
      "Epoch: 286 Train loss: 3.391 Aux train loss: 2.127 Val loss: 2.396 Aux val loss: 1.458 Train MAE: 5.924 Val MAE: 13.539 Epoch time: 92.287 seconds \n",
      "Epoch: 287 Train loss: 3.655 Aux train loss: 2.294 Val loss: 2.384 Aux val loss: 1.456 Train MAE: 6.130 Val MAE: 13.586 Epoch time: 92.708 seconds \n",
      "Epoch: 288 Train loss: 3.303 Aux train loss: 2.077 Val loss: 2.397 Aux val loss: 1.455 Train MAE: 5.622 Val MAE: 11.998 Epoch time: 88.693 seconds \n",
      "Epoch: 289 Train loss: 3.703 Aux train loss: 2.336 Val loss: 2.395 Aux val loss: 1.471 Train MAE: 5.508 Val MAE: 12.413 Epoch time: 90.071 seconds \n",
      "Epoch: 290 Train loss: 3.715 Aux train loss: 2.333 Val loss: 2.388 Aux val loss: 1.451 Train MAE: 5.543 Val MAE: 12.629 Epoch time: 93.684 seconds \n",
      "Epoch: 291 Train loss: 3.338 Aux train loss: 2.091 Val loss: 2.426 Aux val loss: 1.468 Train MAE: 5.467 Val MAE: 15.490 Epoch time: 90.313 seconds \n",
      "Epoch: 292 Train loss: 3.748 Aux train loss: 2.354 Val loss: 2.393 Aux val loss: 1.465 Train MAE: 5.867 Val MAE: 13.166 Epoch time: 91.912 seconds \n",
      "Epoch: 293 Train loss: 3.224 Aux train loss: 2.026 Val loss: 2.376 Aux val loss: 1.463 Train MAE: 5.184 Val MAE: 12.507 Epoch time: 89.734 seconds \n",
      "Epoch: 294 Train loss: 3.859 Aux train loss: 2.418 Val loss: 2.388 Aux val loss: 1.461 Train MAE: 5.527 Val MAE: 13.375 Epoch time: 90.759 seconds \n",
      "Epoch: 295 Train loss: 3.066 Aux train loss: 1.933 Val loss: 2.392 Aux val loss: 1.466 Train MAE: 5.119 Val MAE: 12.318 Epoch time: 88.623 seconds \n",
      "Epoch: 296 Train loss: 3.887 Aux train loss: 2.439 Val loss: 2.428 Aux val loss: 1.482 Train MAE: 5.953 Val MAE: 14.607 Epoch time: 88.446 seconds \n",
      "Epoch: 297 Train loss: 3.347 Aux train loss: 2.105 Val loss: 2.462 Aux val loss: 1.484 Train MAE: 5.479 Val MAE: 13.478 Epoch time: 89.045 seconds \n",
      "Epoch: 298 Train loss: 3.392 Aux train loss: 2.120 Val loss: 2.414 Aux val loss: 1.469 Train MAE: 5.596 Val MAE: 13.488 Epoch time: 92.093 seconds \n",
      "Epoch: 299 Train loss: 3.390 Aux train loss: 2.134 Val loss: 2.410 Aux val loss: 1.468 Train MAE: 5.546 Val MAE: 12.648 Epoch time: 92.120 seconds \n",
      "Epoch: 300 Train loss: 3.229 Aux train loss: 2.029 Val loss: 2.429 Aux val loss: 1.474 Train MAE: 5.057 Val MAE: 13.421 Epoch time: 90.323 seconds \n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=4,5,6,7 torchrun --nproc_per_node=4 module12/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient20_freeze_gd --epochs=300 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feaedad0-7e9d-47b4-86e7-b05a326f4100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Val set MAE: 11.63 RMSE: 36.09\n",
      "GT count: tensor([30.,  8.], device='cuda:0')\n",
      "Predicted count: tensor([24.1850,  6.1180], device='cuda:0')\n",
      "Test set MAE: 13.85 RMSE: 83.75\n",
      "GT count: tensor([17., 34.], device='cuda:0')\n",
      "Predicted count: tensor([22.3869, 30.3905], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=7 torchrun --nproc_per_node=1 module12/evaluate.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient20_freeze_gd --epochs=300 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40194ba-2e0c-40ad-9f35-59481184bd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3decf-377a-4b45-9463-f91e58ea6d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad5d4b4-4e08-48e1-b27d-cf800a4253d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0323 11:08:56.349000 2462288 site-packages/torch/distributed/run.py:793] \n",
      "W0323 11:08:56.349000 2462288 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0323 11:08:56.349000 2462288 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0323 11:08:56.349000 2462288 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "Epoch: 1 Train loss: 9.067 Aux train loss: 5.781 Val loss: 3.850 Aux val loss: 2.462 Train MAE: 60.702 Val MAE: 45.501 Epoch time: 87.910 seconds best\n",
      "Epoch: 2 Train loss: 8.410 Aux train loss: 5.088 Val loss: 3.738 Aux val loss: 2.251 Train MAE: 38.590 Val MAE: 38.012 Epoch time: 85.292 seconds best\n",
      "Epoch: 3 Train loss: 9.056 Aux train loss: 5.463 Val loss: 3.672 Aux val loss: 2.203 Train MAE: 40.856 Val MAE: 31.899 Epoch time: 94.506 seconds best\n",
      "Epoch: 4 Train loss: 8.016 Aux train loss: 4.809 Val loss: 3.633 Aux val loss: 2.189 Train MAE: 34.982 Val MAE: 25.494 Epoch time: 89.594 seconds best\n",
      "Epoch: 5 Train loss: 7.719 Aux train loss: 4.618 Val loss: 3.650 Aux val loss: 2.219 Train MAE: 36.738 Val MAE: 47.181 Epoch time: 92.760 seconds \n",
      "Epoch: 6 Train loss: 7.844 Aux train loss: 4.711 Val loss: 3.474 Aux val loss: 2.104 Train MAE: 31.657 Val MAE: 27.856 Epoch time: 89.580 seconds \n",
      "Epoch: 7 Train loss: 7.344 Aux train loss: 4.450 Val loss: 3.510 Aux val loss: 2.122 Train MAE: 30.483 Val MAE: 37.915 Epoch time: 90.391 seconds \n",
      "Epoch: 8 Train loss: 7.046 Aux train loss: 4.266 Val loss: 3.302 Aux val loss: 1.996 Train MAE: 29.966 Val MAE: 24.354 Epoch time: 87.054 seconds best\n",
      "Epoch: 9 Train loss: 6.936 Aux train loss: 4.178 Val loss: 3.402 Aux val loss: 2.008 Train MAE: 29.314 Val MAE: 30.970 Epoch time: 94.108 seconds \n",
      "Epoch: 10 Train loss: 6.788 Aux train loss: 4.086 Val loss: 3.312 Aux val loss: 1.979 Train MAE: 27.646 Val MAE: 36.564 Epoch time: 89.081 seconds \n",
      "Epoch: 11 Train loss: 6.368 Aux train loss: 3.845 Val loss: 3.215 Aux val loss: 1.926 Train MAE: 27.586 Val MAE: 19.061 Epoch time: 90.490 seconds best\n",
      "Epoch: 12 Train loss: 6.234 Aux train loss: 3.772 Val loss: 3.189 Aux val loss: 1.934 Train MAE: 25.794 Val MAE: 31.149 Epoch time: 88.835 seconds \n",
      "Epoch: 13 Train loss: 6.258 Aux train loss: 3.804 Val loss: 3.128 Aux val loss: 1.894 Train MAE: 23.877 Val MAE: 21.795 Epoch time: 87.397 seconds \n",
      "Epoch: 14 Train loss: 6.508 Aux train loss: 3.929 Val loss: 3.154 Aux val loss: 1.919 Train MAE: 28.760 Val MAE: 26.177 Epoch time: 89.839 seconds \n",
      "Epoch: 15 Train loss: 6.050 Aux train loss: 3.665 Val loss: 3.186 Aux val loss: 1.895 Train MAE: 24.217 Val MAE: 39.917 Epoch time: 88.872 seconds \n",
      "Epoch: 16 Train loss: 6.296 Aux train loss: 3.817 Val loss: 3.200 Aux val loss: 1.934 Train MAE: 25.794 Val MAE: 25.174 Epoch time: 89.781 seconds \n",
      "Epoch: 17 Train loss: 5.811 Aux train loss: 3.535 Val loss: 3.071 Aux val loss: 1.857 Train MAE: 22.645 Val MAE: 25.906 Epoch time: 90.100 seconds \n",
      "Epoch: 18 Train loss: 5.707 Aux train loss: 3.475 Val loss: 3.059 Aux val loss: 1.840 Train MAE: 22.889 Val MAE: 21.153 Epoch time: 86.863 seconds \n",
      "Epoch: 19 Train loss: 5.970 Aux train loss: 3.614 Val loss: 3.052 Aux val loss: 1.821 Train MAE: 24.620 Val MAE: 33.115 Epoch time: 91.823 seconds \n",
      "Epoch: 20 Train loss: 6.132 Aux train loss: 3.716 Val loss: 3.053 Aux val loss: 1.834 Train MAE: 23.357 Val MAE: 33.051 Epoch time: 88.232 seconds \n",
      "Epoch: 21 Train loss: 5.879 Aux train loss: 3.569 Val loss: 2.949 Aux val loss: 1.788 Train MAE: 20.960 Val MAE: 26.790 Epoch time: 87.545 seconds \n",
      "Epoch: 22 Train loss: 5.454 Aux train loss: 3.309 Val loss: 2.961 Aux val loss: 1.803 Train MAE: 21.680 Val MAE: 22.934 Epoch time: 90.328 seconds \n",
      "Epoch: 23 Train loss: 5.164 Aux train loss: 3.133 Val loss: 2.973 Aux val loss: 1.803 Train MAE: 21.669 Val MAE: 25.802 Epoch time: 90.666 seconds \n",
      "Epoch: 24 Train loss: 5.403 Aux train loss: 3.287 Val loss: 2.921 Aux val loss: 1.776 Train MAE: 20.088 Val MAE: 21.922 Epoch time: 89.074 seconds \n",
      "Epoch: 25 Train loss: 5.693 Aux train loss: 3.467 Val loss: 2.954 Aux val loss: 1.791 Train MAE: 22.439 Val MAE: 21.188 Epoch time: 86.886 seconds \n",
      "Epoch: 26 Train loss: 5.708 Aux train loss: 3.491 Val loss: 2.832 Aux val loss: 1.739 Train MAE: 22.453 Val MAE: 18.819 Epoch time: 89.361 seconds best\n",
      "Epoch: 27 Train loss: 5.788 Aux train loss: 3.513 Val loss: 2.914 Aux val loss: 1.762 Train MAE: 19.780 Val MAE: 21.373 Epoch time: 90.803 seconds \n",
      "Epoch: 28 Train loss: 5.337 Aux train loss: 3.240 Val loss: 2.913 Aux val loss: 1.774 Train MAE: 20.168 Val MAE: 29.698 Epoch time: 89.199 seconds \n",
      "Epoch: 29 Train loss: 5.669 Aux train loss: 3.435 Val loss: 2.882 Aux val loss: 1.736 Train MAE: 24.144 Val MAE: 29.564 Epoch time: 91.894 seconds \n",
      "Epoch: 30 Train loss: 5.695 Aux train loss: 3.480 Val loss: 2.873 Aux val loss: 1.741 Train MAE: 20.287 Val MAE: 23.569 Epoch time: 91.731 seconds \n",
      "Epoch: 31 Train loss: 5.109 Aux train loss: 3.115 Val loss: 2.906 Aux val loss: 1.771 Train MAE: 18.352 Val MAE: 22.975 Epoch time: 90.707 seconds \n",
      "Epoch: 32 Train loss: 4.883 Aux train loss: 2.950 Val loss: 2.936 Aux val loss: 1.786 Train MAE: 18.749 Val MAE: 30.532 Epoch time: 91.329 seconds \n",
      "Epoch: 33 Train loss: 4.864 Aux train loss: 2.961 Val loss: 2.900 Aux val loss: 1.763 Train MAE: 17.937 Val MAE: 25.186 Epoch time: 88.066 seconds \n",
      "Epoch: 34 Train loss: 5.408 Aux train loss: 3.267 Val loss: 2.842 Aux val loss: 1.721 Train MAE: 19.795 Val MAE: 34.833 Epoch time: 89.830 seconds \n",
      "Epoch: 35 Train loss: 5.975 Aux train loss: 3.605 Val loss: 2.854 Aux val loss: 1.742 Train MAE: 22.129 Val MAE: 27.483 Epoch time: 92.582 seconds \n",
      "Epoch: 36 Train loss: 5.745 Aux train loss: 3.498 Val loss: 2.857 Aux val loss: 1.728 Train MAE: 22.082 Val MAE: 19.065 Epoch time: 88.723 seconds \n",
      "Epoch: 37 Train loss: 5.766 Aux train loss: 3.501 Val loss: 2.894 Aux val loss: 1.777 Train MAE: 20.539 Val MAE: 20.777 Epoch time: 90.884 seconds \n",
      "Epoch: 38 Train loss: 5.710 Aux train loss: 3.451 Val loss: 2.789 Aux val loss: 1.674 Train MAE: 22.907 Val MAE: 18.116 Epoch time: 92.752 seconds best\n",
      "Epoch: 39 Train loss: 5.215 Aux train loss: 3.156 Val loss: 2.791 Aux val loss: 1.691 Train MAE: 16.986 Val MAE: 20.074 Epoch time: 88.941 seconds \n",
      "Epoch: 40 Train loss: 5.383 Aux train loss: 3.249 Val loss: 2.810 Aux val loss: 1.670 Train MAE: 18.868 Val MAE: 31.144 Epoch time: 89.423 seconds \n",
      "Epoch: 41 Train loss: 5.195 Aux train loss: 3.140 Val loss: 2.845 Aux val loss: 1.717 Train MAE: 17.996 Val MAE: 24.469 Epoch time: 89.725 seconds \n",
      "Epoch: 42 Train loss: 5.548 Aux train loss: 3.362 Val loss: 2.802 Aux val loss: 1.687 Train MAE: 18.364 Val MAE: 19.995 Epoch time: 90.125 seconds \n",
      "Epoch: 43 Train loss: 5.045 Aux train loss: 3.062 Val loss: 2.856 Aux val loss: 1.731 Train MAE: 17.250 Val MAE: 23.523 Epoch time: 91.867 seconds \n",
      "Epoch: 44 Train loss: 5.137 Aux train loss: 3.119 Val loss: 2.828 Aux val loss: 1.707 Train MAE: 16.659 Val MAE: 21.958 Epoch time: 87.854 seconds \n",
      "Epoch: 45 Train loss: 4.614 Aux train loss: 2.796 Val loss: 2.788 Aux val loss: 1.694 Train MAE: 17.463 Val MAE: 21.092 Epoch time: 86.975 seconds \n",
      "Epoch: 46 Train loss: 4.899 Aux train loss: 2.950 Val loss: 2.756 Aux val loss: 1.665 Train MAE: 16.671 Val MAE: 28.248 Epoch time: 91.350 seconds \n",
      "Epoch: 47 Train loss: 4.454 Aux train loss: 2.686 Val loss: 2.778 Aux val loss: 1.673 Train MAE: 16.829 Val MAE: 25.568 Epoch time: 89.767 seconds \n",
      "Epoch: 48 Train loss: 5.373 Aux train loss: 3.251 Val loss: 2.805 Aux val loss: 1.687 Train MAE: 18.842 Val MAE: 29.513 Epoch time: 89.120 seconds \n",
      "Epoch: 49 Train loss: 4.894 Aux train loss: 2.953 Val loss: 2.762 Aux val loss: 1.661 Train MAE: 16.596 Val MAE: 17.697 Epoch time: 87.556 seconds best\n",
      "Epoch: 50 Train loss: 5.153 Aux train loss: 3.124 Val loss: 2.783 Aux val loss: 1.676 Train MAE: 18.926 Val MAE: 18.369 Epoch time: 86.475 seconds \n",
      "Epoch: 51 Train loss: 5.006 Aux train loss: 3.044 Val loss: 2.754 Aux val loss: 1.683 Train MAE: 17.938 Val MAE: 20.767 Epoch time: 87.484 seconds \n",
      "Epoch: 52 Train loss: 5.395 Aux train loss: 3.299 Val loss: 2.732 Aux val loss: 1.647 Train MAE: 17.682 Val MAE: 23.285 Epoch time: 91.166 seconds \n",
      "Epoch: 53 Train loss: 4.665 Aux train loss: 2.835 Val loss: 2.901 Aux val loss: 1.739 Train MAE: 16.528 Val MAE: 23.332 Epoch time: 85.412 seconds \n",
      "Epoch: 54 Train loss: 4.995 Aux train loss: 3.033 Val loss: 2.777 Aux val loss: 1.675 Train MAE: 15.987 Val MAE: 24.481 Epoch time: 90.741 seconds \n",
      "Epoch: 55 Train loss: 5.086 Aux train loss: 3.078 Val loss: 2.803 Aux val loss: 1.675 Train MAE: 17.412 Val MAE: 23.909 Epoch time: 89.450 seconds \n",
      "Epoch: 56 Train loss: 5.169 Aux train loss: 3.137 Val loss: 2.753 Aux val loss: 1.668 Train MAE: 15.677 Val MAE: 30.613 Epoch time: 89.860 seconds \n",
      "Epoch: 57 Train loss: 4.838 Aux train loss: 2.937 Val loss: 2.705 Aux val loss: 1.638 Train MAE: 15.566 Val MAE: 20.798 Epoch time: 89.566 seconds \n",
      "Epoch: 58 Train loss: 5.105 Aux train loss: 3.096 Val loss: 2.742 Aux val loss: 1.673 Train MAE: 16.441 Val MAE: 23.998 Epoch time: 88.517 seconds \n",
      "Epoch: 59 Train loss: 5.097 Aux train loss: 3.061 Val loss: 2.729 Aux val loss: 1.641 Train MAE: 20.923 Val MAE: 22.247 Epoch time: 90.517 seconds \n",
      "Epoch: 60 Train loss: 4.855 Aux train loss: 2.937 Val loss: 2.789 Aux val loss: 1.670 Train MAE: 17.683 Val MAE: 28.180 Epoch time: 90.023 seconds \n",
      "Epoch: 61 Train loss: 4.909 Aux train loss: 2.975 Val loss: 2.732 Aux val loss: 1.633 Train MAE: 17.281 Val MAE: 21.807 Epoch time: 88.040 seconds \n",
      "Epoch: 62 Train loss: 4.335 Aux train loss: 2.625 Val loss: 2.661 Aux val loss: 1.615 Train MAE: 14.558 Val MAE: 18.150 Epoch time: 90.961 seconds \n",
      "Epoch: 63 Train loss: 4.575 Aux train loss: 2.763 Val loss: 2.734 Aux val loss: 1.640 Train MAE: 15.311 Val MAE: 18.425 Epoch time: 91.988 seconds \n",
      "Epoch: 64 Train loss: 4.816 Aux train loss: 2.907 Val loss: 2.706 Aux val loss: 1.659 Train MAE: 15.294 Val MAE: 18.147 Epoch time: 88.051 seconds \n",
      "Epoch: 65 Train loss: 4.957 Aux train loss: 2.999 Val loss: 2.747 Aux val loss: 1.666 Train MAE: 15.475 Val MAE: 18.897 Epoch time: 90.970 seconds \n",
      "Epoch: 66 Train loss: 4.940 Aux train loss: 2.980 Val loss: 2.654 Aux val loss: 1.617 Train MAE: 16.988 Val MAE: 15.500 Epoch time: 91.532 seconds best\n",
      "Epoch: 67 Train loss: 4.549 Aux train loss: 2.744 Val loss: 2.745 Aux val loss: 1.675 Train MAE: 16.105 Val MAE: 18.103 Epoch time: 91.753 seconds \n",
      "Epoch: 68 Train loss: 4.939 Aux train loss: 2.999 Val loss: 2.650 Aux val loss: 1.604 Train MAE: 15.894 Val MAE: 22.438 Epoch time: 87.707 seconds \n",
      "Epoch: 69 Train loss: 5.118 Aux train loss: 3.098 Val loss: 2.682 Aux val loss: 1.643 Train MAE: 16.277 Val MAE: 18.740 Epoch time: 89.505 seconds \n",
      "Epoch: 70 Train loss: 4.465 Aux train loss: 2.711 Val loss: 2.673 Aux val loss: 1.627 Train MAE: 15.997 Val MAE: 17.431 Epoch time: 91.864 seconds \n",
      "Epoch: 71 Train loss: 5.029 Aux train loss: 3.038 Val loss: 2.660 Aux val loss: 1.607 Train MAE: 15.011 Val MAE: 16.073 Epoch time: 94.631 seconds \n",
      "Epoch: 72 Train loss: 4.992 Aux train loss: 3.025 Val loss: 2.681 Aux val loss: 1.615 Train MAE: 14.675 Val MAE: 16.763 Epoch time: 95.253 seconds \n",
      "Epoch: 73 Train loss: 4.777 Aux train loss: 2.888 Val loss: 2.776 Aux val loss: 1.682 Train MAE: 15.401 Val MAE: 24.632 Epoch time: 92.709 seconds \n",
      "Epoch: 74 Train loss: 4.582 Aux train loss: 2.767 Val loss: 2.714 Aux val loss: 1.625 Train MAE: 15.027 Val MAE: 18.437 Epoch time: 94.418 seconds \n",
      "Epoch: 75 Train loss: 4.938 Aux train loss: 2.991 Val loss: 2.620 Aux val loss: 1.580 Train MAE: 15.167 Val MAE: 16.690 Epoch time: 91.247 seconds \n",
      "Epoch: 76 Train loss: 4.900 Aux train loss: 2.982 Val loss: 2.771 Aux val loss: 1.652 Train MAE: 16.107 Val MAE: 18.797 Epoch time: 94.021 seconds \n",
      "Epoch: 77 Train loss: 4.734 Aux train loss: 2.876 Val loss: 2.648 Aux val loss: 1.592 Train MAE: 13.751 Val MAE: 17.991 Epoch time: 91.654 seconds \n",
      "Epoch: 78 Train loss: 4.493 Aux train loss: 2.722 Val loss: 2.664 Aux val loss: 1.606 Train MAE: 14.315 Val MAE: 22.429 Epoch time: 93.426 seconds \n",
      "Epoch: 79 Train loss: 4.797 Aux train loss: 2.902 Val loss: 2.671 Aux val loss: 1.626 Train MAE: 16.021 Val MAE: 16.846 Epoch time: 91.842 seconds \n",
      "Epoch: 80 Train loss: 4.950 Aux train loss: 2.969 Val loss: 2.717 Aux val loss: 1.629 Train MAE: 18.555 Val MAE: 20.023 Epoch time: 93.017 seconds \n",
      "Epoch: 81 Train loss: 4.577 Aux train loss: 2.756 Val loss: 2.712 Aux val loss: 1.636 Train MAE: 14.564 Val MAE: 17.963 Epoch time: 94.438 seconds \n",
      "Epoch: 82 Train loss: 4.419 Aux train loss: 2.678 Val loss: 2.784 Aux val loss: 1.679 Train MAE: 14.815 Val MAE: 21.394 Epoch time: 91.621 seconds \n",
      "Epoch: 83 Train loss: 4.892 Aux train loss: 2.961 Val loss: 2.694 Aux val loss: 1.607 Train MAE: 14.553 Val MAE: 17.587 Epoch time: 92.777 seconds \n",
      "Epoch: 84 Train loss: 4.818 Aux train loss: 2.914 Val loss: 2.715 Aux val loss: 1.613 Train MAE: 14.889 Val MAE: 29.067 Epoch time: 92.649 seconds \n",
      "Epoch: 85 Train loss: 4.683 Aux train loss: 2.835 Val loss: 2.723 Aux val loss: 1.636 Train MAE: 14.460 Val MAE: 26.051 Epoch time: 92.191 seconds \n",
      "Epoch: 86 Train loss: 4.695 Aux train loss: 2.859 Val loss: 2.780 Aux val loss: 1.659 Train MAE: 16.227 Val MAE: 23.838 Epoch time: 90.611 seconds \n",
      "Epoch: 87 Train loss: 4.736 Aux train loss: 2.866 Val loss: 2.710 Aux val loss: 1.622 Train MAE: 14.174 Val MAE: 17.234 Epoch time: 96.584 seconds \n",
      "Epoch: 88 Train loss: 4.650 Aux train loss: 2.822 Val loss: 2.637 Aux val loss: 1.579 Train MAE: 13.078 Val MAE: 16.632 Epoch time: 92.638 seconds \n",
      "Epoch: 89 Train loss: 4.921 Aux train loss: 2.982 Val loss: 2.642 Aux val loss: 1.598 Train MAE: 14.486 Val MAE: 26.304 Epoch time: 90.288 seconds \n",
      "Epoch: 90 Train loss: 4.592 Aux train loss: 2.782 Val loss: 2.690 Aux val loss: 1.629 Train MAE: 14.644 Val MAE: 19.696 Epoch time: 92.584 seconds \n",
      "Epoch: 91 Train loss: 4.455 Aux train loss: 2.692 Val loss: 2.637 Aux val loss: 1.570 Train MAE: 13.837 Val MAE: 19.242 Epoch time: 96.704 seconds \n",
      "Epoch: 92 Train loss: 4.401 Aux train loss: 2.653 Val loss: 2.644 Aux val loss: 1.588 Train MAE: 13.712 Val MAE: 22.315 Epoch time: 93.133 seconds \n",
      "Epoch: 93 Train loss: 4.602 Aux train loss: 2.797 Val loss: 2.732 Aux val loss: 1.621 Train MAE: 12.917 Val MAE: 18.813 Epoch time: 108.602 seconds \n",
      "Epoch: 94 Train loss: 4.588 Aux train loss: 2.774 Val loss: 2.685 Aux val loss: 1.614 Train MAE: 14.005 Val MAE: 29.079 Epoch time: 94.923 seconds \n",
      "Epoch: 95 Train loss: 4.408 Aux train loss: 2.669 Val loss: 2.663 Aux val loss: 1.592 Train MAE: 14.718 Val MAE: 21.221 Epoch time: 95.533 seconds \n",
      "Epoch: 96 Train loss: 4.409 Aux train loss: 2.658 Val loss: 2.667 Aux val loss: 1.603 Train MAE: 15.106 Val MAE: 16.691 Epoch time: 93.082 seconds \n",
      "Epoch: 97 Train loss: 4.717 Aux train loss: 2.850 Val loss: 2.678 Aux val loss: 1.595 Train MAE: 14.299 Val MAE: 18.694 Epoch time: 92.332 seconds \n",
      "Epoch: 98 Train loss: 4.274 Aux train loss: 2.588 Val loss: 2.600 Aux val loss: 1.553 Train MAE: 12.679 Val MAE: 15.774 Epoch time: 95.294 seconds \n",
      "Epoch: 99 Train loss: 4.887 Aux train loss: 2.960 Val loss: 2.631 Aux val loss: 1.588 Train MAE: 14.185 Val MAE: 15.887 Epoch time: 96.307 seconds \n",
      "Epoch: 100 Train loss: 4.530 Aux train loss: 2.754 Val loss: 2.726 Aux val loss: 1.639 Train MAE: 14.415 Val MAE: 21.366 Epoch time: 94.305 seconds \n",
      "Epoch: 101 Train loss: 4.566 Aux train loss: 2.760 Val loss: 2.603 Aux val loss: 1.558 Train MAE: 14.627 Val MAE: 18.455 Epoch time: 93.717 seconds \n",
      "Epoch: 102 Train loss: 4.850 Aux train loss: 2.928 Val loss: 2.597 Aux val loss: 1.544 Train MAE: 14.546 Val MAE: 16.986 Epoch time: 96.434 seconds \n",
      "Epoch: 103 Train loss: 4.750 Aux train loss: 2.876 Val loss: 2.756 Aux val loss: 1.661 Train MAE: 15.562 Val MAE: 19.863 Epoch time: 96.681 seconds \n",
      "Epoch: 104 Train loss: 4.552 Aux train loss: 2.762 Val loss: 2.613 Aux val loss: 1.564 Train MAE: 14.009 Val MAE: 20.321 Epoch time: 95.091 seconds \n",
      "Epoch: 105 Train loss: 4.223 Aux train loss: 2.555 Val loss: 2.661 Aux val loss: 1.605 Train MAE: 13.656 Val MAE: 20.282 Epoch time: 92.517 seconds \n",
      "Epoch: 106 Train loss: 4.190 Aux train loss: 2.533 Val loss: 2.643 Aux val loss: 1.576 Train MAE: 12.420 Val MAE: 20.744 Epoch time: 94.087 seconds \n",
      "Epoch: 107 Train loss: 4.176 Aux train loss: 2.531 Val loss: 2.559 Aux val loss: 1.539 Train MAE: 14.081 Val MAE: 15.413 Epoch time: 93.454 seconds best\n",
      "Epoch: 108 Train loss: 4.656 Aux train loss: 2.816 Val loss: 2.710 Aux val loss: 1.641 Train MAE: 14.321 Val MAE: 21.068 Epoch time: 95.688 seconds \n",
      "Epoch: 109 Train loss: 4.705 Aux train loss: 2.852 Val loss: 2.627 Aux val loss: 1.587 Train MAE: 14.167 Val MAE: 18.426 Epoch time: 97.191 seconds \n",
      "Epoch: 110 Train loss: 4.539 Aux train loss: 2.734 Val loss: 2.576 Aux val loss: 1.552 Train MAE: 14.452 Val MAE: 21.945 Epoch time: 95.057 seconds \n",
      "Epoch: 111 Train loss: 4.538 Aux train loss: 2.741 Val loss: 2.541 Aux val loss: 1.536 Train MAE: 15.060 Val MAE: 13.793 Epoch time: 97.335 seconds best\n",
      "Epoch: 112 Train loss: 4.662 Aux train loss: 2.822 Val loss: 2.582 Aux val loss: 1.582 Train MAE: 14.607 Val MAE: 20.847 Epoch time: 94.188 seconds \n",
      "Epoch: 113 Train loss: 4.548 Aux train loss: 2.740 Val loss: 2.611 Aux val loss: 1.575 Train MAE: 13.266 Val MAE: 17.751 Epoch time: 92.014 seconds \n",
      "Epoch: 114 Train loss: 4.525 Aux train loss: 2.744 Val loss: 2.660 Aux val loss: 1.598 Train MAE: 13.713 Val MAE: 21.133 Epoch time: 94.164 seconds \n",
      "Epoch: 115 Train loss: 4.383 Aux train loss: 2.653 Val loss: 2.576 Aux val loss: 1.557 Train MAE: 12.453 Val MAE: 18.624 Epoch time: 95.080 seconds \n",
      "Epoch: 116 Train loss: 4.143 Aux train loss: 2.514 Val loss: 2.648 Aux val loss: 1.596 Train MAE: 12.734 Val MAE: 25.326 Epoch time: 95.668 seconds \n",
      "Epoch: 117 Train loss: 4.447 Aux train loss: 2.685 Val loss: 2.545 Aux val loss: 1.547 Train MAE: 15.983 Val MAE: 15.626 Epoch time: 93.686 seconds \n",
      "Epoch: 118 Train loss: 4.400 Aux train loss: 2.714 Val loss: 2.625 Aux val loss: 1.587 Train MAE: 13.045 Val MAE: 17.503 Epoch time: 94.043 seconds \n",
      "Epoch: 119 Train loss: 4.183 Aux train loss: 2.549 Val loss: 2.621 Aux val loss: 1.586 Train MAE: 12.444 Val MAE: 17.138 Epoch time: 92.118 seconds \n",
      "Epoch: 120 Train loss: 4.229 Aux train loss: 2.563 Val loss: 2.515 Aux val loss: 1.521 Train MAE: 13.317 Val MAE: 16.499 Epoch time: 91.818 seconds \n",
      "Epoch: 121 Train loss: 4.021 Aux train loss: 2.443 Val loss: 2.581 Aux val loss: 1.544 Train MAE: 12.156 Val MAE: 18.767 Epoch time: 93.471 seconds \n",
      "Epoch: 122 Train loss: 4.320 Aux train loss: 2.610 Val loss: 2.488 Aux val loss: 1.496 Train MAE: 12.072 Val MAE: 15.708 Epoch time: 91.426 seconds \n",
      "Epoch: 123 Train loss: 4.782 Aux train loss: 2.887 Val loss: 2.491 Aux val loss: 1.495 Train MAE: 14.655 Val MAE: 14.586 Epoch time: 96.102 seconds \n",
      "Epoch: 124 Train loss: 4.124 Aux train loss: 2.495 Val loss: 2.553 Aux val loss: 1.526 Train MAE: 13.593 Val MAE: 16.757 Epoch time: 94.227 seconds \n",
      "Epoch: 125 Train loss: 4.356 Aux train loss: 2.628 Val loss: 2.540 Aux val loss: 1.530 Train MAE: 13.262 Val MAE: 18.420 Epoch time: 93.784 seconds \n",
      "Epoch: 126 Train loss: 5.017 Aux train loss: 3.042 Val loss: 2.494 Aux val loss: 1.503 Train MAE: 13.746 Val MAE: 16.787 Epoch time: 96.843 seconds \n",
      "Epoch: 127 Train loss: 4.655 Aux train loss: 2.826 Val loss: 2.608 Aux val loss: 1.557 Train MAE: 12.935 Val MAE: 28.270 Epoch time: 94.014 seconds \n",
      "Epoch: 128 Train loss: 4.270 Aux train loss: 2.591 Val loss: 2.633 Aux val loss: 1.576 Train MAE: 13.190 Val MAE: 16.331 Epoch time: 98.949 seconds \n",
      "Epoch: 129 Train loss: 4.134 Aux train loss: 2.498 Val loss: 2.575 Aux val loss: 1.562 Train MAE: 12.964 Val MAE: 15.862 Epoch time: 95.819 seconds \n",
      "Epoch: 130 Train loss: 4.348 Aux train loss: 2.621 Val loss: 2.500 Aux val loss: 1.500 Train MAE: 13.127 Val MAE: 20.786 Epoch time: 90.312 seconds \n",
      "Epoch: 131 Train loss: 4.428 Aux train loss: 2.681 Val loss: 2.620 Aux val loss: 1.567 Train MAE: 13.300 Val MAE: 15.732 Epoch time: 93.598 seconds \n",
      "Epoch: 132 Train loss: 4.160 Aux train loss: 2.515 Val loss: 2.663 Aux val loss: 1.607 Train MAE: 13.892 Val MAE: 18.752 Epoch time: 99.799 seconds \n",
      "Epoch: 133 Train loss: 4.175 Aux train loss: 2.527 Val loss: 2.566 Aux val loss: 1.531 Train MAE: 12.471 Val MAE: 19.313 Epoch time: 115.781 seconds \n",
      "Epoch: 134 Train loss: 4.384 Aux train loss: 2.650 Val loss: 2.584 Aux val loss: 1.543 Train MAE: 12.837 Val MAE: 17.401 Epoch time: 105.379 seconds \n",
      "Epoch: 135 Train loss: 4.354 Aux train loss: 2.635 Val loss: 2.580 Aux val loss: 1.558 Train MAE: 12.656 Val MAE: 18.059 Epoch time: 103.780 seconds \n",
      "Epoch: 136 Train loss: 4.013 Aux train loss: 2.431 Val loss: 2.556 Aux val loss: 1.549 Train MAE: 12.374 Val MAE: 13.802 Epoch time: 115.668 seconds \n",
      "Epoch: 137 Train loss: 4.524 Aux train loss: 2.751 Val loss: 2.545 Aux val loss: 1.534 Train MAE: 12.406 Val MAE: 20.847 Epoch time: 103.238 seconds \n",
      "Epoch: 138 Train loss: 3.902 Aux train loss: 2.361 Val loss: 2.542 Aux val loss: 1.516 Train MAE: 12.653 Val MAE: 15.434 Epoch time: 96.306 seconds \n",
      "Epoch: 139 Train loss: 4.769 Aux train loss: 2.874 Val loss: 2.661 Aux val loss: 1.565 Train MAE: 13.191 Val MAE: 20.543 Epoch time: 111.197 seconds \n",
      "Epoch: 140 Train loss: 4.339 Aux train loss: 2.624 Val loss: 2.500 Aux val loss: 1.503 Train MAE: 12.579 Val MAE: 13.798 Epoch time: 107.422 seconds \n",
      "Epoch: 141 Train loss: 4.249 Aux train loss: 2.563 Val loss: 2.537 Aux val loss: 1.519 Train MAE: 13.670 Val MAE: 23.166 Epoch time: 115.719 seconds \n",
      "Epoch: 142 Train loss: 4.248 Aux train loss: 2.564 Val loss: 2.634 Aux val loss: 1.576 Train MAE: 13.257 Val MAE: 24.259 Epoch time: 98.739 seconds \n",
      "Epoch: 143 Train loss: 4.183 Aux train loss: 2.531 Val loss: 2.558 Aux val loss: 1.531 Train MAE: 12.325 Val MAE: 14.595 Epoch time: 106.961 seconds \n",
      "Epoch: 144 Train loss: 4.066 Aux train loss: 2.454 Val loss: 2.600 Aux val loss: 1.559 Train MAE: 12.004 Val MAE: 22.438 Epoch time: 107.889 seconds \n",
      "Epoch: 145 Train loss: 4.039 Aux train loss: 2.437 Val loss: 2.534 Aux val loss: 1.518 Train MAE: 12.007 Val MAE: 16.860 Epoch time: 104.646 seconds \n",
      "Epoch: 146 Train loss: 4.496 Aux train loss: 2.780 Val loss: 2.594 Aux val loss: 1.652 Train MAE: 12.598 Val MAE: 13.636 Epoch time: 105.489 seconds best\n",
      "Epoch: 147 Train loss: 4.551 Aux train loss: 2.793 Val loss: 2.712 Aux val loss: 1.660 Train MAE: 14.180 Val MAE: 24.849 Epoch time: 105.933 seconds \n",
      "Epoch: 148 Train loss: 4.538 Aux train loss: 2.758 Val loss: 2.545 Aux val loss: 1.520 Train MAE: 13.608 Val MAE: 19.437 Epoch time: 99.006 seconds \n",
      "Epoch: 149 Train loss: 4.234 Aux train loss: 2.560 Val loss: 2.484 Aux val loss: 1.504 Train MAE: 13.726 Val MAE: 15.400 Epoch time: 113.558 seconds \n",
      "Epoch: 150 Train loss: 4.792 Aux train loss: 2.907 Val loss: 2.546 Aux val loss: 1.544 Train MAE: 12.655 Val MAE: 16.628 Epoch time: 119.874 seconds \n",
      "Epoch: 151 Train loss: 4.415 Aux train loss: 2.676 Val loss: 2.484 Aux val loss: 1.495 Train MAE: 11.311 Val MAE: 13.772 Epoch time: 113.921 seconds \n",
      "Epoch: 152 Train loss: 3.666 Aux train loss: 2.216 Val loss: 2.473 Aux val loss: 1.489 Train MAE: 11.215 Val MAE: 16.538 Epoch time: 96.570 seconds \n",
      "Epoch: 153 Train loss: 3.855 Aux train loss: 2.335 Val loss: 2.497 Aux val loss: 1.522 Train MAE: 10.803 Val MAE: 15.071 Epoch time: 108.066 seconds \n",
      "Epoch: 154 Train loss: 4.057 Aux train loss: 2.458 Val loss: 2.587 Aux val loss: 1.558 Train MAE: 10.889 Val MAE: 20.289 Epoch time: 110.215 seconds \n",
      "Epoch: 155 Train loss: 3.936 Aux train loss: 2.385 Val loss: 2.618 Aux val loss: 1.564 Train MAE: 11.009 Val MAE: 21.287 Epoch time: 115.523 seconds \n",
      "Epoch: 156 Train loss: 4.068 Aux train loss: 2.466 Val loss: 2.594 Aux val loss: 1.559 Train MAE: 10.411 Val MAE: 17.043 Epoch time: 105.416 seconds \n",
      "Epoch: 157 Train loss: 4.332 Aux train loss: 2.610 Val loss: 2.569 Aux val loss: 1.551 Train MAE: 12.864 Val MAE: 19.703 Epoch time: 112.945 seconds \n",
      "Epoch: 158 Train loss: 4.210 Aux train loss: 2.556 Val loss: 2.507 Aux val loss: 1.501 Train MAE: 12.156 Val MAE: 20.094 Epoch time: 107.611 seconds \n",
      "Epoch: 159 Train loss: 3.719 Aux train loss: 2.258 Val loss: 2.541 Aux val loss: 1.521 Train MAE: 11.163 Val MAE: 15.261 Epoch time: 111.211 seconds \n",
      "Epoch: 160 Train loss: 4.213 Aux train loss: 2.555 Val loss: 2.537 Aux val loss: 1.525 Train MAE: 13.502 Val MAE: 16.500 Epoch time: 97.439 seconds \n",
      "Epoch: 161 Train loss: 4.213 Aux train loss: 2.547 Val loss: 2.524 Aux val loss: 1.513 Train MAE: 11.899 Val MAE: 18.087 Epoch time: 115.341 seconds \n",
      "Epoch: 162 Train loss: 4.636 Aux train loss: 2.797 Val loss: 2.555 Aux val loss: 1.528 Train MAE: 11.709 Val MAE: 24.174 Epoch time: 113.695 seconds \n",
      "Epoch: 163 Train loss: 4.191 Aux train loss: 2.524 Val loss: 2.759 Aux val loss: 1.631 Train MAE: 13.912 Val MAE: 21.863 Epoch time: 117.219 seconds \n",
      "Epoch: 164 Train loss: 4.154 Aux train loss: 2.515 Val loss: 2.645 Aux val loss: 1.577 Train MAE: 11.454 Val MAE: 19.034 Epoch time: 110.427 seconds \n",
      "Epoch: 165 Train loss: 4.012 Aux train loss: 2.428 Val loss: 2.572 Aux val loss: 1.538 Train MAE: 11.626 Val MAE: 21.421 Epoch time: 112.432 seconds \n",
      "Epoch: 166 Train loss: 4.230 Aux train loss: 2.564 Val loss: 2.572 Aux val loss: 1.539 Train MAE: 11.629 Val MAE: 18.834 Epoch time: 111.259 seconds \n",
      "Epoch: 167 Train loss: 4.288 Aux train loss: 2.607 Val loss: 2.569 Aux val loss: 1.548 Train MAE: 14.870 Val MAE: 14.844 Epoch time: 109.232 seconds \n",
      "Epoch: 168 Train loss: 4.034 Aux train loss: 2.451 Val loss: 2.680 Aux val loss: 1.615 Train MAE: 10.912 Val MAE: 18.338 Epoch time: 101.981 seconds \n",
      "Epoch: 169 Train loss: 3.737 Aux train loss: 2.271 Val loss: 2.571 Aux val loss: 1.543 Train MAE: 10.546 Val MAE: 17.604 Epoch time: 98.977 seconds \n",
      "Epoch: 170 Train loss: 4.352 Aux train loss: 2.624 Val loss: 2.550 Aux val loss: 1.536 Train MAE: 12.972 Val MAE: 14.903 Epoch time: 108.654 seconds \n",
      "Epoch: 171 Train loss: 4.663 Aux train loss: 2.831 Val loss: 2.598 Aux val loss: 1.541 Train MAE: 11.488 Val MAE: 22.004 Epoch time: 106.851 seconds \n",
      "Epoch: 172 Train loss: 4.408 Aux train loss: 2.676 Val loss: 2.670 Aux val loss: 1.542 Train MAE: 11.786 Val MAE: 22.216 Epoch time: 102.487 seconds \n",
      "Epoch: 173 Train loss: 3.720 Aux train loss: 2.256 Val loss: 2.576 Aux val loss: 1.550 Train MAE: 11.753 Val MAE: 20.506 Epoch time: 106.840 seconds \n",
      "Epoch: 174 Train loss: 4.145 Aux train loss: 2.519 Val loss: 2.589 Aux val loss: 1.575 Train MAE: 10.756 Val MAE: 17.446 Epoch time: 106.620 seconds \n",
      "Epoch: 175 Train loss: 4.355 Aux train loss: 2.641 Val loss: 2.483 Aux val loss: 1.491 Train MAE: 12.208 Val MAE: 16.945 Epoch time: 109.568 seconds \n",
      "Epoch: 176 Train loss: 4.149 Aux train loss: 2.502 Val loss: 2.607 Aux val loss: 1.559 Train MAE: 12.715 Val MAE: 22.330 Epoch time: 102.631 seconds \n",
      "Epoch: 177 Train loss: 4.058 Aux train loss: 2.451 Val loss: 2.636 Aux val loss: 1.576 Train MAE: 11.095 Val MAE: 25.347 Epoch time: 108.989 seconds \n",
      "Epoch: 178 Train loss: 4.126 Aux train loss: 2.497 Val loss: 2.495 Aux val loss: 1.513 Train MAE: 13.745 Val MAE: 19.585 Epoch time: 102.416 seconds \n",
      "Epoch: 179 Train loss: 4.191 Aux train loss: 2.547 Val loss: 2.517 Aux val loss: 1.520 Train MAE: 12.894 Val MAE: 16.423 Epoch time: 103.280 seconds \n",
      "Epoch: 180 Train loss: 4.123 Aux train loss: 2.491 Val loss: 2.593 Aux val loss: 1.565 Train MAE: 13.031 Val MAE: 20.847 Epoch time: 99.288 seconds \n",
      "Epoch: 181 Train loss: 4.051 Aux train loss: 2.456 Val loss: 2.502 Aux val loss: 1.509 Train MAE: 11.748 Val MAE: 15.705 Epoch time: 101.307 seconds \n",
      "Epoch: 182 Train loss: 4.112 Aux train loss: 2.497 Val loss: 2.525 Aux val loss: 1.523 Train MAE: 12.057 Val MAE: 24.281 Epoch time: 96.997 seconds \n",
      "Epoch: 183 Train loss: 4.317 Aux train loss: 2.633 Val loss: 2.566 Aux val loss: 1.534 Train MAE: 11.745 Val MAE: 26.735 Epoch time: 101.421 seconds \n",
      "Epoch: 184 Train loss: 4.301 Aux train loss: 2.606 Val loss: 2.556 Aux val loss: 1.536 Train MAE: 12.125 Val MAE: 17.150 Epoch time: 97.410 seconds \n",
      "Epoch: 185 Train loss: 3.959 Aux train loss: 2.395 Val loss: 2.506 Aux val loss: 1.518 Train MAE: 12.374 Val MAE: 15.304 Epoch time: 102.297 seconds \n",
      "Epoch: 186 Train loss: 3.954 Aux train loss: 2.390 Val loss: 2.545 Aux val loss: 1.519 Train MAE: 12.436 Val MAE: 20.050 Epoch time: 99.847 seconds \n",
      "Epoch: 187 Train loss: 3.883 Aux train loss: 2.352 Val loss: 2.503 Aux val loss: 1.498 Train MAE: 11.058 Val MAE: 23.439 Epoch time: 103.524 seconds \n",
      "Epoch: 188 Train loss: 3.961 Aux train loss: 2.417 Val loss: 2.496 Aux val loss: 1.512 Train MAE: 11.288 Val MAE: 14.816 Epoch time: 100.583 seconds \n",
      "Epoch: 189 Train loss: 4.119 Aux train loss: 2.496 Val loss: 2.517 Aux val loss: 1.497 Train MAE: 11.745 Val MAE: 15.780 Epoch time: 102.325 seconds \n",
      "Epoch: 190 Train loss: 3.941 Aux train loss: 2.378 Val loss: 2.445 Aux val loss: 1.486 Train MAE: 11.576 Val MAE: 21.853 Epoch time: 108.253 seconds \n",
      "Epoch: 191 Train loss: 3.908 Aux train loss: 2.360 Val loss: 2.539 Aux val loss: 1.524 Train MAE: 10.960 Val MAE: 16.265 Epoch time: 100.583 seconds \n",
      "Epoch: 192 Train loss: 4.439 Aux train loss: 2.692 Val loss: 2.574 Aux val loss: 1.543 Train MAE: 11.562 Val MAE: 17.739 Epoch time: 104.758 seconds \n",
      "Epoch: 193 Train loss: 4.198 Aux train loss: 2.551 Val loss: 2.490 Aux val loss: 1.485 Train MAE: 11.696 Val MAE: 18.117 Epoch time: 93.301 seconds \n",
      "Epoch: 194 Train loss: 4.018 Aux train loss: 2.451 Val loss: 2.387 Aux val loss: 1.443 Train MAE: 11.864 Val MAE: 14.366 Epoch time: 95.117 seconds \n",
      "Epoch: 195 Train loss: 4.015 Aux train loss: 2.431 Val loss: 2.510 Aux val loss: 1.511 Train MAE: 12.406 Val MAE: 16.186 Epoch time: 100.112 seconds \n",
      "Epoch: 196 Train loss: 4.601 Aux train loss: 2.783 Val loss: 2.541 Aux val loss: 1.513 Train MAE: 13.293 Val MAE: 14.926 Epoch time: 104.834 seconds \n",
      "Epoch: 197 Train loss: 3.919 Aux train loss: 2.393 Val loss: 2.473 Aux val loss: 1.494 Train MAE: 10.861 Val MAE: 18.919 Epoch time: 103.035 seconds \n",
      "Epoch: 198 Train loss: 3.663 Aux train loss: 2.212 Val loss: 2.562 Aux val loss: 1.524 Train MAE: 11.192 Val MAE: 19.739 Epoch time: 107.701 seconds \n",
      "Epoch: 199 Train loss: 4.082 Aux train loss: 2.466 Val loss: 2.525 Aux val loss: 1.533 Train MAE: 12.466 Val MAE: 16.162 Epoch time: 106.161 seconds \n",
      "Epoch: 200 Train loss: 3.997 Aux train loss: 2.407 Val loss: 2.573 Aux val loss: 1.533 Train MAE: 13.200 Val MAE: 16.438 Epoch time: 96.125 seconds \n",
      "Epoch: 201 Train loss: 4.054 Aux train loss: 2.451 Val loss: 2.451 Aux val loss: 1.479 Train MAE: 9.391 Val MAE: 14.391 Epoch time: 105.930 seconds \n",
      "Epoch: 202 Train loss: 3.843 Aux train loss: 2.331 Val loss: 2.491 Aux val loss: 1.500 Train MAE: 7.710 Val MAE: 16.502 Epoch time: 112.399 seconds \n",
      "Epoch: 203 Train loss: 3.778 Aux train loss: 2.298 Val loss: 2.489 Aux val loss: 1.491 Train MAE: 7.968 Val MAE: 15.771 Epoch time: 102.912 seconds \n",
      "Epoch: 204 Train loss: 3.801 Aux train loss: 2.305 Val loss: 2.492 Aux val loss: 1.493 Train MAE: 7.852 Val MAE: 15.873 Epoch time: 96.390 seconds \n",
      "Epoch: 205 Train loss: 4.246 Aux train loss: 2.579 Val loss: 2.503 Aux val loss: 1.501 Train MAE: 7.725 Val MAE: 15.497 Epoch time: 106.336 seconds \n",
      "Epoch: 206 Train loss: 3.804 Aux train loss: 2.307 Val loss: 2.512 Aux val loss: 1.498 Train MAE: 7.093 Val MAE: 14.969 Epoch time: 106.611 seconds \n",
      "Epoch: 207 Train loss: 3.859 Aux train loss: 2.340 Val loss: 2.484 Aux val loss: 1.503 Train MAE: 7.508 Val MAE: 16.903 Epoch time: 100.852 seconds \n",
      "Epoch: 208 Train loss: 3.722 Aux train loss: 2.259 Val loss: 2.493 Aux val loss: 1.494 Train MAE: 7.250 Val MAE: 15.091 Epoch time: 101.360 seconds \n",
      "Epoch: 209 Train loss: 4.015 Aux train loss: 2.433 Val loss: 2.514 Aux val loss: 1.500 Train MAE: 7.215 Val MAE: 15.958 Epoch time: 104.404 seconds \n",
      "Epoch: 210 Train loss: 3.603 Aux train loss: 2.187 Val loss: 2.479 Aux val loss: 1.482 Train MAE: 7.635 Val MAE: 15.548 Epoch time: 107.185 seconds \n",
      "Epoch: 211 Train loss: 4.292 Aux train loss: 2.603 Val loss: 2.506 Aux val loss: 1.493 Train MAE: 7.301 Val MAE: 16.856 Epoch time: 100.065 seconds \n",
      "Epoch: 212 Train loss: 3.772 Aux train loss: 2.288 Val loss: 2.514 Aux val loss: 1.503 Train MAE: 7.256 Val MAE: 14.480 Epoch time: 103.358 seconds \n",
      "Epoch: 213 Train loss: 4.000 Aux train loss: 2.426 Val loss: 2.496 Aux val loss: 1.501 Train MAE: 7.399 Val MAE: 15.620 Epoch time: 101.148 seconds \n",
      "Epoch: 214 Train loss: 3.615 Aux train loss: 2.194 Val loss: 2.511 Aux val loss: 1.505 Train MAE: 6.945 Val MAE: 15.916 Epoch time: 100.861 seconds \n",
      "Epoch: 215 Train loss: 3.893 Aux train loss: 2.365 Val loss: 2.487 Aux val loss: 1.492 Train MAE: 7.752 Val MAE: 15.101 Epoch time: 105.680 seconds \n",
      "Epoch: 216 Train loss: 3.773 Aux train loss: 2.290 Val loss: 2.549 Aux val loss: 1.519 Train MAE: 7.147 Val MAE: 14.672 Epoch time: 103.987 seconds \n",
      "Epoch: 217 Train loss: 3.807 Aux train loss: 2.306 Val loss: 2.521 Aux val loss: 1.506 Train MAE: 7.571 Val MAE: 14.838 Epoch time: 102.956 seconds \n",
      "Epoch: 218 Train loss: 4.028 Aux train loss: 2.443 Val loss: 2.566 Aux val loss: 1.533 Train MAE: 7.635 Val MAE: 18.600 Epoch time: 103.162 seconds \n",
      "Epoch: 219 Train loss: 3.689 Aux train loss: 2.242 Val loss: 2.546 Aux val loss: 1.520 Train MAE: 6.931 Val MAE: 16.297 Epoch time: 100.228 seconds \n",
      "Epoch: 220 Train loss: 3.831 Aux train loss: 2.328 Val loss: 2.537 Aux val loss: 1.509 Train MAE: 7.009 Val MAE: 14.997 Epoch time: 98.184 seconds \n",
      "Epoch: 221 Train loss: 3.745 Aux train loss: 2.269 Val loss: 2.520 Aux val loss: 1.497 Train MAE: 7.395 Val MAE: 14.581 Epoch time: 96.371 seconds \n",
      "Epoch: 222 Train loss: 3.731 Aux train loss: 2.266 Val loss: 2.491 Aux val loss: 1.486 Train MAE: 7.022 Val MAE: 14.685 Epoch time: 105.455 seconds \n",
      "Epoch: 223 Train loss: 3.755 Aux train loss: 2.279 Val loss: 2.520 Aux val loss: 1.501 Train MAE: 7.838 Val MAE: 15.124 Epoch time: 107.214 seconds \n",
      "Epoch: 224 Train loss: 3.734 Aux train loss: 2.262 Val loss: 2.532 Aux val loss: 1.510 Train MAE: 7.394 Val MAE: 14.908 Epoch time: 110.475 seconds \n",
      "Epoch: 225 Train loss: 4.029 Aux train loss: 2.445 Val loss: 2.498 Aux val loss: 1.494 Train MAE: 7.352 Val MAE: 15.711 Epoch time: 98.977 seconds \n",
      "Epoch: 226 Train loss: 3.840 Aux train loss: 2.327 Val loss: 2.537 Aux val loss: 1.511 Train MAE: 6.998 Val MAE: 15.678 Epoch time: 106.715 seconds \n",
      "Epoch: 227 Train loss: 3.905 Aux train loss: 2.377 Val loss: 2.485 Aux val loss: 1.479 Train MAE: 7.273 Val MAE: 17.087 Epoch time: 104.902 seconds \n",
      "Epoch: 228 Train loss: 3.715 Aux train loss: 2.256 Val loss: 2.538 Aux val loss: 1.505 Train MAE: 6.977 Val MAE: 15.886 Epoch time: 97.671 seconds \n",
      "Epoch: 229 Train loss: 3.696 Aux train loss: 2.241 Val loss: 2.511 Aux val loss: 1.504 Train MAE: 7.119 Val MAE: 14.626 Epoch time: 107.193 seconds \n",
      "Epoch: 230 Train loss: 4.133 Aux train loss: 2.505 Val loss: 2.513 Aux val loss: 1.501 Train MAE: 7.888 Val MAE: 15.361 Epoch time: 104.238 seconds \n",
      "Epoch: 231 Train loss: 3.901 Aux train loss: 2.366 Val loss: 2.503 Aux val loss: 1.487 Train MAE: 6.871 Val MAE: 15.311 Epoch time: 100.124 seconds \n",
      "Epoch: 232 Train loss: 3.971 Aux train loss: 2.409 Val loss: 2.523 Aux val loss: 1.502 Train MAE: 7.197 Val MAE: 15.289 Epoch time: 109.777 seconds \n",
      "Epoch: 233 Train loss: 3.869 Aux train loss: 2.345 Val loss: 2.501 Aux val loss: 1.492 Train MAE: 6.968 Val MAE: 14.614 Epoch time: 109.851 seconds \n",
      "Epoch: 234 Train loss: 3.655 Aux train loss: 2.218 Val loss: 2.498 Aux val loss: 1.491 Train MAE: 7.000 Val MAE: 16.948 Epoch time: 103.661 seconds \n",
      "Epoch: 235 Train loss: 3.651 Aux train loss: 2.212 Val loss: 2.553 Aux val loss: 1.516 Train MAE: 6.700 Val MAE: 15.998 Epoch time: 102.402 seconds \n",
      "Epoch: 236 Train loss: 3.581 Aux train loss: 2.175 Val loss: 2.533 Aux val loss: 1.500 Train MAE: 7.510 Val MAE: 15.862 Epoch time: 107.142 seconds \n",
      "Epoch: 237 Train loss: 3.827 Aux train loss: 2.319 Val loss: 2.492 Aux val loss: 1.485 Train MAE: 7.670 Val MAE: 14.941 Epoch time: 103.674 seconds \n",
      "Epoch: 238 Train loss: 4.082 Aux train loss: 2.482 Val loss: 2.495 Aux val loss: 1.482 Train MAE: 7.544 Val MAE: 15.252 Epoch time: 99.077 seconds \n",
      "Epoch: 239 Train loss: 3.731 Aux train loss: 2.259 Val loss: 2.476 Aux val loss: 1.473 Train MAE: 6.756 Val MAE: 15.472 Epoch time: 107.601 seconds \n",
      "Epoch: 240 Train loss: 4.310 Aux train loss: 2.616 Val loss: 2.563 Aux val loss: 1.513 Train MAE: 7.797 Val MAE: 16.220 Epoch time: 107.028 seconds \n",
      "Epoch: 241 Train loss: 3.589 Aux train loss: 2.187 Val loss: 2.557 Aux val loss: 1.509 Train MAE: 6.688 Val MAE: 15.830 Epoch time: 95.083 seconds \n",
      "Epoch: 242 Train loss: 4.038 Aux train loss: 2.451 Val loss: 2.531 Aux val loss: 1.512 Train MAE: 7.134 Val MAE: 14.465 Epoch time: 99.587 seconds \n",
      "Epoch: 243 Train loss: 3.946 Aux train loss: 2.390 Val loss: 2.516 Aux val loss: 1.497 Train MAE: 6.751 Val MAE: 16.791 Epoch time: 94.116 seconds \n",
      "Epoch: 244 Train loss: 3.662 Aux train loss: 2.222 Val loss: 2.508 Aux val loss: 1.510 Train MAE: 7.260 Val MAE: 17.486 Epoch time: 101.805 seconds \n",
      "Epoch: 245 Train loss: 3.567 Aux train loss: 2.166 Val loss: 2.515 Aux val loss: 1.503 Train MAE: 6.840 Val MAE: 14.635 Epoch time: 95.312 seconds \n",
      "Epoch: 246 Train loss: 3.664 Aux train loss: 2.222 Val loss: 2.523 Aux val loss: 1.505 Train MAE: 6.778 Val MAE: 17.809 Epoch time: 105.622 seconds \n",
      "Epoch: 247 Train loss: 3.286 Aux train loss: 1.997 Val loss: 2.548 Aux val loss: 1.520 Train MAE: 6.570 Val MAE: 15.615 Epoch time: 107.168 seconds \n",
      "Epoch: 248 Train loss: 3.762 Aux train loss: 2.284 Val loss: 2.526 Aux val loss: 1.509 Train MAE: 6.773 Val MAE: 15.534 Epoch time: 112.878 seconds \n",
      "Epoch: 249 Train loss: 3.818 Aux train loss: 2.319 Val loss: 2.532 Aux val loss: 1.516 Train MAE: 6.692 Val MAE: 14.532 Epoch time: 107.986 seconds \n",
      "Epoch: 250 Train loss: 3.726 Aux train loss: 2.256 Val loss: 2.581 Aux val loss: 1.535 Train MAE: 6.875 Val MAE: 17.153 Epoch time: 114.307 seconds \n",
      "Epoch: 251 Train loss: 3.564 Aux train loss: 2.169 Val loss: 2.519 Aux val loss: 1.503 Train MAE: 7.101 Val MAE: 15.845 Epoch time: 98.972 seconds \n",
      "Epoch: 252 Train loss: 4.210 Aux train loss: 2.549 Val loss: 2.611 Aux val loss: 1.547 Train MAE: 6.700 Val MAE: 18.579 Epoch time: 113.748 seconds \n",
      "Epoch: 253 Train loss: 3.810 Aux train loss: 2.316 Val loss: 2.542 Aux val loss: 1.507 Train MAE: 7.203 Val MAE: 16.334 Epoch time: 106.878 seconds \n",
      "Epoch: 254 Train loss: 3.244 Aux train loss: 1.970 Val loss: 2.502 Aux val loss: 1.496 Train MAE: 6.479 Val MAE: 14.454 Epoch time: 111.258 seconds \n",
      "Epoch: 255 Train loss: 3.674 Aux train loss: 2.233 Val loss: 2.546 Aux val loss: 1.518 Train MAE: 7.031 Val MAE: 15.771 Epoch time: 114.774 seconds \n",
      "Epoch: 256 Train loss: 3.477 Aux train loss: 2.114 Val loss: 2.545 Aux val loss: 1.511 Train MAE: 6.965 Val MAE: 15.050 Epoch time: 111.676 seconds \n",
      "Epoch: 257 Train loss: 3.953 Aux train loss: 2.396 Val loss: 2.570 Aux val loss: 1.539 Train MAE: 7.251 Val MAE: 16.736 Epoch time: 112.593 seconds \n",
      "Epoch: 258 Train loss: 3.642 Aux train loss: 2.214 Val loss: 2.543 Aux val loss: 1.525 Train MAE: 6.770 Val MAE: 14.760 Epoch time: 111.314 seconds \n",
      "Epoch: 259 Train loss: 3.612 Aux train loss: 2.190 Val loss: 2.525 Aux val loss: 1.516 Train MAE: 7.017 Val MAE: 14.812 Epoch time: 110.101 seconds \n",
      "Epoch: 260 Train loss: 3.678 Aux train loss: 2.226 Val loss: 2.557 Aux val loss: 1.530 Train MAE: 7.197 Val MAE: 16.460 Epoch time: 105.287 seconds \n",
      "Epoch: 261 Train loss: 3.700 Aux train loss: 2.242 Val loss: 2.594 Aux val loss: 1.549 Train MAE: 6.991 Val MAE: 17.152 Epoch time: 113.276 seconds \n",
      "Epoch: 262 Train loss: 3.826 Aux train loss: 2.331 Val loss: 2.583 Aux val loss: 1.543 Train MAE: 7.460 Val MAE: 15.595 Epoch time: 120.875 seconds \n",
      "Epoch: 263 Train loss: 3.601 Aux train loss: 2.183 Val loss: 2.579 Aux val loss: 1.535 Train MAE: 6.488 Val MAE: 15.575 Epoch time: 114.860 seconds \n",
      "Epoch: 264 Train loss: 3.609 Aux train loss: 2.186 Val loss: 2.565 Aux val loss: 1.530 Train MAE: 6.842 Val MAE: 17.199 Epoch time: 100.315 seconds \n",
      "Epoch: 265 Train loss: 3.496 Aux train loss: 2.130 Val loss: 2.540 Aux val loss: 1.507 Train MAE: 7.097 Val MAE: 14.971 Epoch time: 104.892 seconds \n",
      "Epoch: 266 Train loss: 3.718 Aux train loss: 2.255 Val loss: 2.562 Aux val loss: 1.523 Train MAE: 6.728 Val MAE: 15.746 Epoch time: 114.919 seconds \n",
      "Epoch: 267 Train loss: 3.497 Aux train loss: 2.125 Val loss: 2.526 Aux val loss: 1.507 Train MAE: 6.861 Val MAE: 15.172 Epoch time: 115.360 seconds \n",
      "Epoch: 268 Train loss: 3.525 Aux train loss: 2.141 Val loss: 2.580 Aux val loss: 1.540 Train MAE: 6.944 Val MAE: 16.348 Epoch time: 110.507 seconds \n",
      "Epoch: 269 Train loss: 3.867 Aux train loss: 2.348 Val loss: 2.510 Aux val loss: 1.501 Train MAE: 6.575 Val MAE: 13.942 Epoch time: 115.274 seconds \n",
      "Epoch: 270 Train loss: 4.011 Aux train loss: 2.437 Val loss: 2.542 Aux val loss: 1.505 Train MAE: 7.663 Val MAE: 16.000 Epoch time: 105.910 seconds \n",
      "Epoch: 271 Train loss: 3.603 Aux train loss: 2.187 Val loss: 2.546 Aux val loss: 1.519 Train MAE: 6.844 Val MAE: 15.283 Epoch time: 112.156 seconds \n",
      "Epoch: 272 Train loss: 3.682 Aux train loss: 2.237 Val loss: 2.510 Aux val loss: 1.496 Train MAE: 6.705 Val MAE: 15.877 Epoch time: 112.134 seconds \n",
      "Epoch: 273 Train loss: 3.891 Aux train loss: 2.365 Val loss: 2.516 Aux val loss: 1.497 Train MAE: 6.737 Val MAE: 15.174 Epoch time: 109.130 seconds \n",
      "Epoch: 274 Train loss: 3.579 Aux train loss: 2.175 Val loss: 2.512 Aux val loss: 1.498 Train MAE: 6.722 Val MAE: 14.611 Epoch time: 109.068 seconds \n",
      "Epoch: 275 Train loss: 4.136 Aux train loss: 2.509 Val loss: 2.519 Aux val loss: 1.506 Train MAE: 7.298 Val MAE: 16.816 Epoch time: 96.524 seconds \n",
      "Epoch: 276 Train loss: 3.900 Aux train loss: 2.369 Val loss: 2.490 Aux val loss: 1.487 Train MAE: 6.984 Val MAE: 15.692 Epoch time: 113.553 seconds \n",
      "Epoch: 277 Train loss: 3.739 Aux train loss: 2.272 Val loss: 2.515 Aux val loss: 1.509 Train MAE: 6.858 Val MAE: 14.947 Epoch time: 105.665 seconds \n",
      "Epoch: 278 Train loss: 3.682 Aux train loss: 2.240 Val loss: 2.542 Aux val loss: 1.511 Train MAE: 6.625 Val MAE: 17.010 Epoch time: 112.576 seconds \n",
      "Epoch: 279 Train loss: 4.069 Aux train loss: 2.465 Val loss: 2.549 Aux val loss: 1.514 Train MAE: 6.716 Val MAE: 15.018 Epoch time: 111.323 seconds \n",
      "Epoch: 280 Train loss: 3.776 Aux train loss: 2.291 Val loss: 2.500 Aux val loss: 1.488 Train MAE: 6.636 Val MAE: 15.243 Epoch time: 114.829 seconds \n",
      "Epoch: 281 Train loss: 3.527 Aux train loss: 2.144 Val loss: 2.519 Aux val loss: 1.503 Train MAE: 7.111 Val MAE: 14.542 Epoch time: 112.473 seconds \n",
      "Epoch: 282 Train loss: 3.453 Aux train loss: 2.099 Val loss: 2.507 Aux val loss: 1.497 Train MAE: 6.330 Val MAE: 19.080 Epoch time: 109.320 seconds \n",
      "Epoch: 283 Train loss: 3.355 Aux train loss: 2.040 Val loss: 2.518 Aux val loss: 1.507 Train MAE: 6.483 Val MAE: 14.661 Epoch time: 113.531 seconds \n",
      "Epoch: 284 Train loss: 3.418 Aux train loss: 2.078 Val loss: 2.501 Aux val loss: 1.495 Train MAE: 6.733 Val MAE: 16.038 Epoch time: 109.176 seconds \n",
      "Epoch: 285 Train loss: 3.955 Aux train loss: 2.400 Val loss: 2.539 Aux val loss: 1.513 Train MAE: 6.752 Val MAE: 14.717 Epoch time: 110.505 seconds \n",
      "Epoch: 286 Train loss: 3.656 Aux train loss: 2.223 Val loss: 2.539 Aux val loss: 1.514 Train MAE: 7.166 Val MAE: 15.608 Epoch time: 108.529 seconds \n",
      "Epoch: 287 Train loss: 3.771 Aux train loss: 2.288 Val loss: 2.506 Aux val loss: 1.497 Train MAE: 6.775 Val MAE: 16.596 Epoch time: 110.143 seconds \n",
      "Epoch: 288 Train loss: 3.374 Aux train loss: 2.051 Val loss: 2.481 Aux val loss: 1.480 Train MAE: 6.502 Val MAE: 14.630 Epoch time: 109.541 seconds \n",
      "Epoch: 289 Train loss: 4.266 Aux train loss: 2.593 Val loss: 2.472 Aux val loss: 1.480 Train MAE: 7.081 Val MAE: 13.788 Epoch time: 110.784 seconds \n",
      "Epoch: 290 Train loss: 3.550 Aux train loss: 2.164 Val loss: 2.531 Aux val loss: 1.502 Train MAE: 6.224 Val MAE: 15.023 Epoch time: 112.701 seconds \n",
      "Epoch: 291 Train loss: 3.682 Aux train loss: 2.234 Val loss: 2.549 Aux val loss: 1.512 Train MAE: 6.579 Val MAE: 15.669 Epoch time: 112.789 seconds \n",
      "Epoch: 292 Train loss: 3.688 Aux train loss: 2.241 Val loss: 2.488 Aux val loss: 1.485 Train MAE: 6.747 Val MAE: 13.872 Epoch time: 113.443 seconds \n",
      "Epoch: 293 Train loss: 3.484 Aux train loss: 2.119 Val loss: 2.534 Aux val loss: 1.509 Train MAE: 6.445 Val MAE: 17.199 Epoch time: 109.786 seconds \n",
      "Epoch: 294 Train loss: 3.606 Aux train loss: 2.192 Val loss: 2.553 Aux val loss: 1.509 Train MAE: 7.074 Val MAE: 17.168 Epoch time: 116.635 seconds \n",
      "Epoch: 295 Train loss: 4.020 Aux train loss: 2.438 Val loss: 2.460 Aux val loss: 1.476 Train MAE: 6.700 Val MAE: 14.309 Epoch time: 112.775 seconds \n",
      "Epoch: 296 Train loss: 3.751 Aux train loss: 2.279 Val loss: 2.529 Aux val loss: 1.500 Train MAE: 7.520 Val MAE: 17.164 Epoch time: 116.916 seconds \n",
      "Epoch: 297 Train loss: 3.811 Aux train loss: 2.314 Val loss: 2.559 Aux val loss: 1.523 Train MAE: 6.608 Val MAE: 15.958 Epoch time: 106.744 seconds \n",
      "Epoch: 298 Train loss: 3.550 Aux train loss: 2.163 Val loss: 2.538 Aux val loss: 1.510 Train MAE: 6.960 Val MAE: 15.006 Epoch time: 107.571 seconds \n",
      "Epoch: 299 Train loss: 3.358 Aux train loss: 2.044 Val loss: 2.513 Aux val loss: 1.494 Train MAE: 6.979 Val MAE: 16.900 Epoch time: 96.816 seconds \n",
      "Epoch: 300 Train loss: 3.554 Aux train loss: 2.155 Val loss: 2.532 Aux val loss: 1.508 Train MAE: 6.634 Val MAE: 17.615 Epoch time: 120.130 seconds \n",
      "[E323 19:29:51.125742833 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=368106, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\n",
      "ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \n",
      "Last error:\n",
      "\n",
      "Exception raised from checkForNCCLErrorsInternal at /opt/conda/conda-bld/pytorch_1729647352509/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6feefaa446 in /opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f6ff0310000 in /opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x7f6ff031024c in /opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isStarted() + 0x90 (0x7f6ff0310520 in /opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x9f8 (0x7f6ff03183f8 in /opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f6ff031969d in /opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0x145c0 (0x7f70402435c0 in /opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/lib/libtorch.so)\n",
      "frame #7: <unknown function> + 0x8609 (0x7f7049d27609 in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
      "frame #8: clone + 0x43 (0x7f7049af2353 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=4,5,6,7 torchrun --nproc_per_node=4 module12/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--model_name=efficient21_freeze_gd --epochs=300 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b026e48-e6d5-4f8b-a6ce-d3d6b01f84b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Val set MAE: 13.50 RMSE: 38.30\n",
      "GT count: tensor([30.,  8.], device='cuda:0')\n",
      "Predicted count: tensor([27.6143,  4.6073], device='cuda:0')\n",
      "Test set MAE: 13.89 RMSE: 82.43\n",
      "GT count: tensor([17., 34.], device='cuda:0')\n",
      "Predicted count: tensor([15.9332, 28.7420], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=7 torchrun --nproc_per_node=1 module12/evaluate.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--model_name=efficient21_freeze_gd --epochs=300 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7d52e-bc1e-4737-a48e-52d605404d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f5e70-1918-4fa7-97b2-cd8b7349dda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045dde5-022a-4072-ae82-0cd450e11e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f777248-ebe8-4c27-9437-6f1d537c54e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0324 11:49:56.213000 3343369 site-packages/torch/distributed/run.py:793] \n",
      "W0324 11:49:56.213000 3343369 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0324 11:49:56.213000 3343369 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0324 11:49:56.213000 3343369 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "0\n",
      "1\n",
      "Epoch: 1 Train loss: 16.139 Aux train loss: 9.685 Val loss: 6.327 Aux val loss: 3.802 Train MAE: 52.485 Val MAE: 32.647 Epoch time: 203.662 seconds best\n",
      "Epoch: 2 Train loss: 14.957 Aux train loss: 8.967 Val loss: 6.222 Aux val loss: 3.733 Train MAE: 37.768 Val MAE: 34.179 Epoch time: 193.354 seconds \n",
      "Epoch: 3 Train loss: 15.123 Aux train loss: 9.016 Val loss: 6.165 Aux val loss: 3.669 Train MAE: 37.296 Val MAE: 28.881 Epoch time: 189.884 seconds best\n",
      "Epoch: 4 Train loss: 14.444 Aux train loss: 8.583 Val loss: 5.924 Aux val loss: 3.543 Train MAE: 30.875 Val MAE: 25.965 Epoch time: 181.005 seconds best\n",
      "Epoch: 5 Train loss: 12.995 Aux train loss: 7.782 Val loss: 5.717 Aux val loss: 3.435 Train MAE: 29.227 Val MAE: 25.434 Epoch time: 200.120 seconds best\n",
      "Epoch: 6 Train loss: 13.837 Aux train loss: 8.309 Val loss: 5.560 Aux val loss: 3.344 Train MAE: 32.667 Val MAE: 22.799 Epoch time: 188.585 seconds best\n",
      "Epoch: 7 Train loss: 12.244 Aux train loss: 7.355 Val loss: 5.623 Aux val loss: 3.381 Train MAE: 27.198 Val MAE: 34.099 Epoch time: 186.609 seconds \n",
      "Epoch: 8 Train loss: 12.848 Aux train loss: 7.711 Val loss: 5.381 Aux val loss: 3.232 Train MAE: 25.262 Val MAE: 21.907 Epoch time: 185.572 seconds best\n",
      "Epoch: 9 Train loss: 11.941 Aux train loss: 7.173 Val loss: 5.475 Aux val loss: 3.289 Train MAE: 25.614 Val MAE: 24.471 Epoch time: 186.976 seconds \n",
      "Epoch: 10 Train loss: 11.907 Aux train loss: 7.092 Val loss: 5.386 Aux val loss: 3.233 Train MAE: 24.484 Val MAE: 29.998 Epoch time: 194.734 seconds \n",
      "Epoch: 11 Train loss: 11.111 Aux train loss: 6.699 Val loss: 5.399 Aux val loss: 3.247 Train MAE: 25.580 Val MAE: 26.230 Epoch time: 193.573 seconds \n",
      "Epoch: 12 Train loss: 11.348 Aux train loss: 6.809 Val loss: 5.268 Aux val loss: 3.157 Train MAE: 22.832 Val MAE: 24.391 Epoch time: 199.649 seconds \n",
      "Epoch: 13 Train loss: 11.345 Aux train loss: 6.802 Val loss: 5.223 Aux val loss: 3.125 Train MAE: 20.758 Val MAE: 22.135 Epoch time: 189.153 seconds \n",
      "Epoch: 14 Train loss: 11.262 Aux train loss: 6.757 Val loss: 5.336 Aux val loss: 3.192 Train MAE: 20.901 Val MAE: 28.261 Epoch time: 190.624 seconds \n",
      "Epoch: 15 Train loss: 10.827 Aux train loss: 6.489 Val loss: 5.322 Aux val loss: 3.176 Train MAE: 19.781 Val MAE: 24.624 Epoch time: 188.929 seconds \n",
      "Epoch: 16 Train loss: 11.795 Aux train loss: 7.050 Val loss: 5.263 Aux val loss: 3.140 Train MAE: 22.739 Val MAE: 29.932 Epoch time: 184.482 seconds \n",
      "Epoch: 17 Train loss: 10.804 Aux train loss: 6.477 Val loss: 5.356 Aux val loss: 3.211 Train MAE: 19.888 Val MAE: 21.434 Epoch time: 186.414 seconds best\n",
      "Epoch: 18 Train loss: 10.614 Aux train loss: 6.370 Val loss: 5.110 Aux val loss: 3.060 Train MAE: 20.596 Val MAE: 21.559 Epoch time: 201.971 seconds \n",
      "Epoch: 19 Train loss: 10.515 Aux train loss: 6.298 Val loss: 5.251 Aux val loss: 3.133 Train MAE: 20.822 Val MAE: 21.969 Epoch time: 185.998 seconds \n",
      "Epoch: 20 Train loss: 10.770 Aux train loss: 6.456 Val loss: 5.036 Aux val loss: 3.033 Train MAE: 18.730 Val MAE: 25.209 Epoch time: 200.936 seconds \n",
      "Epoch: 21 Train loss: 10.488 Aux train loss: 6.283 Val loss: 5.018 Aux val loss: 2.999 Train MAE: 18.455 Val MAE: 20.575 Epoch time: 184.942 seconds best\n",
      "Epoch: 22 Train loss: 10.076 Aux train loss: 6.041 Val loss: 5.196 Aux val loss: 3.094 Train MAE: 19.197 Val MAE: 28.961 Epoch time: 195.714 seconds \n",
      "Epoch: 23 Train loss: 9.760 Aux train loss: 5.844 Val loss: 5.216 Aux val loss: 3.110 Train MAE: 20.678 Val MAE: 22.983 Epoch time: 191.345 seconds \n",
      "Epoch: 24 Train loss: 9.836 Aux train loss: 5.879 Val loss: 4.954 Aux val loss: 2.961 Train MAE: 18.393 Val MAE: 21.721 Epoch time: 190.732 seconds \n",
      "Epoch: 25 Train loss: 10.274 Aux train loss: 6.155 Val loss: 5.216 Aux val loss: 3.118 Train MAE: 18.926 Val MAE: 24.891 Epoch time: 195.129 seconds \n",
      "Epoch: 26 Train loss: 10.235 Aux train loss: 6.117 Val loss: 4.993 Aux val loss: 2.984 Train MAE: 19.267 Val MAE: 23.435 Epoch time: 184.506 seconds \n",
      "Epoch: 27 Train loss: 9.986 Aux train loss: 5.960 Val loss: 4.960 Aux val loss: 2.971 Train MAE: 17.185 Val MAE: 22.959 Epoch time: 191.569 seconds \n",
      "Epoch: 28 Train loss: 10.009 Aux train loss: 5.999 Val loss: 5.136 Aux val loss: 3.062 Train MAE: 18.998 Val MAE: 28.205 Epoch time: 198.570 seconds \n",
      "Epoch: 29 Train loss: 9.454 Aux train loss: 5.648 Val loss: 4.842 Aux val loss: 2.871 Train MAE: 17.537 Val MAE: 19.776 Epoch time: 190.900 seconds best\n",
      "Epoch: 30 Train loss: 10.022 Aux train loss: 6.002 Val loss: 5.079 Aux val loss: 3.033 Train MAE: 18.848 Val MAE: 22.455 Epoch time: 202.443 seconds \n",
      "Epoch: 31 Train loss: 9.596 Aux train loss: 5.729 Val loss: 5.049 Aux val loss: 3.010 Train MAE: 20.454 Val MAE: 23.893 Epoch time: 204.277 seconds \n",
      "Epoch: 32 Train loss: 9.059 Aux train loss: 5.422 Val loss: 5.032 Aux val loss: 2.998 Train MAE: 15.815 Val MAE: 22.750 Epoch time: 198.005 seconds \n",
      "Epoch: 33 Train loss: 9.388 Aux train loss: 5.612 Val loss: 5.063 Aux val loss: 3.014 Train MAE: 16.918 Val MAE: 21.019 Epoch time: 204.184 seconds \n",
      "Epoch: 34 Train loss: 9.150 Aux train loss: 5.484 Val loss: 5.085 Aux val loss: 3.021 Train MAE: 16.101 Val MAE: 21.814 Epoch time: 189.401 seconds \n",
      "Epoch: 35 Train loss: 9.263 Aux train loss: 5.537 Val loss: 4.777 Aux val loss: 2.856 Train MAE: 17.103 Val MAE: 19.570 Epoch time: 182.919 seconds best\n",
      "Epoch: 36 Train loss: 9.589 Aux train loss: 5.758 Val loss: 4.844 Aux val loss: 2.894 Train MAE: 16.164 Val MAE: 22.877 Epoch time: 197.190 seconds \n",
      "Epoch: 37 Train loss: 9.388 Aux train loss: 5.606 Val loss: 4.999 Aux val loss: 2.997 Train MAE: 16.802 Val MAE: 25.279 Epoch time: 196.332 seconds \n",
      "Epoch: 38 Train loss: 8.794 Aux train loss: 5.258 Val loss: 5.041 Aux val loss: 3.017 Train MAE: 15.114 Val MAE: 22.536 Epoch time: 189.046 seconds \n",
      "Epoch: 39 Train loss: 8.937 Aux train loss: 5.341 Val loss: 4.914 Aux val loss: 2.934 Train MAE: 17.025 Val MAE: 19.776 Epoch time: 185.864 seconds \n",
      "Epoch: 40 Train loss: 8.755 Aux train loss: 5.230 Val loss: 4.726 Aux val loss: 2.819 Train MAE: 15.991 Val MAE: 20.902 Epoch time: 187.001 seconds \n",
      "Epoch: 41 Train loss: 8.843 Aux train loss: 5.287 Val loss: 4.798 Aux val loss: 2.852 Train MAE: 16.272 Val MAE: 16.456 Epoch time: 182.058 seconds best\n",
      "Epoch: 42 Train loss: 9.621 Aux train loss: 5.754 Val loss: 4.855 Aux val loss: 2.914 Train MAE: 15.421 Val MAE: 19.903 Epoch time: 182.917 seconds \n",
      "Epoch: 43 Train loss: 9.563 Aux train loss: 5.736 Val loss: 4.991 Aux val loss: 2.978 Train MAE: 15.869 Val MAE: 22.509 Epoch time: 204.604 seconds \n",
      "Epoch: 44 Train loss: 9.207 Aux train loss: 5.497 Val loss: 4.783 Aux val loss: 2.874 Train MAE: 16.699 Val MAE: 18.845 Epoch time: 178.834 seconds \n",
      "Epoch: 45 Train loss: 8.712 Aux train loss: 5.214 Val loss: 4.779 Aux val loss: 2.840 Train MAE: 15.019 Val MAE: 19.669 Epoch time: 185.823 seconds \n",
      "Epoch: 46 Train loss: 9.398 Aux train loss: 5.570 Val loss: 4.878 Aux val loss: 2.920 Train MAE: 15.807 Val MAE: 21.020 Epoch time: 188.580 seconds \n",
      "Epoch: 47 Train loss: 9.653 Aux train loss: 5.780 Val loss: 4.608 Aux val loss: 2.752 Train MAE: 16.338 Val MAE: 18.024 Epoch time: 182.930 seconds \n",
      "Epoch: 48 Train loss: 8.951 Aux train loss: 5.350 Val loss: 4.799 Aux val loss: 2.868 Train MAE: 15.815 Val MAE: 19.500 Epoch time: 179.717 seconds \n",
      "Epoch: 49 Train loss: 8.453 Aux train loss: 5.055 Val loss: 4.804 Aux val loss: 2.880 Train MAE: 14.859 Val MAE: 20.369 Epoch time: 202.158 seconds \n",
      "Epoch: 50 Train loss: 9.524 Aux train loss: 5.676 Val loss: 4.680 Aux val loss: 2.797 Train MAE: 15.880 Val MAE: 20.913 Epoch time: 187.705 seconds \n",
      "Epoch: 51 Train loss: 8.739 Aux train loss: 5.232 Val loss: 4.733 Aux val loss: 2.835 Train MAE: 15.727 Val MAE: 19.323 Epoch time: 186.107 seconds \n",
      "Epoch: 52 Train loss: 8.951 Aux train loss: 5.353 Val loss: 5.016 Aux val loss: 2.996 Train MAE: 13.807 Val MAE: 26.709 Epoch time: 172.931 seconds \n",
      "Epoch: 53 Train loss: 9.107 Aux train loss: 5.415 Val loss: 4.878 Aux val loss: 2.933 Train MAE: 16.357 Val MAE: 24.639 Epoch time: 190.499 seconds \n",
      "Epoch: 54 Train loss: 9.380 Aux train loss: 5.621 Val loss: 4.896 Aux val loss: 2.940 Train MAE: 14.655 Val MAE: 27.092 Epoch time: 200.245 seconds \n",
      "Epoch: 55 Train loss: 9.355 Aux train loss: 5.592 Val loss: 4.944 Aux val loss: 2.951 Train MAE: 15.001 Val MAE: 21.251 Epoch time: 190.182 seconds \n",
      "Epoch: 56 Train loss: 8.883 Aux train loss: 5.313 Val loss: 4.811 Aux val loss: 2.885 Train MAE: 14.483 Val MAE: 21.543 Epoch time: 187.461 seconds \n",
      "Epoch: 57 Train loss: 9.023 Aux train loss: 5.385 Val loss: 5.077 Aux val loss: 3.034 Train MAE: 14.575 Val MAE: 21.979 Epoch time: 190.125 seconds \n",
      "Epoch: 58 Train loss: 8.336 Aux train loss: 4.974 Val loss: 4.653 Aux val loss: 2.778 Train MAE: 14.788 Val MAE: 21.127 Epoch time: 190.079 seconds \n",
      "Epoch: 59 Train loss: 9.203 Aux train loss: 5.510 Val loss: 4.606 Aux val loss: 2.751 Train MAE: 14.858 Val MAE: 19.920 Epoch time: 189.167 seconds \n",
      "Epoch: 60 Train loss: 9.148 Aux train loss: 5.467 Val loss: 4.725 Aux val loss: 2.823 Train MAE: 16.774 Val MAE: 18.126 Epoch time: 183.780 seconds \n",
      "Epoch: 61 Train loss: 8.810 Aux train loss: 5.255 Val loss: 4.637 Aux val loss: 2.774 Train MAE: 14.703 Val MAE: 18.786 Epoch time: 195.136 seconds \n",
      "Epoch: 62 Train loss: 8.508 Aux train loss: 5.088 Val loss: 4.762 Aux val loss: 2.844 Train MAE: 15.455 Val MAE: 17.247 Epoch time: 197.418 seconds \n",
      "Epoch: 63 Train loss: 8.725 Aux train loss: 5.191 Val loss: 4.649 Aux val loss: 2.774 Train MAE: 12.655 Val MAE: 17.912 Epoch time: 188.619 seconds \n",
      "Epoch: 64 Train loss: 9.123 Aux train loss: 5.460 Val loss: 4.677 Aux val loss: 2.797 Train MAE: 14.936 Val MAE: 18.620 Epoch time: 190.463 seconds \n",
      "Epoch: 65 Train loss: 8.824 Aux train loss: 5.272 Val loss: 4.731 Aux val loss: 2.847 Train MAE: 14.023 Val MAE: 23.022 Epoch time: 179.479 seconds \n",
      "Epoch: 66 Train loss: 8.126 Aux train loss: 4.864 Val loss: 4.566 Aux val loss: 2.741 Train MAE: 12.735 Val MAE: 20.578 Epoch time: 203.996 seconds \n",
      "Epoch: 67 Train loss: 7.336 Aux train loss: 4.376 Val loss: 4.675 Aux val loss: 2.800 Train MAE: 13.664 Val MAE: 20.709 Epoch time: 200.739 seconds \n",
      "Epoch: 68 Train loss: 8.092 Aux train loss: 4.838 Val loss: 4.644 Aux val loss: 2.767 Train MAE: 13.199 Val MAE: 19.849 Epoch time: 205.629 seconds \n",
      "Epoch: 69 Train loss: 8.716 Aux train loss: 5.195 Val loss: 4.709 Aux val loss: 2.809 Train MAE: 15.108 Val MAE: 23.136 Epoch time: 203.418 seconds \n",
      "Epoch: 70 Train loss: 8.168 Aux train loss: 4.890 Val loss: 4.657 Aux val loss: 2.777 Train MAE: 13.543 Val MAE: 21.580 Epoch time: 198.371 seconds \n",
      "Epoch: 71 Train loss: 8.627 Aux train loss: 5.157 Val loss: 4.640 Aux val loss: 2.783 Train MAE: 12.925 Val MAE: 18.616 Epoch time: 194.196 seconds \n",
      "Epoch: 72 Train loss: 8.995 Aux train loss: 5.383 Val loss: 4.607 Aux val loss: 2.767 Train MAE: 14.084 Val MAE: 16.562 Epoch time: 179.883 seconds \n",
      "Epoch: 73 Train loss: 7.866 Aux train loss: 4.704 Val loss: 4.504 Aux val loss: 2.693 Train MAE: 14.062 Val MAE: 17.020 Epoch time: 192.158 seconds \n",
      "Epoch: 74 Train loss: 8.030 Aux train loss: 4.804 Val loss: 4.750 Aux val loss: 2.834 Train MAE: 12.826 Val MAE: 25.242 Epoch time: 206.056 seconds \n",
      "Epoch: 75 Train loss: 7.902 Aux train loss: 4.722 Val loss: 4.726 Aux val loss: 2.797 Train MAE: 12.338 Val MAE: 18.028 Epoch time: 181.586 seconds \n",
      "Epoch: 76 Train loss: 7.670 Aux train loss: 4.593 Val loss: 4.577 Aux val loss: 2.735 Train MAE: 12.100 Val MAE: 18.562 Epoch time: 176.929 seconds \n",
      "Epoch: 77 Train loss: 8.763 Aux train loss: 5.239 Val loss: 4.693 Aux val loss: 2.811 Train MAE: 14.800 Val MAE: 19.660 Epoch time: 196.237 seconds \n",
      "Epoch: 78 Train loss: 8.084 Aux train loss: 4.827 Val loss: 4.457 Aux val loss: 2.656 Train MAE: 13.387 Val MAE: 17.833 Epoch time: 190.018 seconds \n",
      "Epoch: 79 Train loss: 8.310 Aux train loss: 4.980 Val loss: 4.564 Aux val loss: 2.729 Train MAE: 13.392 Val MAE: 16.727 Epoch time: 197.943 seconds \n",
      "Epoch: 80 Train loss: 7.683 Aux train loss: 4.602 Val loss: 4.576 Aux val loss: 2.744 Train MAE: 12.198 Val MAE: 20.496 Epoch time: 190.330 seconds \n",
      "Epoch: 81 Train loss: 8.100 Aux train loss: 4.842 Val loss: 4.640 Aux val loss: 2.779 Train MAE: 12.097 Val MAE: 18.589 Epoch time: 184.903 seconds \n",
      "Epoch: 82 Train loss: 8.079 Aux train loss: 4.838 Val loss: 4.526 Aux val loss: 2.706 Train MAE: 11.917 Val MAE: 15.901 Epoch time: 188.763 seconds best\n",
      "Epoch: 83 Train loss: 8.076 Aux train loss: 4.831 Val loss: 4.553 Aux val loss: 2.702 Train MAE: 12.300 Val MAE: 18.423 Epoch time: 197.604 seconds \n",
      "Epoch: 84 Train loss: 8.376 Aux train loss: 5.017 Val loss: 4.577 Aux val loss: 2.722 Train MAE: 12.586 Val MAE: 22.035 Epoch time: 194.165 seconds \n",
      "Epoch: 85 Train loss: 8.059 Aux train loss: 4.844 Val loss: 4.727 Aux val loss: 2.840 Train MAE: 13.230 Val MAE: 23.928 Epoch time: 197.433 seconds \n",
      "Epoch: 86 Train loss: 8.432 Aux train loss: 5.051 Val loss: 4.670 Aux val loss: 2.797 Train MAE: 14.226 Val MAE: 18.336 Epoch time: 193.524 seconds \n",
      "Epoch: 87 Train loss: 8.414 Aux train loss: 5.036 Val loss: 4.702 Aux val loss: 2.805 Train MAE: 12.749 Val MAE: 17.777 Epoch time: 185.809 seconds \n",
      "Epoch: 88 Train loss: 7.902 Aux train loss: 4.736 Val loss: 4.589 Aux val loss: 2.742 Train MAE: 12.606 Val MAE: 16.413 Epoch time: 187.076 seconds \n",
      "Epoch: 89 Train loss: 8.209 Aux train loss: 4.906 Val loss: 4.577 Aux val loss: 2.739 Train MAE: 12.512 Val MAE: 17.752 Epoch time: 185.166 seconds \n",
      "Epoch: 90 Train loss: 8.098 Aux train loss: 4.853 Val loss: 4.572 Aux val loss: 2.711 Train MAE: 12.770 Val MAE: 21.735 Epoch time: 191.522 seconds \n",
      "Epoch: 91 Train loss: 7.566 Aux train loss: 4.532 Val loss: 4.611 Aux val loss: 2.756 Train MAE: 12.710 Val MAE: 19.072 Epoch time: 186.827 seconds \n",
      "Epoch: 92 Train loss: 7.969 Aux train loss: 4.770 Val loss: 4.744 Aux val loss: 2.832 Train MAE: 12.198 Val MAE: 18.760 Epoch time: 194.256 seconds \n",
      "Epoch: 93 Train loss: 7.892 Aux train loss: 4.721 Val loss: 4.579 Aux val loss: 2.745 Train MAE: 12.792 Val MAE: 18.488 Epoch time: 189.080 seconds \n",
      "Epoch: 94 Train loss: 7.933 Aux train loss: 4.759 Val loss: 4.717 Aux val loss: 2.811 Train MAE: 12.026 Val MAE: 21.157 Epoch time: 199.264 seconds \n",
      "Epoch: 95 Train loss: 8.462 Aux train loss: 5.049 Val loss: 4.557 Aux val loss: 2.732 Train MAE: 12.989 Val MAE: 17.794 Epoch time: 192.960 seconds \n",
      "Epoch: 96 Train loss: 7.987 Aux train loss: 4.801 Val loss: 4.576 Aux val loss: 2.740 Train MAE: 13.069 Val MAE: 21.048 Epoch time: 184.913 seconds \n",
      "Epoch: 97 Train loss: 7.792 Aux train loss: 4.654 Val loss: 4.715 Aux val loss: 2.811 Train MAE: 11.678 Val MAE: 28.787 Epoch time: 184.624 seconds \n",
      "Epoch: 98 Train loss: 8.321 Aux train loss: 4.983 Val loss: 4.556 Aux val loss: 2.708 Train MAE: 12.494 Val MAE: 20.329 Epoch time: 186.875 seconds \n",
      "Epoch: 99 Train loss: 7.615 Aux train loss: 4.560 Val loss: 4.688 Aux val loss: 2.808 Train MAE: 12.146 Val MAE: 19.457 Epoch time: 132.831 seconds \n",
      "Epoch: 100 Train loss: 8.027 Aux train loss: 4.802 Val loss: 4.527 Aux val loss: 2.694 Train MAE: 11.954 Val MAE: 16.292 Epoch time: 129.660 seconds \n",
      "Epoch: 101 Train loss: 7.308 Aux train loss: 4.331 Val loss: 4.586 Aux val loss: 2.744 Train MAE: 11.403 Val MAE: 19.450 Epoch time: 129.638 seconds \n",
      "Epoch: 102 Train loss: 7.553 Aux train loss: 4.514 Val loss: 4.856 Aux val loss: 2.907 Train MAE: 11.535 Val MAE: 21.082 Epoch time: 147.697 seconds \n",
      "Epoch: 103 Train loss: 7.324 Aux train loss: 4.379 Val loss: 4.456 Aux val loss: 2.660 Train MAE: 12.295 Val MAE: 16.647 Epoch time: 154.091 seconds \n",
      "Epoch: 104 Train loss: 7.926 Aux train loss: 4.735 Val loss: 4.674 Aux val loss: 2.790 Train MAE: 12.522 Val MAE: 16.793 Epoch time: 132.466 seconds \n",
      "Epoch: 105 Train loss: 7.520 Aux train loss: 4.501 Val loss: 4.673 Aux val loss: 2.792 Train MAE: 11.669 Val MAE: 21.864 Epoch time: 147.974 seconds \n",
      "Epoch: 106 Train loss: 7.360 Aux train loss: 4.407 Val loss: 4.605 Aux val loss: 2.752 Train MAE: 12.320 Val MAE: 20.329 Epoch time: 159.276 seconds \n",
      "Epoch: 107 Train loss: 7.528 Aux train loss: 4.504 Val loss: 4.614 Aux val loss: 2.737 Train MAE: 11.320 Val MAE: 20.683 Epoch time: 152.851 seconds \n",
      "Epoch: 108 Train loss: 7.328 Aux train loss: 4.381 Val loss: 4.779 Aux val loss: 2.838 Train MAE: 11.688 Val MAE: 19.368 Epoch time: 155.164 seconds \n",
      "Epoch: 109 Train loss: 8.347 Aux train loss: 4.992 Val loss: 4.547 Aux val loss: 2.723 Train MAE: 13.159 Val MAE: 20.100 Epoch time: 144.302 seconds \n",
      "Epoch: 110 Train loss: 7.257 Aux train loss: 4.345 Val loss: 4.466 Aux val loss: 2.674 Train MAE: 11.739 Val MAE: 19.775 Epoch time: 138.257 seconds \n",
      "Epoch: 111 Train loss: 8.182 Aux train loss: 4.898 Val loss: 4.546 Aux val loss: 2.719 Train MAE: 11.887 Val MAE: 15.638 Epoch time: 156.926 seconds best\n",
      "Epoch: 112 Train loss: 7.907 Aux train loss: 4.744 Val loss: 4.467 Aux val loss: 2.668 Train MAE: 11.684 Val MAE: 18.047 Epoch time: 150.762 seconds \n",
      "Epoch: 113 Train loss: 7.510 Aux train loss: 4.499 Val loss: 4.648 Aux val loss: 2.773 Train MAE: 11.382 Val MAE: 17.772 Epoch time: 139.950 seconds \n",
      "Epoch: 114 Train loss: 7.529 Aux train loss: 4.503 Val loss: 4.574 Aux val loss: 2.740 Train MAE: 10.626 Val MAE: 18.319 Epoch time: 154.541 seconds \n",
      "Epoch: 115 Train loss: 7.296 Aux train loss: 4.364 Val loss: 4.639 Aux val loss: 2.770 Train MAE: 10.793 Val MAE: 17.415 Epoch time: 152.962 seconds \n",
      "Epoch: 116 Train loss: 7.571 Aux train loss: 4.545 Val loss: 4.677 Aux val loss: 2.797 Train MAE: 10.814 Val MAE: 19.477 Epoch time: 145.998 seconds \n",
      "Epoch: 117 Train loss: 7.709 Aux train loss: 4.614 Val loss: 4.544 Aux val loss: 2.684 Train MAE: 11.515 Val MAE: 18.186 Epoch time: 136.803 seconds \n",
      "Epoch: 118 Train loss: 7.244 Aux train loss: 4.348 Val loss: 4.580 Aux val loss: 2.751 Train MAE: 11.635 Val MAE: 20.611 Epoch time: 149.839 seconds \n",
      "Epoch: 119 Train loss: 7.347 Aux train loss: 4.401 Val loss: 4.731 Aux val loss: 2.832 Train MAE: 11.302 Val MAE: 20.386 Epoch time: 152.955 seconds \n",
      "Epoch: 120 Train loss: 8.013 Aux train loss: 4.793 Val loss: 4.715 Aux val loss: 2.809 Train MAE: 11.471 Val MAE: 26.025 Epoch time: 129.227 seconds \n",
      "Epoch: 121 Train loss: 7.903 Aux train loss: 4.711 Val loss: 4.591 Aux val loss: 2.744 Train MAE: 12.463 Val MAE: 17.406 Epoch time: 129.609 seconds \n",
      "Epoch: 122 Train loss: 7.281 Aux train loss: 4.355 Val loss: 4.481 Aux val loss: 2.671 Train MAE: 11.282 Val MAE: 15.673 Epoch time: 134.655 seconds \n",
      "Epoch: 123 Train loss: 7.786 Aux train loss: 4.659 Val loss: 4.585 Aux val loss: 2.758 Train MAE: 11.155 Val MAE: 17.762 Epoch time: 144.545 seconds \n",
      "Epoch: 124 Train loss: 7.679 Aux train loss: 4.593 Val loss: 4.582 Aux val loss: 2.725 Train MAE: 10.993 Val MAE: 16.282 Epoch time: 152.265 seconds \n",
      "Epoch: 125 Train loss: 7.177 Aux train loss: 4.289 Val loss: 4.709 Aux val loss: 2.828 Train MAE: 12.423 Val MAE: 18.417 Epoch time: 133.161 seconds \n",
      "Epoch: 126 Train loss: 7.533 Aux train loss: 4.504 Val loss: 4.555 Aux val loss: 2.736 Train MAE: 11.179 Val MAE: 18.368 Epoch time: 158.800 seconds \n",
      "Epoch: 127 Train loss: 7.418 Aux train loss: 4.459 Val loss: 4.638 Aux val loss: 2.774 Train MAE: 10.284 Val MAE: 18.246 Epoch time: 160.568 seconds \n",
      "Epoch: 128 Train loss: 7.238 Aux train loss: 4.326 Val loss: 4.785 Aux val loss: 2.851 Train MAE: 10.843 Val MAE: 18.704 Epoch time: 157.144 seconds \n",
      "Epoch: 129 Train loss: 7.579 Aux train loss: 4.535 Val loss: 4.715 Aux val loss: 2.811 Train MAE: 12.662 Val MAE: 19.857 Epoch time: 140.113 seconds \n",
      "Epoch: 130 Train loss: 7.673 Aux train loss: 4.588 Val loss: 4.669 Aux val loss: 2.796 Train MAE: 10.971 Val MAE: 22.502 Epoch time: 150.650 seconds \n",
      "Epoch: 131 Train loss: 6.983 Aux train loss: 4.185 Val loss: 4.698 Aux val loss: 2.816 Train MAE: 10.482 Val MAE: 19.246 Epoch time: 156.944 seconds \n",
      "Epoch: 132 Train loss: 7.165 Aux train loss: 4.289 Val loss: 4.427 Aux val loss: 2.656 Train MAE: 10.845 Val MAE: 16.852 Epoch time: 158.271 seconds \n",
      "Epoch: 133 Train loss: 7.784 Aux train loss: 4.653 Val loss: 4.456 Aux val loss: 2.673 Train MAE: 11.894 Val MAE: 20.167 Epoch time: 141.354 seconds \n",
      "Epoch: 134 Train loss: 7.301 Aux train loss: 4.360 Val loss: 4.528 Aux val loss: 2.681 Train MAE: 11.449 Val MAE: 18.267 Epoch time: 153.975 seconds \n",
      "Epoch: 135 Train loss: 7.042 Aux train loss: 4.213 Val loss: 4.504 Aux val loss: 2.694 Train MAE: 10.471 Val MAE: 18.938 Epoch time: 144.378 seconds \n",
      "Epoch: 136 Train loss: 7.271 Aux train loss: 4.350 Val loss: 4.424 Aux val loss: 2.644 Train MAE: 10.218 Val MAE: 16.117 Epoch time: 136.580 seconds \n",
      "Epoch: 137 Train loss: 7.133 Aux train loss: 4.266 Val loss: 4.439 Aux val loss: 2.667 Train MAE: 11.285 Val MAE: 19.615 Epoch time: 146.026 seconds \n",
      "Epoch: 138 Train loss: 7.018 Aux train loss: 4.201 Val loss: 4.540 Aux val loss: 2.722 Train MAE: 12.021 Val MAE: 17.699 Epoch time: 150.320 seconds \n",
      "Epoch: 139 Train loss: 7.024 Aux train loss: 4.209 Val loss: 4.614 Aux val loss: 2.763 Train MAE: 11.667 Val MAE: 17.289 Epoch time: 151.523 seconds \n",
      "Epoch: 140 Train loss: 7.501 Aux train loss: 4.491 Val loss: 4.622 Aux val loss: 2.751 Train MAE: 11.455 Val MAE: 22.493 Epoch time: 158.693 seconds \n",
      "Epoch: 141 Train loss: 6.896 Aux train loss: 4.130 Val loss: 4.659 Aux val loss: 2.780 Train MAE: 10.300 Val MAE: 24.282 Epoch time: 146.739 seconds \n",
      "Epoch: 142 Train loss: 7.525 Aux train loss: 4.492 Val loss: 4.541 Aux val loss: 2.731 Train MAE: 11.885 Val MAE: 18.121 Epoch time: 155.073 seconds \n",
      "Epoch: 143 Train loss: 7.915 Aux train loss: 4.740 Val loss: 4.585 Aux val loss: 2.767 Train MAE: 10.925 Val MAE: 24.894 Epoch time: 151.507 seconds \n",
      "Epoch: 144 Train loss: 7.103 Aux train loss: 4.255 Val loss: 4.370 Aux val loss: 2.627 Train MAE: 10.582 Val MAE: 20.248 Epoch time: 131.230 seconds \n",
      "Epoch: 145 Train loss: 8.131 Aux train loss: 4.865 Val loss: 4.466 Aux val loss: 2.672 Train MAE: 11.924 Val MAE: 16.830 Epoch time: 146.050 seconds \n",
      "Epoch: 146 Train loss: 6.572 Aux train loss: 3.931 Val loss: 4.565 Aux val loss: 2.739 Train MAE: 10.508 Val MAE: 16.603 Epoch time: 159.766 seconds \n",
      "Epoch: 147 Train loss: 7.690 Aux train loss: 4.609 Val loss: 4.536 Aux val loss: 2.719 Train MAE: 10.544 Val MAE: 17.473 Epoch time: 155.361 seconds \n",
      "Epoch: 148 Train loss: 7.303 Aux train loss: 4.372 Val loss: 4.463 Aux val loss: 2.675 Train MAE: 10.578 Val MAE: 16.871 Epoch time: 146.454 seconds \n",
      "Epoch: 149 Train loss: 7.063 Aux train loss: 4.237 Val loss: 4.493 Aux val loss: 2.695 Train MAE: 11.196 Val MAE: 16.581 Epoch time: 134.751 seconds \n",
      "Epoch: 150 Train loss: 7.499 Aux train loss: 4.489 Val loss: 4.507 Aux val loss: 2.706 Train MAE: 11.266 Val MAE: 16.244 Epoch time: 157.425 seconds \n",
      "Epoch: 151 Train loss: 7.073 Aux train loss: 4.239 Val loss: 4.549 Aux val loss: 2.723 Train MAE: 10.525 Val MAE: 17.070 Epoch time: 149.212 seconds \n",
      "Epoch: 152 Train loss: 7.423 Aux train loss: 4.446 Val loss: 4.535 Aux val loss: 2.714 Train MAE: 11.234 Val MAE: 19.864 Epoch time: 139.461 seconds \n",
      "Epoch: 153 Train loss: 7.319 Aux train loss: 4.377 Val loss: 4.513 Aux val loss: 2.716 Train MAE: 11.260 Val MAE: 19.589 Epoch time: 156.476 seconds \n",
      "Epoch: 154 Train loss: 7.631 Aux train loss: 4.574 Val loss: 4.570 Aux val loss: 2.732 Train MAE: 11.522 Val MAE: 21.555 Epoch time: 154.516 seconds \n",
      "Epoch: 155 Train loss: 7.695 Aux train loss: 4.601 Val loss: 4.456 Aux val loss: 2.660 Train MAE: 10.758 Val MAE: 14.681 Epoch time: 142.467 seconds best\n",
      "Epoch: 156 Train loss: 6.999 Aux train loss: 4.179 Val loss: 4.603 Aux val loss: 2.745 Train MAE: 11.749 Val MAE: 19.688 Epoch time: 150.433 seconds \n",
      "Epoch: 157 Train loss: 7.241 Aux train loss: 4.349 Val loss: 4.502 Aux val loss: 2.689 Train MAE: 10.303 Val MAE: 16.171 Epoch time: 160.033 seconds \n",
      "Epoch: 158 Train loss: 7.532 Aux train loss: 4.514 Val loss: 4.563 Aux val loss: 2.748 Train MAE: 10.399 Val MAE: 16.445 Epoch time: 154.893 seconds \n",
      "Epoch: 159 Train loss: 6.928 Aux train loss: 4.152 Val loss: 4.605 Aux val loss: 2.735 Train MAE: 9.649 Val MAE: 21.174 Epoch time: 153.488 seconds \n",
      "Epoch: 160 Train loss: 7.527 Aux train loss: 4.498 Val loss: 4.568 Aux val loss: 2.738 Train MAE: 10.321 Val MAE: 18.761 Epoch time: 142.093 seconds \n",
      "Epoch: 161 Train loss: 7.714 Aux train loss: 4.618 Val loss: 4.567 Aux val loss: 2.730 Train MAE: 10.156 Val MAE: 17.905 Epoch time: 130.523 seconds \n",
      "Epoch: 162 Train loss: 6.916 Aux train loss: 4.138 Val loss: 4.560 Aux val loss: 2.740 Train MAE: 10.592 Val MAE: 17.703 Epoch time: 135.031 seconds \n",
      "Epoch: 163 Train loss: 6.895 Aux train loss: 4.128 Val loss: 4.694 Aux val loss: 2.802 Train MAE: 9.839 Val MAE: 18.649 Epoch time: 154.984 seconds \n",
      "Epoch: 164 Train loss: 6.899 Aux train loss: 4.134 Val loss: 4.693 Aux val loss: 2.804 Train MAE: 10.740 Val MAE: 17.072 Epoch time: 152.590 seconds \n",
      "Epoch: 165 Train loss: 7.179 Aux train loss: 4.303 Val loss: 4.557 Aux val loss: 2.740 Train MAE: 10.411 Val MAE: 20.190 Epoch time: 159.648 seconds \n",
      "Epoch: 166 Train loss: 7.262 Aux train loss: 4.337 Val loss: 4.600 Aux val loss: 2.765 Train MAE: 10.764 Val MAE: 24.176 Epoch time: 158.471 seconds \n",
      "Epoch: 167 Train loss: 6.892 Aux train loss: 4.139 Val loss: 4.491 Aux val loss: 2.704 Train MAE: 9.827 Val MAE: 28.406 Epoch time: 155.180 seconds \n",
      "Epoch: 168 Train loss: 6.918 Aux train loss: 4.138 Val loss: 4.601 Aux val loss: 2.758 Train MAE: 10.993 Val MAE: 21.965 Epoch time: 153.168 seconds \n",
      "Epoch: 169 Train loss: 7.246 Aux train loss: 4.328 Val loss: 4.681 Aux val loss: 2.794 Train MAE: 10.398 Val MAE: 17.371 Epoch time: 149.580 seconds \n",
      "Epoch: 170 Train loss: 6.720 Aux train loss: 4.029 Val loss: 4.602 Aux val loss: 2.766 Train MAE: 10.106 Val MAE: 18.371 Epoch time: 155.768 seconds \n",
      "Epoch: 171 Train loss: 7.942 Aux train loss: 4.770 Val loss: 4.410 Aux val loss: 2.649 Train MAE: 10.871 Val MAE: 19.351 Epoch time: 151.638 seconds \n",
      "Epoch: 172 Train loss: 7.082 Aux train loss: 4.234 Val loss: 4.611 Aux val loss: 2.780 Train MAE: 10.194 Val MAE: 17.512 Epoch time: 150.539 seconds \n",
      "Epoch: 173 Train loss: 6.915 Aux train loss: 4.145 Val loss: 4.539 Aux val loss: 2.725 Train MAE: 10.120 Val MAE: 19.782 Epoch time: 151.026 seconds \n",
      "Epoch: 174 Train loss: 6.987 Aux train loss: 4.190 Val loss: 4.601 Aux val loss: 2.755 Train MAE: 9.140 Val MAE: 20.653 Epoch time: 157.961 seconds \n",
      "Epoch: 175 Train loss: 7.063 Aux train loss: 4.233 Val loss: 4.610 Aux val loss: 2.753 Train MAE: 10.986 Val MAE: 18.505 Epoch time: 154.934 seconds \n",
      "Epoch: 176 Train loss: 7.231 Aux train loss: 4.322 Val loss: 4.560 Aux val loss: 2.738 Train MAE: 11.081 Val MAE: 17.908 Epoch time: 146.436 seconds \n",
      "Epoch: 177 Train loss: 7.463 Aux train loss: 4.478 Val loss: 4.460 Aux val loss: 2.679 Train MAE: 12.265 Val MAE: 16.145 Epoch time: 159.895 seconds \n",
      "Epoch: 178 Train loss: 7.411 Aux train loss: 4.434 Val loss: 4.474 Aux val loss: 2.699 Train MAE: 10.176 Val MAE: 17.572 Epoch time: 142.786 seconds \n",
      "Epoch: 179 Train loss: 6.862 Aux train loss: 4.114 Val loss: 4.524 Aux val loss: 2.710 Train MAE: 9.653 Val MAE: 18.859 Epoch time: 135.228 seconds \n",
      "Epoch: 180 Train loss: 6.963 Aux train loss: 4.176 Val loss: 4.622 Aux val loss: 2.770 Train MAE: 10.953 Val MAE: 16.991 Epoch time: 146.928 seconds \n",
      "Epoch: 181 Train loss: 6.724 Aux train loss: 4.036 Val loss: 4.624 Aux val loss: 2.780 Train MAE: 9.446 Val MAE: 22.342 Epoch time: 153.932 seconds \n",
      "Epoch: 182 Train loss: 6.661 Aux train loss: 3.980 Val loss: 4.456 Aux val loss: 2.676 Train MAE: 9.960 Val MAE: 20.280 Epoch time: 154.795 seconds \n",
      "Epoch: 183 Train loss: 7.355 Aux train loss: 4.414 Val loss: 4.510 Aux val loss: 2.709 Train MAE: 11.032 Val MAE: 18.817 Epoch time: 140.706 seconds \n",
      "Epoch: 184 Train loss: 6.755 Aux train loss: 4.041 Val loss: 4.613 Aux val loss: 2.765 Train MAE: 10.996 Val MAE: 19.154 Epoch time: 133.488 seconds \n",
      "Epoch: 185 Train loss: 7.137 Aux train loss: 4.276 Val loss: 4.637 Aux val loss: 2.784 Train MAE: 10.638 Val MAE: 17.860 Epoch time: 156.427 seconds \n",
      "Epoch: 186 Train loss: 7.250 Aux train loss: 4.341 Val loss: 4.490 Aux val loss: 2.693 Train MAE: 10.422 Val MAE: 15.498 Epoch time: 151.076 seconds \n",
      "Epoch: 187 Train loss: 7.737 Aux train loss: 4.632 Val loss: 4.445 Aux val loss: 2.661 Train MAE: 11.679 Val MAE: 18.798 Epoch time: 141.930 seconds \n",
      "Epoch: 188 Train loss: 6.865 Aux train loss: 4.116 Val loss: 4.576 Aux val loss: 2.738 Train MAE: 9.772 Val MAE: 18.589 Epoch time: 134.136 seconds \n",
      "Epoch: 189 Train loss: 6.789 Aux train loss: 4.071 Val loss: 4.529 Aux val loss: 2.699 Train MAE: 9.572 Val MAE: 22.315 Epoch time: 154.670 seconds \n",
      "Epoch: 190 Train loss: 6.651 Aux train loss: 3.986 Val loss: 4.573 Aux val loss: 2.742 Train MAE: 9.517 Val MAE: 17.950 Epoch time: 155.822 seconds \n",
      "Epoch: 191 Train loss: 6.611 Aux train loss: 3.958 Val loss: 4.392 Aux val loss: 2.643 Train MAE: 10.014 Val MAE: 15.067 Epoch time: 133.119 seconds \n",
      "Epoch: 192 Train loss: 6.799 Aux train loss: 4.073 Val loss: 4.501 Aux val loss: 2.713 Train MAE: 9.010 Val MAE: 16.592 Epoch time: 155.538 seconds \n",
      "Epoch: 193 Train loss: 7.300 Aux train loss: 4.379 Val loss: 4.373 Aux val loss: 2.641 Train MAE: 9.751 Val MAE: 15.725 Epoch time: 153.014 seconds \n",
      "Epoch: 194 Train loss: 6.622 Aux train loss: 3.962 Val loss: 4.446 Aux val loss: 2.670 Train MAE: 9.227 Val MAE: 19.628 Epoch time: 143.678 seconds \n",
      "Epoch: 195 Train loss: 6.930 Aux train loss: 4.152 Val loss: 4.517 Aux val loss: 2.716 Train MAE: 9.477 Val MAE: 16.921 Epoch time: 152.773 seconds \n",
      "Epoch: 196 Train loss: 6.593 Aux train loss: 3.958 Val loss: 4.555 Aux val loss: 2.719 Train MAE: 9.213 Val MAE: 19.606 Epoch time: 149.248 seconds \n",
      "Epoch: 197 Train loss: 6.389 Aux train loss: 3.826 Val loss: 4.679 Aux val loss: 2.797 Train MAE: 9.140 Val MAE: 18.856 Epoch time: 163.739 seconds \n",
      "Epoch: 198 Train loss: 7.302 Aux train loss: 4.376 Val loss: 4.728 Aux val loss: 2.839 Train MAE: 10.562 Val MAE: 17.913 Epoch time: 156.088 seconds \n",
      "Epoch: 199 Train loss: 7.120 Aux train loss: 4.261 Val loss: 4.691 Aux val loss: 2.816 Train MAE: 11.329 Val MAE: 18.593 Epoch time: 159.191 seconds \n",
      "Epoch: 200 Train loss: 6.742 Aux train loss: 4.043 Val loss: 4.612 Aux val loss: 2.768 Train MAE: 9.528 Val MAE: 17.619 Epoch time: 136.918 seconds \n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=4,5 torchrun --nproc_per_node=2 --master_port=29501 module12/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--model_name=efficient23_freeze_gd --epochs=200 --pre_norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8eab8d-1d00-4df4-bd80-520cfe20fe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Val set MAE: 14.72 RMSE: 47.50\n",
      "GT count: tensor([30.,  8.], device='cuda:0')\n",
      "Predicted count: tensor([23.6692,  6.4431], device='cuda:0')\n",
      "Test set MAE: 13.97 RMSE: 89.81\n",
      "GT count: tensor([17., 34.], device='cuda:0')\n",
      "Predicted count: tensor([14.7221, 24.8863], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=5 torchrun --nproc_per_node=1 --master_port=29501 module12/evaluate.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--model_name=efficient23_freeze_gd --epochs=200 --pre_norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a9d9c-cad2-4bfa-bdd3-1522b10d5627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ce17d6-0ce8-4605-95db-2817647508f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0325 12:42:27.936000 3708957 site-packages/torch/distributed/run.py:793] \n",
      "W0325 12:42:27.936000 3708957 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0325 12:42:27.936000 3708957 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0325 12:42:27.936000 3708957 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "1\n",
      "0\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module12/train.py\", line 247, in <module>\n",
      "[rank0]:     train(args)\n",
      "[rank0]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module12/train.py\", line 131, in train\n",
      "[rank0]:     out, aux_out = model(img, bboxes)\n",
      "[rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1639, in forward\n",
      "[rank0]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)\n",
      "[rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1528, in _pre_forward\n",
      "[rank0]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\n",
      "[rank0]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \n",
      "[rank0]: making sure all `forward` function outputs participate in calculating loss. \n",
      "[rank0]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n",
      "[rank0]: Parameters which did not receive grad for rank 0: iefl.linear_attention.norm.bias, iefl.linear_attention.norm.weight\n",
      "[rank0]: Parameter indices which did not receive grad for rank 0: 104 105\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module12/train.py\", line 247, in <module>\n",
      "[rank1]:     train(args)\n",
      "[rank1]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module12/train.py\", line 131, in train\n",
      "[rank1]:     out, aux_out = model(img, bboxes)\n",
      "[rank1]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\n",
      "[rank1]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank1]:     return forward_call(*args, **kwargs)\n",
      "[rank1]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1639, in forward\n",
      "[rank1]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)\n",
      "[rank1]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1528, in _pre_forward\n",
      "[rank1]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\n",
      "[rank1]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \n",
      "[rank1]: making sure all `forward` function outputs participate in calculating loss. \n",
      "[rank1]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n",
      "[rank1]: Parameters which did not receive grad for rank 1: iefl.linear_attention.norm.bias, iefl.linear_attention.norm.weight\n",
      "[rank1]: Parameter indices which did not receive grad for rank 1: 104 105\n",
      "[rank0]:[W325 12:42:45.298658255 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "W0325 12:42:47.607000 3708957 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3708962 closing signal SIGTERM\n",
      "E0325 12:42:47.772000 3708957 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 3708961) of binary: /opt/miniconda/envs/Rey2/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda/envs/Rey2/bin/torchrun\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('torch==2.5.1', 'console_scripts', 'torchrun')())\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "module12/train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-03-25_12:42:47\n",
      "  host      : viplab-G481-H81-00\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 3708961)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc_per_node=2 module12/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--model_name=efficient24_freeze_gd --epochs=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab968bea-669f-41e1-8272-b676028f6db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0325 10:25:17.761000 3701620 site-packages/torch/distributed/run.py:793] \n",
      "W0325 10:25:17.761000 3701620 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0325 10:25:17.761000 3701620 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0325 10:25:17.761000 3701620 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_with_gdino_weights.pth\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module12/evaluate.py\", line 93, in <module>\n",
      "[rank0]:     evaluate(args)\n",
      "[rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module12/evaluate.py\", line 36, in evaluate\n",
      "[rank0]:     state_dict = torch.load(os.path.join(args.model_path, f'{args.model_name}.pt'), weights_only=True)['model']\n",
      "[rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/serialization.py\", line 1319, in load\n",
      "[rank0]:     with _open_file_like(f, \"rb\") as opened_file:\n",
      "[rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/serialization.py\", line 659, in _open_file_like\n",
      "[rank0]:     return _open_file(name_or_buffer, mode)\n",
      "[rank0]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/serialization.py\", line 640, in __init__\n",
      "[rank0]:     super().__init__(open(name, mode))\n",
      "[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/home/renaldy_fredyan/PhDResearch/ELS/checkpoints/efficient24_freeze_gd.pt'\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module12/evaluate.py\", line 93, in <module>\n",
      "[rank1]:     evaluate(args)\n",
      "[rank1]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "[rank1]:     return func(*args, **kwargs)\n",
      "[rank1]:   File \"/home/renaldy_fredyan/PhDResearch/ELS/module12/evaluate.py\", line 36, in evaluate\n",
      "[rank1]:     state_dict = torch.load(os.path.join(args.model_path, f'{args.model_name}.pt'), weights_only=True)['model']\n",
      "[rank1]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/serialization.py\", line 1319, in load\n",
      "[rank1]:     with _open_file_like(f, \"rb\") as opened_file:\n",
      "[rank1]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/serialization.py\", line 659, in _open_file_like\n",
      "[rank1]:     return _open_file(name_or_buffer, mode)\n",
      "[rank1]:   File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/serialization.py\", line 640, in __init__\n",
      "[rank1]:     super().__init__(open(name, mode))\n",
      "[rank1]: FileNotFoundError: [Errno 2] No such file or directory: '/home/renaldy_fredyan/PhDResearch/ELS/checkpoints/efficient24_freeze_gd.pt'\n",
      "[rank0]:[W325 10:25:26.320635529 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "W0325 10:25:28.202000 3701620 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3701623 closing signal SIGTERM\n",
      "E0325 10:25:28.317000 3701620 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 3701622) of binary: /opt/miniconda/envs/Rey2/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda/envs/Rey2/bin/torchrun\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('torch==2.5.1', 'console_scripts', 'torchrun')())\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "module12/evaluate.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-03-25_10:25:28\n",
      "  host      : viplab-G481-H81-00\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 3701622)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc_per_node=2 module12/evaluate.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--model_name=efficient24_freeze_gd --epochs=200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
