{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca5ac1f-d3b6-47a8-b454-9b824cd916b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING BACKBONE\n",
      "==================================================\n",
      "Using device: cuda\n",
      "Backbone structure:\n",
      "  patch_embed.proj: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  patch_embed.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.downsample: Identity()\n",
      "  layers_0.blocks.0.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.0.drop_path1: Identity()\n",
      "  layers_0.blocks.0.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.mlp.norm: Identity()\n",
      "  layers_0.blocks.0.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.drop_path2: Identity()\n",
      "  layers_0.blocks.1.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.1.drop_path1: DropPath(drop_prob=0.004)\n",
      "  layers_0.blocks.1.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.mlp.norm: Identity()\n",
      "  layers_0.blocks.1.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.drop_path2: DropPath(drop_prob=0.004)\n",
      "  layers_1.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  layers_1.blocks.0.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.0.drop_path1: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.0.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.mlp.norm: Identity()\n",
      "  layers_1.blocks.0.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.drop_path2: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.1.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.1.drop_path1: DropPath(drop_prob=0.013)\n",
      "  layers_1.blocks.1.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.mlp.norm: Identity()\n",
      "  layers_1.blocks.1.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.drop_path2: DropPath(drop_prob=0.013)\n",
      "  layers_2.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  layers_2.blocks.0.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.0.drop_path1: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.0.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.mlp.norm: Identity()\n",
      "  layers_2.blocks.0.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.drop_path2: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.1.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.1.drop_path1: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.1.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.mlp.norm: Identity()\n",
      "  layers_2.blocks.1.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.drop_path2: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.2.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.2.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.2.drop_path1: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.2.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.2.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.2.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.mlp.norm: Identity()\n",
      "  layers_2.blocks.2.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.drop_path2: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.3.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.3.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.3.drop_path1: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.3.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.3.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.3.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.mlp.norm: Identity()\n",
      "  layers_2.blocks.3.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.drop_path2: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.4.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.4.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.4.drop_path1: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.4.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.4.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.4.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.mlp.norm: Identity()\n",
      "  layers_2.blocks.4.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.drop_path2: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.5.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.5.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.5.drop_path1: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.5.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.5.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.5.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.mlp.norm: Identity()\n",
      "  layers_2.blocks.5.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.drop_path2: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.6.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.6.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.6.drop_path1: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.6.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.6.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.6.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.mlp.norm: Identity()\n",
      "  layers_2.blocks.6.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.drop_path2: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.7.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.7.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.7.drop_path1: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.7.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.7.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.7.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.mlp.norm: Identity()\n",
      "  layers_2.blocks.7.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.drop_path2: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.8.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.8.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.8.drop_path1: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.8.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.8.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.8.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.mlp.norm: Identity()\n",
      "  layers_2.blocks.8.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.drop_path2: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.9.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.9.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.9.drop_path1: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.9.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.9.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.9.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.mlp.norm: Identity()\n",
      "  layers_2.blocks.9.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.drop_path2: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.10.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.10.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.10.drop_path1: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.10.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.10.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.10.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.mlp.norm: Identity()\n",
      "  layers_2.blocks.10.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.drop_path2: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.11.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.11.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.11.drop_path1: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.11.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.11.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.11.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.mlp.norm: Identity()\n",
      "  layers_2.blocks.11.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.drop_path2: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.12.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.12.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.12.drop_path1: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.12.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.12.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.12.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.mlp.norm: Identity()\n",
      "  layers_2.blocks.12.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.drop_path2: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.13.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.13.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.13.drop_path1: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.13.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.13.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.13.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.mlp.norm: Identity()\n",
      "  layers_2.blocks.13.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.drop_path2: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.14.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.14.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.14.drop_path1: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.14.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.14.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.14.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.mlp.norm: Identity()\n",
      "  layers_2.blocks.14.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.drop_path2: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.15.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.15.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.15.drop_path1: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.15.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.15.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.15.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.mlp.norm: Identity()\n",
      "  layers_2.blocks.15.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.drop_path2: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.16.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.16.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.16.drop_path1: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.16.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.16.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.16.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.mlp.norm: Identity()\n",
      "  layers_2.blocks.16.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.drop_path2: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.17.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.17.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.17.drop_path1: DropPath(drop_prob=0.091)\n",
      "  layers_2.blocks.17.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.17.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.17.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.mlp.norm: Identity()\n",
      "  layers_2.blocks.17.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.drop_path2: DropPath(drop_prob=0.091)\n",
      "  layers_3.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  layers_3.blocks.0.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.0.drop_path1: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.0.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.mlp.norm: Identity()\n",
      "  layers_3.blocks.0.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.drop_path2: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.1.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.1.drop_path1: DropPath(drop_prob=0.100)\n",
      "  layers_3.blocks.1.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.mlp.norm: Identity()\n",
      "  layers_3.blocks.1.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.drop_path2: DropPath(drop_prob=0.100)\n",
      "Creating new parameter mapping from GroundingDINO...\n",
      "config.json: 100%|█████████████████████████| 1.74k/1.74k [00:00<00:00, 2.79MB/s]\n",
      "model.safetensors: 100%|██████████████████████| 933M/933M [00:08<00:00, 111MB/s]\n",
      "/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "\n",
      "GroundingDINO Backbone Structure:\n",
      "  embeddings.patch_embeddings.projection: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  embeddings.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  embeddings.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.layernorm_before: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.drop_path: Identity()\n",
      "  encoder.layers.0.blocks.0.layernorm_after: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.0.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  encoder.layers.0.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.0.blocks.0.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.layernorm_before: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.drop_path: SwinDropPath(p=0.004347826354205608)\n",
      "  encoder.layers.0.blocks.1.layernorm_after: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.1.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  encoder.layers.0.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.0.blocks.1.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  encoder.layers.0.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.layernorm_before: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.query: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.key: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.value: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.0.attention.output.dense: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.0.drop_path: SwinDropPath(p=0.008695652708411217)\n",
      "  encoder.layers.1.blocks.0.layernorm_after: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.intermediate.dense: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  encoder.layers.1.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.1.blocks.0.output.dense: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.layernorm_before: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.query: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.key: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.value: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.attention.output.dense: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.drop_path: SwinDropPath(p=0.013043479062616825)\n",
      "  encoder.layers.1.blocks.1.layernorm_after: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.1.intermediate.dense: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  encoder.layers.1.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.1.blocks.1.output.dense: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  encoder.layers.1.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.0.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.0.drop_path: SwinDropPath(p=0.017391305416822433)\n",
      "  encoder.layers.2.blocks.0.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.0.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.drop_path: SwinDropPath(p=0.021739132702350616)\n",
      "  encoder.layers.2.blocks.1.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.1.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.1.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.drop_path: SwinDropPath(p=0.02608695812523365)\n",
      "  encoder.layers.2.blocks.2.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.2.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.2.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.2.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.drop_path: SwinDropPath(p=0.030434783548116684)\n",
      "  encoder.layers.2.blocks.3.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.3.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.3.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.3.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.drop_path: SwinDropPath(p=0.03478261083364487)\n",
      "  encoder.layers.2.blocks.4.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.4.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.4.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.4.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.drop_path: SwinDropPath(p=0.03913043811917305)\n",
      "  encoder.layers.2.blocks.5.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.5.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.5.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.5.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.drop_path: SwinDropPath(p=0.04347826540470123)\n",
      "  encoder.layers.2.blocks.6.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.6.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.6.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.6.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.drop_path: SwinDropPath(p=0.04782608896493912)\n",
      "  encoder.layers.2.blocks.7.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.7.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.7.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.7.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.drop_path: SwinDropPath(p=0.052173912525177)\n",
      "  encoder.layers.2.blocks.8.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.8.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.8.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.8.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.drop_path: SwinDropPath(p=0.056521736085414886)\n",
      "  encoder.layers.2.blocks.9.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.9.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.9.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.9.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.drop_path: SwinDropPath(p=0.06086956337094307)\n",
      "  encoder.layers.2.blocks.10.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.10.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.10.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.10.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.drop_path: SwinDropPath(p=0.06521739065647125)\n",
      "  encoder.layers.2.blocks.11.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.11.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.11.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.11.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.drop_path: SwinDropPath(p=0.06956521421670914)\n",
      "  encoder.layers.2.blocks.12.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.12.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.12.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.12.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.drop_path: SwinDropPath(p=0.07391304522752762)\n",
      "  encoder.layers.2.blocks.13.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.13.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.13.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.13.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.drop_path: SwinDropPath(p=0.0782608687877655)\n",
      "  encoder.layers.2.blocks.14.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.14.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.14.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.14.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.drop_path: SwinDropPath(p=0.08260869979858398)\n",
      "  encoder.layers.2.blocks.15.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.15.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.15.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.15.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.drop_path: SwinDropPath(p=0.08695652335882187)\n",
      "  encoder.layers.2.blocks.16.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.16.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.16.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.16.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.drop_path: SwinDropPath(p=0.09130434691905975)\n",
      "  encoder.layers.2.blocks.17.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.17.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.17.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.17.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  encoder.layers.2.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.layernorm_before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.query: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.key: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.value: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.0.attention.output.dense: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.0.drop_path: SwinDropPath(p=0.09565217792987823)\n",
      "  encoder.layers.3.blocks.0.layernorm_after: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.intermediate.dense: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  encoder.layers.3.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.3.blocks.0.output.dense: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.layernorm_before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.query: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.key: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.value: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.attention.output.dense: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.drop_path: SwinDropPath(p=0.10000000149011612)\n",
      "  encoder.layers.3.blocks.1.layernorm_after: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.1.intermediate.dense: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  encoder.layers.3.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.3.blocks.1.output.dense: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  hidden_states_norms.stage2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  hidden_states_norms.stage3: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  hidden_states_norms.stage4: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "Parameter stats sebelum mapping:\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "\n",
      "Sample TIMM keys:\n",
      "  patch_embed.proj.weight: torch.Size([128, 3, 4, 4])\n",
      "  patch_embed.proj.bias: torch.Size([128])\n",
      "  patch_embed.norm.weight: torch.Size([128])\n",
      "  patch_embed.norm.bias: torch.Size([128])\n",
      "  layers_0.blocks.0.norm1.weight: torch.Size([128])\n",
      "  layers_0.blocks.0.norm1.bias: torch.Size([128])\n",
      "  layers_0.blocks.0.attn.relative_position_bias_table: torch.Size([169, 4])\n",
      "  layers_0.blocks.0.attn.qkv.weight: torch.Size([384, 128])\n",
      "  layers_0.blocks.0.attn.qkv.bias: torch.Size([384])\n",
      "  layers_0.blocks.0.attn.proj.weight: torch.Size([128, 128])\n",
      "\n",
      "Sample GroundingDINO keys:\n",
      "  embeddings.patch_embeddings.projection.weight: torch.Size([128, 3, 4, 4])\n",
      "  embeddings.patch_embeddings.projection.bias: torch.Size([128])\n",
      "  embeddings.norm.weight: torch.Size([128])\n",
      "  embeddings.norm.bias: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.layernorm_before.weight: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.layernorm_before.bias: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.attention.self.relative_position_bias_table: torch.Size([529, 4])\n",
      "  encoder.layers.0.blocks.0.attention.self.relative_position_index: torch.Size([144, 144])\n",
      "  encoder.layers.0.blocks.0.attention.self.query.weight: torch.Size([128, 128])\n",
      "  encoder.layers.0.blocks.0.attention.self.query.bias: torch.Size([128])\n",
      "Mapped 4/325 parameters successfully\n",
      "QKV parameters: 0/15 potential matches found\n",
      "Saved mapped parameters to ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "\n",
      "Parameter stats setelah mapping:\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "Backbone parameters frozen (requires_grad=False)\n",
      "Backbone in EVAL mode\n",
      "Input tensor shape: torch.Size([2, 3, 512, 512])\n",
      "\n",
      "Testing forward_multiscale...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -5.0436, max: 4.9420, mean: 0.0010, std: 1.0011\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4240, max: 2.3352, mean: -0.0044, std: 0.4798\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -5.6362, max: 5.6722, mean: 0.0345, std: 1.2204\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -6.0518, max: 5.0640, mean: 0.0133, std: 1.1655\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "\n",
      "Testing forward_concatenated...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -5.0436, max: 4.9420, mean: 0.0010, std: 1.0011\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4240, max: 2.3352, mean: -0.0044, std: 0.4798\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -5.6362, max: 5.6722, mean: 0.0345, std: 1.2204\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -6.0518, max: 5.0640, mean: 0.0133, std: 1.1655\n",
      "Permuting feature dimensions from BHWC to BCHW\n",
      "Target size after reduction: (64, 64)\n",
      "Concatenated feature shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "\n",
      "==================================================\n",
      "BACKBONE TEST COMPLETE\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "TESTING BACKBONE\n",
      "==================================================\n",
      "Using device: cuda\n",
      "Backbone structure:\n",
      "  patch_embed.proj: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  patch_embed.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.downsample: Identity()\n",
      "  layers_0.blocks.0.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.0.drop_path1: Identity()\n",
      "  layers_0.blocks.0.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.mlp.norm: Identity()\n",
      "  layers_0.blocks.0.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.drop_path2: Identity()\n",
      "  layers_0.blocks.1.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.1.drop_path1: DropPath(drop_prob=0.004)\n",
      "  layers_0.blocks.1.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.mlp.norm: Identity()\n",
      "  layers_0.blocks.1.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.drop_path2: DropPath(drop_prob=0.004)\n",
      "  layers_1.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  layers_1.blocks.0.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.0.drop_path1: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.0.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.mlp.norm: Identity()\n",
      "  layers_1.blocks.0.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.drop_path2: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.1.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.1.drop_path1: DropPath(drop_prob=0.013)\n",
      "  layers_1.blocks.1.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.mlp.norm: Identity()\n",
      "  layers_1.blocks.1.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.drop_path2: DropPath(drop_prob=0.013)\n",
      "  layers_2.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  layers_2.blocks.0.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.0.drop_path1: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.0.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.mlp.norm: Identity()\n",
      "  layers_2.blocks.0.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.drop_path2: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.1.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.1.drop_path1: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.1.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.mlp.norm: Identity()\n",
      "  layers_2.blocks.1.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.drop_path2: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.2.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.2.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.2.drop_path1: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.2.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.2.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.2.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.mlp.norm: Identity()\n",
      "  layers_2.blocks.2.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.drop_path2: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.3.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.3.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.3.drop_path1: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.3.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.3.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.3.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.mlp.norm: Identity()\n",
      "  layers_2.blocks.3.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.drop_path2: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.4.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.4.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.4.drop_path1: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.4.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.4.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.4.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.mlp.norm: Identity()\n",
      "  layers_2.blocks.4.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.drop_path2: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.5.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.5.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.5.drop_path1: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.5.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.5.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.5.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.mlp.norm: Identity()\n",
      "  layers_2.blocks.5.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.drop_path2: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.6.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.6.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.6.drop_path1: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.6.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.6.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.6.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.mlp.norm: Identity()\n",
      "  layers_2.blocks.6.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.drop_path2: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.7.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.7.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.7.drop_path1: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.7.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.7.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.7.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.mlp.norm: Identity()\n",
      "  layers_2.blocks.7.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.drop_path2: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.8.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.8.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.8.drop_path1: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.8.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.8.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.8.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.mlp.norm: Identity()\n",
      "  layers_2.blocks.8.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.drop_path2: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.9.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.9.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.9.drop_path1: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.9.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.9.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.9.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.mlp.norm: Identity()\n",
      "  layers_2.blocks.9.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.drop_path2: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.10.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.10.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.10.drop_path1: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.10.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.10.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.10.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.mlp.norm: Identity()\n",
      "  layers_2.blocks.10.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.drop_path2: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.11.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.11.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.11.drop_path1: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.11.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.11.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.11.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.mlp.norm: Identity()\n",
      "  layers_2.blocks.11.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.drop_path2: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.12.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.12.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.12.drop_path1: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.12.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.12.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.12.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.mlp.norm: Identity()\n",
      "  layers_2.blocks.12.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.drop_path2: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.13.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.13.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.13.drop_path1: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.13.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.13.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.13.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.mlp.norm: Identity()\n",
      "  layers_2.blocks.13.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.drop_path2: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.14.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.14.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.14.drop_path1: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.14.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.14.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.14.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.mlp.norm: Identity()\n",
      "  layers_2.blocks.14.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.drop_path2: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.15.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.15.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.15.drop_path1: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.15.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.15.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.15.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.mlp.norm: Identity()\n",
      "  layers_2.blocks.15.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.drop_path2: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.16.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.16.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.16.drop_path1: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.16.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.16.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.16.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.mlp.norm: Identity()\n",
      "  layers_2.blocks.16.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.drop_path2: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.17.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.17.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.17.drop_path1: DropPath(drop_prob=0.091)\n",
      "  layers_2.blocks.17.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.17.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.17.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.mlp.norm: Identity()\n",
      "  layers_2.blocks.17.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.drop_path2: DropPath(drop_prob=0.091)\n",
      "  layers_3.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  layers_3.blocks.0.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.0.drop_path1: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.0.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.mlp.norm: Identity()\n",
      "  layers_3.blocks.0.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.drop_path2: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.1.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.1.drop_path1: DropPath(drop_prob=0.100)\n",
      "  layers_3.blocks.1.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.mlp.norm: Identity()\n",
      "  layers_3.blocks.1.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.drop_path2: DropPath(drop_prob=0.100)\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "Backbone parameters set to trainable (requires_grad=True)\n",
      "Backbone in TRAIN mode\n",
      "Input tensor shape: torch.Size([2, 3, 512, 512])\n",
      "\n",
      "Testing forward_multiscale...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -4.7221, max: 5.1338, mean: -0.0016, std: 1.0001\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4599, max: 2.5013, mean: -0.0051, std: 0.4809\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -6.0789, max: 5.6200, mean: 0.0361, std: 1.2340\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -6.2115, max: 5.5309, mean: 0.0098, std: 1.0917\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "\n",
      "Testing forward_concatenated...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -4.7221, max: 5.1338, mean: -0.0016, std: 1.0001\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4599, max: 2.5013, mean: -0.0051, std: 0.4809\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -6.8098, max: 5.6830, mean: 0.0376, std: 1.2516\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -7.9927, max: 6.2326, mean: 0.0210, std: 1.1489\n",
      "Permuting feature dimensions from BHWC to BCHW\n",
      "Target size after reduction: (64, 64)\n",
      "Concatenated feature shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "\n",
      "Testing backward pass...\n",
      "Dummy loss: 0.021941233426332474\n",
      "Top 5 gradients by norm:\n",
      "  backbone.layers_3.downsample.reduction.weight: 0.426721\n",
      "  backbone.layers_2.downsample.reduction.weight: 0.291985\n",
      "  backbone.layers_1.downsample.reduction.weight: 0.251706\n",
      "  backbone.layers_3.blocks.1.mlp.fc2.weight: 0.251678\n",
      "  backbone.layers_2.blocks.0.mlp.fc2.weight: 0.193036\n",
      "Optimizer step completed\n",
      "\n",
      "==================================================\n",
      "BACKBONE TEST COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!python module11/debug_backbone.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2807378f-4ac3-41f5-a7ef-fe939c251bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n"
     ]
    }
   ],
   "source": [
    "!python module11/backbone.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4a8c6-c85e-47c3-9dfc-be6de51e3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python module11/test_debuge.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581ae1d-0472-400e-8783-e7583a955ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765387d9-4965-4694-9f07-34b6b582298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc_per_node=2 module11/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient17_freeze_gd --epochs=500 --pre_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
