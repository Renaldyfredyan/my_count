{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca5ac1f-d3b6-47a8-b454-9b824cd916b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING BACKBONE\n",
      "==================================================\n",
      "Using device: cuda\n",
      "Backbone structure:\n",
      "  patch_embed.proj: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  patch_embed.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.downsample: Identity()\n",
      "  layers_0.blocks.0.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.0.drop_path1: Identity()\n",
      "  layers_0.blocks.0.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.mlp.norm: Identity()\n",
      "  layers_0.blocks.0.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.drop_path2: Identity()\n",
      "  layers_0.blocks.1.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.1.drop_path1: DropPath(drop_prob=0.004)\n",
      "  layers_0.blocks.1.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.mlp.norm: Identity()\n",
      "  layers_0.blocks.1.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.drop_path2: DropPath(drop_prob=0.004)\n",
      "  layers_1.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  layers_1.blocks.0.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.0.drop_path1: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.0.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.mlp.norm: Identity()\n",
      "  layers_1.blocks.0.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.drop_path2: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.1.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.1.drop_path1: DropPath(drop_prob=0.013)\n",
      "  layers_1.blocks.1.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.mlp.norm: Identity()\n",
      "  layers_1.blocks.1.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.drop_path2: DropPath(drop_prob=0.013)\n",
      "  layers_2.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  layers_2.blocks.0.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.0.drop_path1: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.0.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.mlp.norm: Identity()\n",
      "  layers_2.blocks.0.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.drop_path2: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.1.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.1.drop_path1: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.1.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.mlp.norm: Identity()\n",
      "  layers_2.blocks.1.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.drop_path2: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.2.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.2.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.2.drop_path1: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.2.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.2.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.2.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.mlp.norm: Identity()\n",
      "  layers_2.blocks.2.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.drop_path2: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.3.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.3.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.3.drop_path1: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.3.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.3.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.3.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.mlp.norm: Identity()\n",
      "  layers_2.blocks.3.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.drop_path2: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.4.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.4.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.4.drop_path1: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.4.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.4.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.4.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.mlp.norm: Identity()\n",
      "  layers_2.blocks.4.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.drop_path2: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.5.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.5.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.5.drop_path1: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.5.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.5.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.5.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.mlp.norm: Identity()\n",
      "  layers_2.blocks.5.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.drop_path2: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.6.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.6.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.6.drop_path1: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.6.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.6.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.6.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.mlp.norm: Identity()\n",
      "  layers_2.blocks.6.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.drop_path2: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.7.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.7.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.7.drop_path1: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.7.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.7.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.7.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.mlp.norm: Identity()\n",
      "  layers_2.blocks.7.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.drop_path2: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.8.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.8.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.8.drop_path1: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.8.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.8.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.8.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.mlp.norm: Identity()\n",
      "  layers_2.blocks.8.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.drop_path2: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.9.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.9.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.9.drop_path1: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.9.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.9.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.9.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.mlp.norm: Identity()\n",
      "  layers_2.blocks.9.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.drop_path2: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.10.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.10.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.10.drop_path1: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.10.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.10.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.10.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.mlp.norm: Identity()\n",
      "  layers_2.blocks.10.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.drop_path2: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.11.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.11.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.11.drop_path1: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.11.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.11.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.11.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.mlp.norm: Identity()\n",
      "  layers_2.blocks.11.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.drop_path2: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.12.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.12.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.12.drop_path1: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.12.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.12.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.12.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.mlp.norm: Identity()\n",
      "  layers_2.blocks.12.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.drop_path2: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.13.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.13.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.13.drop_path1: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.13.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.13.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.13.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.mlp.norm: Identity()\n",
      "  layers_2.blocks.13.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.drop_path2: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.14.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.14.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.14.drop_path1: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.14.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.14.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.14.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.mlp.norm: Identity()\n",
      "  layers_2.blocks.14.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.drop_path2: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.15.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.15.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.15.drop_path1: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.15.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.15.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.15.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.mlp.norm: Identity()\n",
      "  layers_2.blocks.15.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.drop_path2: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.16.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.16.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.16.drop_path1: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.16.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.16.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.16.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.mlp.norm: Identity()\n",
      "  layers_2.blocks.16.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.drop_path2: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.17.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.17.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.17.drop_path1: DropPath(drop_prob=0.091)\n",
      "  layers_2.blocks.17.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.17.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.17.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.mlp.norm: Identity()\n",
      "  layers_2.blocks.17.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.drop_path2: DropPath(drop_prob=0.091)\n",
      "  layers_3.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  layers_3.blocks.0.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.0.drop_path1: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.0.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.mlp.norm: Identity()\n",
      "  layers_3.blocks.0.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.drop_path2: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.1.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.1.drop_path1: DropPath(drop_prob=0.100)\n",
      "  layers_3.blocks.1.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.mlp.norm: Identity()\n",
      "  layers_3.blocks.1.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.drop_path2: DropPath(drop_prob=0.100)\n",
      "Creating new parameter mapping from GroundingDINO...\n",
      "config.json: 100%|█████████████████████████| 1.74k/1.74k [00:00<00:00, 2.79MB/s]\n",
      "model.safetensors: 100%|██████████████████████| 933M/933M [00:08<00:00, 111MB/s]\n",
      "/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "\n",
      "GroundingDINO Backbone Structure:\n",
      "  embeddings.patch_embeddings.projection: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  embeddings.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  embeddings.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.layernorm_before: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.drop_path: Identity()\n",
      "  encoder.layers.0.blocks.0.layernorm_after: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.0.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  encoder.layers.0.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.0.blocks.0.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.layernorm_before: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.drop_path: SwinDropPath(p=0.004347826354205608)\n",
      "  encoder.layers.0.blocks.1.layernorm_after: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.1.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  encoder.layers.0.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.0.blocks.1.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  encoder.layers.0.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.layernorm_before: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.query: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.key: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.value: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.0.attention.output.dense: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.0.drop_path: SwinDropPath(p=0.008695652708411217)\n",
      "  encoder.layers.1.blocks.0.layernorm_after: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.intermediate.dense: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  encoder.layers.1.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.1.blocks.0.output.dense: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.layernorm_before: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.query: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.key: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.value: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.attention.output.dense: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.drop_path: SwinDropPath(p=0.013043479062616825)\n",
      "  encoder.layers.1.blocks.1.layernorm_after: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.1.intermediate.dense: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  encoder.layers.1.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.1.blocks.1.output.dense: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  encoder.layers.1.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.0.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.0.drop_path: SwinDropPath(p=0.017391305416822433)\n",
      "  encoder.layers.2.blocks.0.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.0.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.drop_path: SwinDropPath(p=0.021739132702350616)\n",
      "  encoder.layers.2.blocks.1.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.1.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.1.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.drop_path: SwinDropPath(p=0.02608695812523365)\n",
      "  encoder.layers.2.blocks.2.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.2.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.2.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.2.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.drop_path: SwinDropPath(p=0.030434783548116684)\n",
      "  encoder.layers.2.blocks.3.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.3.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.3.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.3.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.drop_path: SwinDropPath(p=0.03478261083364487)\n",
      "  encoder.layers.2.blocks.4.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.4.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.4.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.4.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.drop_path: SwinDropPath(p=0.03913043811917305)\n",
      "  encoder.layers.2.blocks.5.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.5.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.5.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.5.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.drop_path: SwinDropPath(p=0.04347826540470123)\n",
      "  encoder.layers.2.blocks.6.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.6.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.6.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.6.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.drop_path: SwinDropPath(p=0.04782608896493912)\n",
      "  encoder.layers.2.blocks.7.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.7.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.7.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.7.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.drop_path: SwinDropPath(p=0.052173912525177)\n",
      "  encoder.layers.2.blocks.8.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.8.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.8.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.8.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.drop_path: SwinDropPath(p=0.056521736085414886)\n",
      "  encoder.layers.2.blocks.9.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.9.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.9.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.9.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.drop_path: SwinDropPath(p=0.06086956337094307)\n",
      "  encoder.layers.2.blocks.10.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.10.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.10.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.10.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.drop_path: SwinDropPath(p=0.06521739065647125)\n",
      "  encoder.layers.2.blocks.11.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.11.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.11.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.11.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.drop_path: SwinDropPath(p=0.06956521421670914)\n",
      "  encoder.layers.2.blocks.12.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.12.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.12.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.12.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.drop_path: SwinDropPath(p=0.07391304522752762)\n",
      "  encoder.layers.2.blocks.13.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.13.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.13.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.13.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.drop_path: SwinDropPath(p=0.0782608687877655)\n",
      "  encoder.layers.2.blocks.14.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.14.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.14.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.14.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.drop_path: SwinDropPath(p=0.08260869979858398)\n",
      "  encoder.layers.2.blocks.15.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.15.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.15.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.15.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.drop_path: SwinDropPath(p=0.08695652335882187)\n",
      "  encoder.layers.2.blocks.16.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.16.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.16.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.16.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.drop_path: SwinDropPath(p=0.09130434691905975)\n",
      "  encoder.layers.2.blocks.17.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.17.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.17.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.17.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  encoder.layers.2.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.layernorm_before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.query: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.key: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.value: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.0.attention.output.dense: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.0.drop_path: SwinDropPath(p=0.09565217792987823)\n",
      "  encoder.layers.3.blocks.0.layernorm_after: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.intermediate.dense: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  encoder.layers.3.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.3.blocks.0.output.dense: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.layernorm_before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.query: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.key: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.value: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.attention.output.dense: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.drop_path: SwinDropPath(p=0.10000000149011612)\n",
      "  encoder.layers.3.blocks.1.layernorm_after: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.1.intermediate.dense: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  encoder.layers.3.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.3.blocks.1.output.dense: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  hidden_states_norms.stage2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  hidden_states_norms.stage3: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  hidden_states_norms.stage4: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "Parameter stats sebelum mapping:\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "\n",
      "Sample TIMM keys:\n",
      "  patch_embed.proj.weight: torch.Size([128, 3, 4, 4])\n",
      "  patch_embed.proj.bias: torch.Size([128])\n",
      "  patch_embed.norm.weight: torch.Size([128])\n",
      "  patch_embed.norm.bias: torch.Size([128])\n",
      "  layers_0.blocks.0.norm1.weight: torch.Size([128])\n",
      "  layers_0.blocks.0.norm1.bias: torch.Size([128])\n",
      "  layers_0.blocks.0.attn.relative_position_bias_table: torch.Size([169, 4])\n",
      "  layers_0.blocks.0.attn.qkv.weight: torch.Size([384, 128])\n",
      "  layers_0.blocks.0.attn.qkv.bias: torch.Size([384])\n",
      "  layers_0.blocks.0.attn.proj.weight: torch.Size([128, 128])\n",
      "\n",
      "Sample GroundingDINO keys:\n",
      "  embeddings.patch_embeddings.projection.weight: torch.Size([128, 3, 4, 4])\n",
      "  embeddings.patch_embeddings.projection.bias: torch.Size([128])\n",
      "  embeddings.norm.weight: torch.Size([128])\n",
      "  embeddings.norm.bias: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.layernorm_before.weight: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.layernorm_before.bias: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.attention.self.relative_position_bias_table: torch.Size([529, 4])\n",
      "  encoder.layers.0.blocks.0.attention.self.relative_position_index: torch.Size([144, 144])\n",
      "  encoder.layers.0.blocks.0.attention.self.query.weight: torch.Size([128, 128])\n",
      "  encoder.layers.0.blocks.0.attention.self.query.bias: torch.Size([128])\n",
      "Mapped 4/325 parameters successfully\n",
      "QKV parameters: 0/15 potential matches found\n",
      "Saved mapped parameters to ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "\n",
      "Parameter stats setelah mapping:\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "Backbone parameters frozen (requires_grad=False)\n",
      "Backbone in EVAL mode\n",
      "Input tensor shape: torch.Size([2, 3, 512, 512])\n",
      "\n",
      "Testing forward_multiscale...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -5.0436, max: 4.9420, mean: 0.0010, std: 1.0011\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4240, max: 2.3352, mean: -0.0044, std: 0.4798\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -5.6362, max: 5.6722, mean: 0.0345, std: 1.2204\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -6.0518, max: 5.0640, mean: 0.0133, std: 1.1655\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "\n",
      "Testing forward_concatenated...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -5.0436, max: 4.9420, mean: 0.0010, std: 1.0011\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4240, max: 2.3352, mean: -0.0044, std: 0.4798\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -5.6362, max: 5.6722, mean: 0.0345, std: 1.2204\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -6.0518, max: 5.0640, mean: 0.0133, std: 1.1655\n",
      "Permuting feature dimensions from BHWC to BCHW\n",
      "Target size after reduction: (64, 64)\n",
      "Concatenated feature shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "\n",
      "==================================================\n",
      "BACKBONE TEST COMPLETE\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "TESTING BACKBONE\n",
      "==================================================\n",
      "Using device: cuda\n",
      "Backbone structure:\n",
      "  patch_embed.proj: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  patch_embed.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.downsample: Identity()\n",
      "  layers_0.blocks.0.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.0.drop_path1: Identity()\n",
      "  layers_0.blocks.0.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.mlp.norm: Identity()\n",
      "  layers_0.blocks.0.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.drop_path2: Identity()\n",
      "  layers_0.blocks.1.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.1.drop_path1: DropPath(drop_prob=0.004)\n",
      "  layers_0.blocks.1.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.mlp.norm: Identity()\n",
      "  layers_0.blocks.1.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.drop_path2: DropPath(drop_prob=0.004)\n",
      "  layers_1.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  layers_1.blocks.0.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.0.drop_path1: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.0.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.mlp.norm: Identity()\n",
      "  layers_1.blocks.0.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.drop_path2: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.1.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.1.drop_path1: DropPath(drop_prob=0.013)\n",
      "  layers_1.blocks.1.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.mlp.norm: Identity()\n",
      "  layers_1.blocks.1.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.drop_path2: DropPath(drop_prob=0.013)\n",
      "  layers_2.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  layers_2.blocks.0.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.0.drop_path1: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.0.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.mlp.norm: Identity()\n",
      "  layers_2.blocks.0.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.drop_path2: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.1.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.1.drop_path1: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.1.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.mlp.norm: Identity()\n",
      "  layers_2.blocks.1.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.drop_path2: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.2.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.2.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.2.drop_path1: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.2.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.2.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.2.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.mlp.norm: Identity()\n",
      "  layers_2.blocks.2.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.drop_path2: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.3.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.3.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.3.drop_path1: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.3.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.3.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.3.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.mlp.norm: Identity()\n",
      "  layers_2.blocks.3.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.drop_path2: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.4.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.4.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.4.drop_path1: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.4.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.4.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.4.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.mlp.norm: Identity()\n",
      "  layers_2.blocks.4.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.drop_path2: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.5.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.5.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.5.drop_path1: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.5.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.5.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.5.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.mlp.norm: Identity()\n",
      "  layers_2.blocks.5.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.drop_path2: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.6.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.6.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.6.drop_path1: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.6.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.6.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.6.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.mlp.norm: Identity()\n",
      "  layers_2.blocks.6.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.drop_path2: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.7.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.7.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.7.drop_path1: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.7.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.7.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.7.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.mlp.norm: Identity()\n",
      "  layers_2.blocks.7.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.drop_path2: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.8.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.8.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.8.drop_path1: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.8.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.8.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.8.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.mlp.norm: Identity()\n",
      "  layers_2.blocks.8.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.drop_path2: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.9.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.9.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.9.drop_path1: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.9.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.9.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.9.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.mlp.norm: Identity()\n",
      "  layers_2.blocks.9.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.drop_path2: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.10.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.10.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.10.drop_path1: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.10.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.10.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.10.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.mlp.norm: Identity()\n",
      "  layers_2.blocks.10.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.drop_path2: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.11.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.11.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.11.drop_path1: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.11.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.11.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.11.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.mlp.norm: Identity()\n",
      "  layers_2.blocks.11.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.drop_path2: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.12.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.12.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.12.drop_path1: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.12.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.12.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.12.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.mlp.norm: Identity()\n",
      "  layers_2.blocks.12.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.drop_path2: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.13.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.13.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.13.drop_path1: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.13.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.13.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.13.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.mlp.norm: Identity()\n",
      "  layers_2.blocks.13.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.drop_path2: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.14.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.14.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.14.drop_path1: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.14.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.14.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.14.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.mlp.norm: Identity()\n",
      "  layers_2.blocks.14.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.drop_path2: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.15.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.15.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.15.drop_path1: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.15.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.15.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.15.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.mlp.norm: Identity()\n",
      "  layers_2.blocks.15.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.drop_path2: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.16.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.16.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.16.drop_path1: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.16.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.16.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.16.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.mlp.norm: Identity()\n",
      "  layers_2.blocks.16.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.drop_path2: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.17.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.17.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.17.drop_path1: DropPath(drop_prob=0.091)\n",
      "  layers_2.blocks.17.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.17.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.17.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.mlp.norm: Identity()\n",
      "  layers_2.blocks.17.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.drop_path2: DropPath(drop_prob=0.091)\n",
      "  layers_3.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  layers_3.blocks.0.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.0.drop_path1: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.0.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.mlp.norm: Identity()\n",
      "  layers_3.blocks.0.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.drop_path2: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.1.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.1.drop_path1: DropPath(drop_prob=0.100)\n",
      "  layers_3.blocks.1.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.mlp.norm: Identity()\n",
      "  layers_3.blocks.1.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.drop_path2: DropPath(drop_prob=0.100)\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "Backbone parameters set to trainable (requires_grad=True)\n",
      "Backbone in TRAIN mode\n",
      "Input tensor shape: torch.Size([2, 3, 512, 512])\n",
      "\n",
      "Testing forward_multiscale...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -4.7221, max: 5.1338, mean: -0.0016, std: 1.0001\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4599, max: 2.5013, mean: -0.0051, std: 0.4809\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -6.0789, max: 5.6200, mean: 0.0361, std: 1.2340\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -6.2115, max: 5.5309, mean: 0.0098, std: 1.0917\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "\n",
      "Testing forward_concatenated...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -4.7221, max: 5.1338, mean: -0.0016, std: 1.0001\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -2.4599, max: 2.5013, mean: -0.0051, std: 0.4809\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -6.8098, max: 5.6830, mean: 0.0376, std: 1.2516\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -7.9927, max: 6.2326, mean: 0.0210, std: 1.1489\n",
      "Permuting feature dimensions from BHWC to BCHW\n",
      "Target size after reduction: (64, 64)\n",
      "Concatenated feature shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 3584, Actual: 1792\n",
      "\n",
      "Testing backward pass...\n",
      "Dummy loss: 0.021941233426332474\n",
      "Top 5 gradients by norm:\n",
      "  backbone.layers_3.downsample.reduction.weight: 0.426721\n",
      "  backbone.layers_2.downsample.reduction.weight: 0.291985\n",
      "  backbone.layers_1.downsample.reduction.weight: 0.251706\n",
      "  backbone.layers_3.blocks.1.mlp.fc2.weight: 0.251678\n",
      "  backbone.layers_2.blocks.0.mlp.fc2.weight: 0.193036\n",
      "Optimizer step completed\n",
      "\n",
      "==================================================\n",
      "BACKBONE TEST COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!python module11/debug_backbone.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2807378f-4ac3-41f5-a7ef-fe939c251bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n"
     ]
    }
   ],
   "source": [
    "!python module11/backbone.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4a8c6-c85e-47c3-9dfc-be6de51e3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python module11/test_debuge.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581ae1d-0472-400e-8783-e7583a955ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765387d9-4965-4694-9f07-34b6b582298a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0319 08:45:02.714000 3931795 site-packages/torch/distributed/run.py:793] \n",
      "W0319 08:45:02.714000 3931795 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0319 08:45:02.714000 3931795 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0319 08:45:02.714000 3931795 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "1\n",
      "0\n",
      "Epoch: 1 Train loss: 24.334 Aux train loss: 11.608 Val loss: 6.931 Aux val loss: 4.191 Train MAE: 106.167 Val MAE: 59.544 Epoch time: 196.604 seconds best\n",
      "Epoch: 2 Train loss: 17.531 Aux train loss: 10.581 Val loss: 6.830 Aux val loss: 4.096 Train MAE: 67.637 Val MAE: 47.228 Epoch time: 198.214 seconds best\n",
      "Epoch: 3 Train loss: 16.988 Aux train loss: 10.208 Val loss: 7.071 Aux val loss: 4.210 Train MAE: 66.847 Val MAE: 63.252 Epoch time: 200.054 seconds \n",
      "Epoch: 4 Train loss: 17.422 Aux train loss: 10.450 Val loss: 6.706 Aux val loss: 4.027 Train MAE: 62.983 Val MAE: 47.326 Epoch time: 200.964 seconds \n",
      "Epoch: 5 Train loss: 17.647 Aux train loss: 10.549 Val loss: 6.803 Aux val loss: 4.089 Train MAE: 63.155 Val MAE: 53.198 Epoch time: 200.626 seconds \n",
      "Epoch: 6 Train loss: 17.098 Aux train loss: 10.282 Val loss: 6.698 Aux val loss: 4.028 Train MAE: 59.646 Val MAE: 47.485 Epoch time: 200.662 seconds \n",
      "Epoch: 7 Train loss: 16.880 Aux train loss: 10.052 Val loss: 6.697 Aux val loss: 4.018 Train MAE: 62.652 Val MAE: 46.162 Epoch time: 199.800 seconds best\n",
      "Epoch: 8 Train loss: 16.398 Aux train loss: 9.793 Val loss: 6.744 Aux val loss: 3.968 Train MAE: 56.856 Val MAE: 48.598 Epoch time: 200.856 seconds \n",
      "Epoch: 9 Train loss: 16.967 Aux train loss: 10.116 Val loss: 6.691 Aux val loss: 4.024 Train MAE: 58.786 Val MAE: 59.173 Epoch time: 200.647 seconds \n",
      "Epoch: 10 Train loss: 16.643 Aux train loss: 9.934 Val loss: 6.573 Aux val loss: 3.944 Train MAE: 54.683 Val MAE: 47.166 Epoch time: 200.627 seconds \n",
      "Epoch: 11 Train loss: 16.270 Aux train loss: 9.738 Val loss: 6.570 Aux val loss: 3.912 Train MAE: 54.801 Val MAE: 41.713 Epoch time: 201.204 seconds best\n",
      "Epoch: 12 Train loss: 16.682 Aux train loss: 9.990 Val loss: 6.424 Aux val loss: 3.837 Train MAE: 54.337 Val MAE: 44.248 Epoch time: 200.590 seconds \n",
      "Epoch: 13 Train loss: 15.825 Aux train loss: 9.483 Val loss: 6.387 Aux val loss: 3.843 Train MAE: 50.311 Val MAE: 35.571 Epoch time: 201.078 seconds best\n",
      "Epoch: 14 Train loss: 16.557 Aux train loss: 9.970 Val loss: 6.441 Aux val loss: 3.866 Train MAE: 50.874 Val MAE: 44.334 Epoch time: 200.718 seconds \n",
      "Epoch: 15 Train loss: 15.426 Aux train loss: 9.246 Val loss: 6.380 Aux val loss: 3.855 Train MAE: 48.942 Val MAE: 36.915 Epoch time: 201.033 seconds \n",
      "Epoch: 16 Train loss: 15.835 Aux train loss: 9.489 Val loss: 6.343 Aux val loss: 3.786 Train MAE: 49.715 Val MAE: 36.931 Epoch time: 200.573 seconds \n",
      "Epoch: 17 Train loss: 15.992 Aux train loss: 9.589 Val loss: 6.643 Aux val loss: 3.915 Train MAE: 50.933 Val MAE: 63.641 Epoch time: 201.127 seconds \n",
      "Epoch: 18 Train loss: 16.290 Aux train loss: 9.749 Val loss: 6.253 Aux val loss: 3.763 Train MAE: 47.304 Val MAE: 35.875 Epoch time: 201.066 seconds \n",
      "Epoch: 19 Train loss: 14.808 Aux train loss: 8.880 Val loss: 6.261 Aux val loss: 3.810 Train MAE: 45.392 Val MAE: 45.408 Epoch time: 200.526 seconds \n",
      "Epoch: 20 Train loss: 16.104 Aux train loss: 9.723 Val loss: 6.240 Aux val loss: 3.731 Train MAE: 47.862 Val MAE: 36.427 Epoch time: 200.333 seconds \n",
      "Epoch: 21 Train loss: 15.200 Aux train loss: 9.114 Val loss: 6.226 Aux val loss: 3.712 Train MAE: 46.154 Val MAE: 35.439 Epoch time: 200.309 seconds best\n",
      "Epoch: 22 Train loss: 14.801 Aux train loss: 8.872 Val loss: 6.336 Aux val loss: 3.708 Train MAE: 42.813 Val MAE: 50.749 Epoch time: 199.602 seconds \n",
      "Epoch: 23 Train loss: 15.417 Aux train loss: 9.278 Val loss: 6.720 Aux val loss: 4.071 Train MAE: 48.123 Val MAE: 66.030 Epoch time: 200.144 seconds \n",
      "Epoch: 24 Train loss: 15.389 Aux train loss: 9.246 Val loss: 6.167 Aux val loss: 3.716 Train MAE: 42.072 Val MAE: 33.036 Epoch time: 198.748 seconds best\n",
      "Epoch: 25 Train loss: 14.847 Aux train loss: 8.919 Val loss: 6.156 Aux val loss: 3.684 Train MAE: 43.735 Val MAE: 33.825 Epoch time: 200.555 seconds \n",
      "Epoch: 26 Train loss: 14.926 Aux train loss: 8.985 Val loss: 6.469 Aux val loss: 3.920 Train MAE: 43.080 Val MAE: 55.130 Epoch time: 199.889 seconds \n",
      "Epoch: 27 Train loss: 15.633 Aux train loss: 9.633 Val loss: 6.067 Aux val loss: 3.624 Train MAE: 42.165 Val MAE: 34.037 Epoch time: 199.252 seconds \n",
      "Epoch: 28 Train loss: 14.900 Aux train loss: 8.981 Val loss: 6.172 Aux val loss: 3.703 Train MAE: 42.472 Val MAE: 37.402 Epoch time: 199.805 seconds \n",
      "Epoch: 29 Train loss: 15.002 Aux train loss: 9.036 Val loss: 6.071 Aux val loss: 3.673 Train MAE: 40.729 Val MAE: 34.128 Epoch time: 199.395 seconds \n",
      "Epoch: 30 Train loss: 13.633 Aux train loss: 8.214 Val loss: 5.999 Aux val loss: 3.600 Train MAE: 37.266 Val MAE: 30.851 Epoch time: 199.424 seconds best\n",
      "Epoch: 31 Train loss: 15.005 Aux train loss: 8.852 Val loss: 5.981 Aux val loss: 3.606 Train MAE: 42.086 Val MAE: 32.327 Epoch time: 199.171 seconds \n",
      "Epoch: 32 Train loss: 14.615 Aux train loss: 9.004 Val loss: 6.003 Aux val loss: 3.635 Train MAE: 43.087 Val MAE: 34.450 Epoch time: 198.997 seconds \n",
      "Epoch: 33 Train loss: 14.880 Aux train loss: 9.013 Val loss: 5.921 Aux val loss: 3.558 Train MAE: 41.712 Val MAE: 30.206 Epoch time: 199.534 seconds best\n",
      "Epoch: 34 Train loss: 14.049 Aux train loss: 8.491 Val loss: 5.960 Aux val loss: 3.600 Train MAE: 40.618 Val MAE: 33.383 Epoch time: 198.397 seconds \n",
      "Epoch: 35 Train loss: 15.223 Aux train loss: 9.228 Val loss: 6.017 Aux val loss: 3.609 Train MAE: 43.164 Val MAE: 34.843 Epoch time: 199.209 seconds \n",
      "Epoch: 36 Train loss: 14.532 Aux train loss: 8.781 Val loss: 5.870 Aux val loss: 3.519 Train MAE: 40.332 Val MAE: 38.802 Epoch time: 199.714 seconds \n",
      "Epoch: 37 Train loss: 14.349 Aux train loss: 8.690 Val loss: 6.016 Aux val loss: 3.668 Train MAE: 39.154 Val MAE: 52.733 Epoch time: 199.703 seconds \n",
      "Epoch: 38 Train loss: 14.702 Aux train loss: 8.832 Val loss: 5.958 Aux val loss: 3.626 Train MAE: 40.649 Val MAE: 44.623 Epoch time: 199.581 seconds \n",
      "Epoch: 39 Train loss: 14.975 Aux train loss: 9.098 Val loss: 5.791 Aux val loss: 3.511 Train MAE: 39.171 Val MAE: 31.928 Epoch time: 199.648 seconds \n",
      "Epoch: 40 Train loss: 14.195 Aux train loss: 8.609 Val loss: 5.902 Aux val loss: 3.552 Train MAE: 41.015 Val MAE: 41.811 Epoch time: 200.442 seconds \n",
      "Epoch: 41 Train loss: 14.132 Aux train loss: 8.641 Val loss: 5.762 Aux val loss: 3.503 Train MAE: 40.053 Val MAE: 31.427 Epoch time: 200.163 seconds \n",
      "Epoch: 42 Train loss: 14.761 Aux train loss: 8.968 Val loss: 6.029 Aux val loss: 3.628 Train MAE: 40.482 Val MAE: 52.071 Epoch time: 199.795 seconds \n",
      "Epoch: 43 Train loss: 14.131 Aux train loss: 8.444 Val loss: 5.661 Aux val loss: 3.449 Train MAE: 37.519 Val MAE: 30.150 Epoch time: 199.409 seconds best\n",
      "Epoch: 44 Train loss: 14.128 Aux train loss: 8.526 Val loss: 5.758 Aux val loss: 3.503 Train MAE: 38.680 Val MAE: 32.110 Epoch time: 199.470 seconds \n",
      "Epoch: 45 Train loss: 13.922 Aux train loss: 8.506 Val loss: 5.630 Aux val loss: 3.463 Train MAE: 39.207 Val MAE: 30.654 Epoch time: 199.506 seconds \n",
      "Epoch: 46 Train loss: 13.000 Aux train loss: 7.961 Val loss: 5.664 Aux val loss: 3.499 Train MAE: 38.757 Val MAE: 32.389 Epoch time: 198.856 seconds \n",
      "Epoch: 47 Train loss: 13.892 Aux train loss: 8.466 Val loss: 5.625 Aux val loss: 3.410 Train MAE: 34.731 Val MAE: 30.069 Epoch time: 199.135 seconds best\n",
      "Epoch: 48 Train loss: 14.127 Aux train loss: 8.545 Val loss: 5.725 Aux val loss: 3.436 Train MAE: 36.388 Val MAE: 27.197 Epoch time: 199.655 seconds best\n",
      "Epoch: 49 Train loss: 13.101 Aux train loss: 7.984 Val loss: 5.686 Aux val loss: 3.513 Train MAE: 33.958 Val MAE: 41.456 Epoch time: 199.370 seconds \n",
      "Epoch: 50 Train loss: 13.259 Aux train loss: 8.083 Val loss: 5.583 Aux val loss: 3.346 Train MAE: 35.465 Val MAE: 31.442 Epoch time: 199.072 seconds \n",
      "Epoch: 51 Train loss: 13.333 Aux train loss: 8.080 Val loss: 5.484 Aux val loss: 3.340 Train MAE: 34.315 Val MAE: 30.044 Epoch time: 199.104 seconds \n",
      "Epoch: 52 Train loss: 14.041 Aux train loss: 8.609 Val loss: 5.625 Aux val loss: 3.408 Train MAE: 37.315 Val MAE: 37.093 Epoch time: 199.705 seconds \n",
      "Epoch: 53 Train loss: 13.603 Aux train loss: 8.316 Val loss: 5.433 Aux val loss: 3.299 Train MAE: 35.889 Val MAE: 31.307 Epoch time: 199.066 seconds \n",
      "Epoch: 54 Train loss: 12.878 Aux train loss: 7.816 Val loss: 5.547 Aux val loss: 3.370 Train MAE: 37.442 Val MAE: 30.982 Epoch time: 199.209 seconds \n",
      "Epoch: 55 Train loss: 13.513 Aux train loss: 8.247 Val loss: 5.551 Aux val loss: 3.422 Train MAE: 36.157 Val MAE: 32.228 Epoch time: 199.793 seconds \n",
      "Epoch: 56 Train loss: 13.221 Aux train loss: 8.055 Val loss: 5.542 Aux val loss: 3.371 Train MAE: 34.957 Val MAE: 31.717 Epoch time: 198.843 seconds \n",
      "Epoch: 57 Train loss: 12.896 Aux train loss: 7.869 Val loss: 5.465 Aux val loss: 3.358 Train MAE: 33.108 Val MAE: 31.204 Epoch time: 199.181 seconds \n",
      "Epoch: 58 Train loss: 13.695 Aux train loss: 8.369 Val loss: 5.597 Aux val loss: 3.378 Train MAE: 36.451 Val MAE: 38.962 Epoch time: 199.189 seconds \n",
      "Epoch: 59 Train loss: 13.275 Aux train loss: 8.075 Val loss: 5.492 Aux val loss: 3.379 Train MAE: 37.219 Val MAE: 30.514 Epoch time: 199.559 seconds \n",
      "Epoch: 60 Train loss: 13.297 Aux train loss: 8.089 Val loss: 5.485 Aux val loss: 3.320 Train MAE: 33.714 Val MAE: 28.741 Epoch time: 199.095 seconds \n",
      "Epoch: 61 Train loss: 12.459 Aux train loss: 7.600 Val loss: 5.536 Aux val loss: 3.362 Train MAE: 33.484 Val MAE: 31.737 Epoch time: 199.192 seconds \n",
      "Epoch: 62 Train loss: 12.261 Aux train loss: 7.453 Val loss: 5.460 Aux val loss: 3.309 Train MAE: 34.577 Val MAE: 27.300 Epoch time: 199.626 seconds \n",
      "Epoch: 63 Train loss: 13.559 Aux train loss: 8.370 Val loss: 5.458 Aux val loss: 3.318 Train MAE: 37.573 Val MAE: 37.480 Epoch time: 198.933 seconds \n",
      "Epoch: 64 Train loss: 12.816 Aux train loss: 7.859 Val loss: 5.581 Aux val loss: 3.397 Train MAE: 33.803 Val MAE: 29.449 Epoch time: 200.390 seconds \n",
      "Epoch: 65 Train loss: 13.032 Aux train loss: 7.947 Val loss: 5.456 Aux val loss: 3.369 Train MAE: 33.689 Val MAE: 27.350 Epoch time: 199.193 seconds \n",
      "Epoch: 66 Train loss: 13.284 Aux train loss: 8.110 Val loss: 5.451 Aux val loss: 3.312 Train MAE: 35.199 Val MAE: 27.947 Epoch time: 198.637 seconds \n",
      "Epoch: 67 Train loss: 12.879 Aux train loss: 7.868 Val loss: 5.421 Aux val loss: 3.276 Train MAE: 36.238 Val MAE: 32.061 Epoch time: 198.730 seconds \n",
      "Epoch: 68 Train loss: 12.810 Aux train loss: 7.754 Val loss: 5.506 Aux val loss: 3.333 Train MAE: 36.598 Val MAE: 41.440 Epoch time: 199.521 seconds \n",
      "Epoch: 69 Train loss: 13.254 Aux train loss: 8.057 Val loss: 5.344 Aux val loss: 3.270 Train MAE: 33.175 Val MAE: 29.809 Epoch time: 197.974 seconds \n",
      "Epoch: 70 Train loss: 12.642 Aux train loss: 7.707 Val loss: 5.422 Aux val loss: 3.265 Train MAE: 32.988 Val MAE: 41.623 Epoch time: 199.804 seconds \n",
      "Epoch: 71 Train loss: 12.397 Aux train loss: 7.643 Val loss: 5.331 Aux val loss: 3.252 Train MAE: 32.604 Val MAE: 27.497 Epoch time: 199.745 seconds \n",
      "Epoch: 72 Train loss: 12.470 Aux train loss: 7.636 Val loss: 5.308 Aux val loss: 3.222 Train MAE: 32.275 Val MAE: 31.145 Epoch time: 200.856 seconds \n",
      "Epoch: 73 Train loss: 12.864 Aux train loss: 7.857 Val loss: 5.327 Aux val loss: 3.264 Train MAE: 34.380 Val MAE: 27.918 Epoch time: 201.379 seconds \n",
      "Epoch: 74 Train loss: 12.908 Aux train loss: 7.902 Val loss: 5.587 Aux val loss: 3.430 Train MAE: 31.679 Val MAE: 44.479 Epoch time: 201.799 seconds \n",
      "Epoch: 75 Train loss: 12.497 Aux train loss: 7.672 Val loss: 5.282 Aux val loss: 3.210 Train MAE: 33.969 Val MAE: 29.999 Epoch time: 201.356 seconds \n",
      "Epoch: 76 Train loss: 11.991 Aux train loss: 7.332 Val loss: 5.307 Aux val loss: 3.237 Train MAE: 30.805 Val MAE: 36.012 Epoch time: 202.429 seconds \n",
      "Epoch: 77 Train loss: 12.349 Aux train loss: 7.614 Val loss: 5.320 Aux val loss: 3.254 Train MAE: 31.567 Val MAE: 28.342 Epoch time: 202.379 seconds \n",
      "Epoch: 78 Train loss: 11.715 Aux train loss: 7.168 Val loss: 5.203 Aux val loss: 3.224 Train MAE: 30.786 Val MAE: 29.122 Epoch time: 202.738 seconds \n",
      "Epoch: 79 Train loss: 13.131 Aux train loss: 8.012 Val loss: 5.338 Aux val loss: 3.255 Train MAE: 36.633 Val MAE: 27.614 Epoch time: 201.660 seconds \n",
      "Epoch: 80 Train loss: 11.982 Aux train loss: 7.326 Val loss: 5.381 Aux val loss: 3.263 Train MAE: 31.965 Val MAE: 29.147 Epoch time: 201.253 seconds \n",
      "Epoch: 81 Train loss: 13.404 Aux train loss: 8.179 Val loss: 5.378 Aux val loss: 3.245 Train MAE: 32.167 Val MAE: 40.842 Epoch time: 201.260 seconds \n",
      "Epoch: 82 Train loss: 12.014 Aux train loss: 7.393 Val loss: 5.344 Aux val loss: 3.277 Train MAE: 31.111 Val MAE: 31.587 Epoch time: 201.961 seconds \n",
      "Epoch: 83 Train loss: 12.026 Aux train loss: 7.398 Val loss: 5.490 Aux val loss: 3.377 Train MAE: 30.914 Val MAE: 38.215 Epoch time: 200.565 seconds \n",
      "Epoch: 84 Train loss: 12.121 Aux train loss: 7.410 Val loss: 5.194 Aux val loss: 3.192 Train MAE: 31.582 Val MAE: 29.989 Epoch time: 201.177 seconds \n",
      "Epoch: 85 Train loss: 12.245 Aux train loss: 7.478 Val loss: 5.198 Aux val loss: 3.192 Train MAE: 29.497 Val MAE: 29.004 Epoch time: 201.138 seconds \n",
      "Epoch: 86 Train loss: 11.987 Aux train loss: 7.388 Val loss: 5.234 Aux val loss: 3.217 Train MAE: 29.903 Val MAE: 24.771 Epoch time: 200.559 seconds best\n",
      "Epoch: 87 Train loss: 11.998 Aux train loss: 7.370 Val loss: 5.316 Aux val loss: 3.199 Train MAE: 29.938 Val MAE: 32.665 Epoch time: 201.257 seconds \n",
      "Epoch: 88 Train loss: 12.541 Aux train loss: 7.659 Val loss: 5.179 Aux val loss: 3.151 Train MAE: 30.621 Val MAE: 31.182 Epoch time: 201.939 seconds \n",
      "Epoch: 89 Train loss: 12.043 Aux train loss: 7.365 Val loss: 5.215 Aux val loss: 3.197 Train MAE: 30.718 Val MAE: 27.406 Epoch time: 201.076 seconds \n",
      "Epoch: 90 Train loss: 12.358 Aux train loss: 7.527 Val loss: 5.187 Aux val loss: 3.156 Train MAE: 30.326 Val MAE: 32.712 Epoch time: 201.123 seconds \n",
      "Epoch: 91 Train loss: 12.257 Aux train loss: 7.417 Val loss: 5.300 Aux val loss: 3.174 Train MAE: 30.023 Val MAE: 36.334 Epoch time: 200.560 seconds \n",
      "Epoch: 92 Train loss: 12.674 Aux train loss: 7.809 Val loss: 5.263 Aux val loss: 3.205 Train MAE: 31.521 Val MAE: 28.993 Epoch time: 200.316 seconds \n",
      "Epoch: 93 Train loss: 11.920 Aux train loss: 7.312 Val loss: 5.165 Aux val loss: 3.187 Train MAE: 29.917 Val MAE: 23.644 Epoch time: 201.480 seconds best\n",
      "Epoch: 94 Train loss: 12.290 Aux train loss: 297.628 Val loss: 5.140 Aux val loss: 3.140 Train MAE: 30.884 Val MAE: 25.226 Epoch time: 202.145 seconds \n",
      "Epoch: 95 Train loss: 12.123 Aux train loss: 7.418 Val loss: 5.329 Aux val loss: 3.221 Train MAE: 29.251 Val MAE: 36.969 Epoch time: 201.330 seconds \n",
      "Epoch: 96 Train loss: 12.666 Aux train loss: 7.804 Val loss: 5.162 Aux val loss: 3.182 Train MAE: 30.064 Val MAE: 28.959 Epoch time: 201.227 seconds \n",
      "Epoch: 97 Train loss: 11.583 Aux train loss: 7.108 Val loss: 5.147 Aux val loss: 3.122 Train MAE: 27.222 Val MAE: 25.963 Epoch time: 200.961 seconds \n",
      "Epoch: 98 Train loss: 12.254 Aux train loss: 7.509 Val loss: 5.109 Aux val loss: 3.137 Train MAE: 28.068 Val MAE: 24.986 Epoch time: 202.839 seconds \n",
      "Epoch: 99 Train loss: 11.984 Aux train loss: 7.360 Val loss: 5.118 Aux val loss: 3.136 Train MAE: 29.370 Val MAE: 25.146 Epoch time: 200.829 seconds \n",
      "Epoch: 100 Train loss: 12.192 Aux train loss: 7.599 Val loss: 5.372 Aux val loss: 3.282 Train MAE: 28.696 Val MAE: 30.728 Epoch time: 201.575 seconds \n",
      "Epoch: 101 Train loss: 12.500 Aux train loss: 7.300 Val loss: 5.318 Aux val loss: 3.236 Train MAE: 31.290 Val MAE: 34.864 Epoch time: 201.290 seconds \n",
      "Epoch: 102 Train loss: 12.220 Aux train loss: 7.481 Val loss: 5.112 Aux val loss: 3.120 Train MAE: 30.354 Val MAE: 24.128 Epoch time: 200.790 seconds \n",
      "Epoch: 103 Train loss: 12.016 Aux train loss: 7.424 Val loss: 5.155 Aux val loss: 3.152 Train MAE: 30.575 Val MAE: 29.525 Epoch time: 202.036 seconds \n",
      "Epoch: 104 Train loss: 11.532 Aux train loss: 7.084 Val loss: 5.071 Aux val loss: 3.092 Train MAE: 25.372 Val MAE: 23.844 Epoch time: 202.420 seconds \n",
      "Epoch: 105 Train loss: 11.278 Aux train loss: 6.919 Val loss: 5.247 Aux val loss: 3.284 Train MAE: 27.254 Val MAE: 39.698 Epoch time: 203.084 seconds \n",
      "Epoch: 106 Train loss: 11.874 Aux train loss: 7.289 Val loss: 5.136 Aux val loss: 3.167 Train MAE: 28.611 Val MAE: 28.505 Epoch time: 201.737 seconds \n",
      "Epoch: 107 Train loss: 10.995 Aux train loss: 6.761 Val loss: 5.095 Aux val loss: 3.103 Train MAE: 27.823 Val MAE: 24.756 Epoch time: 201.195 seconds \n",
      "Epoch: 108 Train loss: 11.399 Aux train loss: 7.024 Val loss: 5.104 Aux val loss: 3.118 Train MAE: 29.045 Val MAE: 23.726 Epoch time: 201.741 seconds \n",
      "Epoch: 109 Train loss: 11.262 Aux train loss: 6.875 Val loss: 5.255 Aux val loss: 3.157 Train MAE: 27.484 Val MAE: 32.181 Epoch time: 202.324 seconds \n",
      "Epoch: 110 Train loss: 11.520 Aux train loss: 7.062 Val loss: 5.042 Aux val loss: 3.090 Train MAE: 28.608 Val MAE: 27.852 Epoch time: 202.749 seconds \n",
      "Epoch: 111 Train loss: 11.621 Aux train loss: 7.154 Val loss: 5.094 Aux val loss: 3.168 Train MAE: 29.587 Val MAE: 30.100 Epoch time: 201.541 seconds \n",
      "Epoch: 112 Train loss: 11.403 Aux train loss: 6.951 Val loss: 5.111 Aux val loss: 3.134 Train MAE: 27.087 Val MAE: 24.310 Epoch time: 200.527 seconds \n",
      "Epoch: 113 Train loss: 11.846 Aux train loss: 7.264 Val loss: 5.010 Aux val loss: 3.075 Train MAE: 28.736 Val MAE: 30.730 Epoch time: 201.620 seconds \n",
      "Epoch: 114 Train loss: 12.123 Aux train loss: 7.324 Val loss: 5.204 Aux val loss: 3.141 Train MAE: 28.839 Val MAE: 36.926 Epoch time: 201.633 seconds \n",
      "Epoch: 115 Train loss: 11.168 Aux train loss: 6.875 Val loss: 4.989 Aux val loss: 3.076 Train MAE: 27.007 Val MAE: 26.199 Epoch time: 199.989 seconds \n",
      "Epoch: 116 Train loss: 11.164 Aux train loss: 6.724 Val loss: 5.169 Aux val loss: 3.102 Train MAE: 27.530 Val MAE: 40.251 Epoch time: 200.397 seconds \n",
      "Epoch: 117 Train loss: 11.945 Aux train loss: 7.346 Val loss: 5.091 Aux val loss: 3.117 Train MAE: 29.134 Val MAE: 27.152 Epoch time: 200.578 seconds \n",
      "Epoch: 118 Train loss: 11.456 Aux train loss: 7.061 Val loss: 5.068 Aux val loss: 3.122 Train MAE: 27.402 Val MAE: 29.856 Epoch time: 200.479 seconds \n",
      "Epoch: 119 Train loss: 11.083 Aux train loss: 6.912 Val loss: 5.020 Aux val loss: 3.084 Train MAE: 27.066 Val MAE: 26.911 Epoch time: 200.901 seconds \n",
      "Epoch: 120 Train loss: 12.266 Aux train loss: 7.541 Val loss: 5.103 Aux val loss: 3.083 Train MAE: 28.694 Val MAE: 33.459 Epoch time: 200.562 seconds \n",
      "Epoch: 121 Train loss: 11.439 Aux train loss: 7.046 Val loss: 5.109 Aux val loss: 3.149 Train MAE: 26.797 Val MAE: 31.881 Epoch time: 200.486 seconds \n",
      "Epoch: 122 Train loss: 11.812 Aux train loss: 7.287 Val loss: 5.178 Aux val loss: 3.138 Train MAE: 27.716 Val MAE: 35.025 Epoch time: 199.938 seconds \n",
      "Epoch: 123 Train loss: 11.730 Aux train loss: 7.251 Val loss: 5.124 Aux val loss: 3.159 Train MAE: 26.922 Val MAE: 36.538 Epoch time: 205.075 seconds \n",
      "Epoch: 124 Train loss: 11.777 Aux train loss: 7.273 Val loss: 4.975 Aux val loss: 3.032 Train MAE: 28.652 Val MAE: 24.925 Epoch time: 213.592 seconds \n",
      "Epoch: 125 Train loss: 11.976 Aux train loss: 7.365 Val loss: 5.114 Aux val loss: 3.195 Train MAE: 28.534 Val MAE: 33.049 Epoch time: 213.228 seconds \n",
      "Epoch: 126 Train loss: 11.252 Aux train loss: 6.994 Val loss: 5.027 Aux val loss: 3.100 Train MAE: 29.092 Val MAE: 27.008 Epoch time: 212.203 seconds \n",
      "Epoch: 127 Train loss: 10.934 Aux train loss: 6.716 Val loss: 5.112 Aux val loss: 3.136 Train MAE: 28.460 Val MAE: 33.025 Epoch time: 209.378 seconds \n",
      "Epoch: 128 Train loss: 11.251 Aux train loss: 6.954 Val loss: 4.900 Aux val loss: 3.059 Train MAE: 26.527 Val MAE: 23.527 Epoch time: 212.007 seconds best\n",
      "Epoch: 129 Train loss: 10.966 Aux train loss: 6.771 Val loss: 5.057 Aux val loss: 3.102 Train MAE: 28.487 Val MAE: 28.302 Epoch time: 210.788 seconds \n",
      "Epoch: 130 Train loss: 12.513 Aux train loss: 7.580 Val loss: 5.087 Aux val loss: 3.167 Train MAE: 28.117 Val MAE: 29.807 Epoch time: 210.481 seconds \n",
      "Epoch: 131 Train loss: 12.661 Aux train loss: 6.934 Val loss: 5.066 Aux val loss: 3.074 Train MAE: 25.736 Val MAE: 23.888 Epoch time: 208.180 seconds \n",
      "Epoch: 132 Train loss: 11.651 Aux train loss: 7.121 Val loss: 4.999 Aux val loss: 3.080 Train MAE: 27.347 Val MAE: 27.667 Epoch time: 208.899 seconds \n",
      "Epoch: 133 Train loss: 11.533 Aux train loss: 7.119 Val loss: 4.989 Aux val loss: 3.049 Train MAE: 26.356 Val MAE: 22.628 Epoch time: 205.128 seconds best\n",
      "Epoch: 134 Train loss: 11.803 Aux train loss: 7.306 Val loss: 5.162 Aux val loss: 3.046 Train MAE: 28.631 Val MAE: 44.421 Epoch time: 208.688 seconds \n",
      "Epoch: 135 Train loss: 11.395 Aux train loss: 7.009 Val loss: 4.933 Aux val loss: 3.019 Train MAE: 27.458 Val MAE: 26.592 Epoch time: 210.315 seconds \n",
      "Epoch: 136 Train loss: 10.840 Aux train loss: 6.674 Val loss: 4.896 Aux val loss: 3.021 Train MAE: 26.014 Val MAE: 23.230 Epoch time: 211.394 seconds \n",
      "Epoch: 137 Train loss: 10.702 Aux train loss: 7.208 Val loss: 4.906 Aux val loss: 3.014 Train MAE: 25.305 Val MAE: 23.187 Epoch time: 207.015 seconds \n",
      "Epoch: 138 Train loss: 10.952 Aux train loss: 6.758 Val loss: 4.917 Aux val loss: 2.996 Train MAE: 25.259 Val MAE: 23.753 Epoch time: 208.372 seconds \n",
      "Epoch: 139 Train loss: 10.740 Aux train loss: 6.630 Val loss: 4.991 Aux val loss: 3.040 Train MAE: 23.924 Val MAE: 31.194 Epoch time: 208.685 seconds \n",
      "Epoch: 140 Train loss: 11.556 Aux train loss: 6.752 Val loss: 4.944 Aux val loss: 3.053 Train MAE: 27.287 Val MAE: 23.648 Epoch time: 205.911 seconds \n",
      "Epoch: 141 Train loss: 11.326 Aux train loss: 6.958 Val loss: 5.011 Aux val loss: 3.049 Train MAE: 26.692 Val MAE: 26.589 Epoch time: 207.601 seconds \n",
      "Epoch: 142 Train loss: 10.781 Aux train loss: 6.668 Val loss: 4.901 Aux val loss: 3.025 Train MAE: 25.452 Val MAE: 22.551 Epoch time: 207.457 seconds best\n",
      "Epoch: 143 Train loss: 11.343 Aux train loss: 6.986 Val loss: 5.022 Aux val loss: 3.067 Train MAE: 25.840 Val MAE: 23.571 Epoch time: 208.098 seconds \n",
      "Epoch: 144 Train loss: 10.522 Aux train loss: 6.502 Val loss: 4.971 Aux val loss: 3.070 Train MAE: 26.172 Val MAE: 23.553 Epoch time: 208.208 seconds \n",
      "Epoch: 145 Train loss: 10.924 Aux train loss: 6.746 Val loss: 4.967 Aux val loss: 3.021 Train MAE: 25.144 Val MAE: 35.633 Epoch time: 209.084 seconds \n",
      "Epoch: 146 Train loss: 11.120 Aux train loss: 6.871 Val loss: 4.956 Aux val loss: 3.060 Train MAE: 24.224 Val MAE: 27.942 Epoch time: 216.505 seconds \n",
      "Epoch: 147 Train loss: 11.213 Aux train loss: 6.951 Val loss: 4.875 Aux val loss: 2.999 Train MAE: 23.993 Val MAE: 23.478 Epoch time: 209.062 seconds \n",
      "Epoch: 148 Train loss: 10.922 Aux train loss: 6.749 Val loss: 4.932 Aux val loss: 3.025 Train MAE: 25.743 Val MAE: 25.250 Epoch time: 209.878 seconds \n",
      "Epoch: 149 Train loss: 11.122 Aux train loss: 6.876 Val loss: 4.966 Aux val loss: 3.016 Train MAE: 25.255 Val MAE: 25.121 Epoch time: 209.697 seconds \n",
      "Epoch: 150 Train loss: 11.069 Aux train loss: 6.784 Val loss: 4.947 Aux val loss: 3.049 Train MAE: 25.613 Val MAE: 21.782 Epoch time: 210.078 seconds best\n",
      "Epoch: 151 Train loss: 11.012 Aux train loss: 6.908 Val loss: 5.009 Aux val loss: 3.079 Train MAE: 24.609 Val MAE: 30.224 Epoch time: 209.853 seconds \n",
      "Epoch: 152 Train loss: 11.053 Aux train loss: 6.831 Val loss: 4.924 Aux val loss: 3.002 Train MAE: 23.442 Val MAE: 22.391 Epoch time: 212.059 seconds \n",
      "Epoch: 153 Train loss: 10.948 Aux train loss: 6.801 Val loss: 4.911 Aux val loss: 2.999 Train MAE: 23.825 Val MAE: 24.345 Epoch time: 211.094 seconds \n",
      "Epoch: 154 Train loss: 11.776 Aux train loss: 7.227 Val loss: 4.965 Aux val loss: 2.975 Train MAE: 25.159 Val MAE: 27.550 Epoch time: 206.706 seconds \n",
      "Epoch: 155 Train loss: 10.647 Aux train loss: 6.534 Val loss: 4.886 Aux val loss: 2.979 Train MAE: 23.436 Val MAE: 23.657 Epoch time: 209.086 seconds \n",
      "Epoch: 156 Train loss: 11.068 Aux train loss: 6.826 Val loss: 5.091 Aux val loss: 3.082 Train MAE: 24.743 Val MAE: 23.402 Epoch time: 209.946 seconds \n",
      "Epoch: 157 Train loss: 10.979 Aux train loss: 6.799 Val loss: 4.841 Aux val loss: 2.963 Train MAE: 24.526 Val MAE: 21.706 Epoch time: 206.190 seconds best\n",
      "Epoch: 158 Train loss: 10.355 Aux train loss: 6.387 Val loss: 4.924 Aux val loss: 3.027 Train MAE: 23.729 Val MAE: 30.558 Epoch time: 204.762 seconds \n",
      "Epoch: 159 Train loss: 10.617 Aux train loss: 6.594 Val loss: 4.868 Aux val loss: 2.974 Train MAE: 22.589 Val MAE: 26.518 Epoch time: 209.044 seconds \n",
      "Epoch: 160 Train loss: 10.996 Aux train loss: 6.766 Val loss: 4.888 Aux val loss: 2.995 Train MAE: 24.435 Val MAE: 26.861 Epoch time: 205.710 seconds \n",
      "Epoch: 161 Train loss: 10.493 Aux train loss: 6.491 Val loss: 4.876 Aux val loss: 2.976 Train MAE: 24.588 Val MAE: 23.243 Epoch time: 209.326 seconds \n",
      "Epoch: 162 Train loss: 11.408 Aux train loss: 6.875 Val loss: 4.905 Aux val loss: 2.986 Train MAE: 25.624 Val MAE: 26.881 Epoch time: 210.841 seconds \n",
      "Epoch: 163 Train loss: 10.236 Aux train loss: 6.307 Val loss: 4.961 Aux val loss: 2.973 Train MAE: 23.159 Val MAE: 27.839 Epoch time: 206.755 seconds \n",
      "Epoch: 164 Train loss: 10.346 Aux train loss: 6.371 Val loss: 4.857 Aux val loss: 2.947 Train MAE: 24.162 Val MAE: 23.317 Epoch time: 207.649 seconds \n",
      "Epoch: 165 Train loss: 11.100 Aux train loss: 6.824 Val loss: 4.818 Aux val loss: 2.961 Train MAE: 25.533 Val MAE: 27.268 Epoch time: 207.549 seconds \n",
      "Epoch: 166 Train loss: 10.117 Aux train loss: 6.245 Val loss: 4.907 Aux val loss: 3.013 Train MAE: 24.638 Val MAE: 26.602 Epoch time: 205.998 seconds \n",
      "Epoch: 167 Train loss: 10.751 Aux train loss: 6.679 Val loss: 4.835 Aux val loss: 2.982 Train MAE: 24.326 Val MAE: 24.809 Epoch time: 204.084 seconds \n",
      "Epoch: 168 Train loss: 11.133 Aux train loss: 6.872 Val loss: 4.914 Aux val loss: 2.980 Train MAE: 23.919 Val MAE: 23.634 Epoch time: 204.577 seconds \n",
      "Epoch: 169 Train loss: 10.066 Aux train loss: 6.228 Val loss: 4.827 Aux val loss: 2.955 Train MAE: 21.522 Val MAE: 27.503 Epoch time: 207.070 seconds \n",
      "Epoch: 170 Train loss: 10.205 Aux train loss: 6.306 Val loss: 4.892 Aux val loss: 2.986 Train MAE: 22.536 Val MAE: 27.642 Epoch time: 204.044 seconds \n",
      "Epoch: 171 Train loss: 10.913 Aux train loss: 6.719 Val loss: 4.973 Aux val loss: 3.038 Train MAE: 23.641 Val MAE: 29.484 Epoch time: 203.550 seconds \n",
      "Epoch: 172 Train loss: 11.413 Aux train loss: 7.040 Val loss: 4.811 Aux val loss: 2.973 Train MAE: 26.730 Val MAE: 27.687 Epoch time: 204.565 seconds \n",
      "Epoch: 173 Train loss: 10.178 Aux train loss: 6.297 Val loss: 4.887 Aux val loss: 2.966 Train MAE: 24.069 Val MAE: 23.277 Epoch time: 207.892 seconds \n",
      "Epoch: 174 Train loss: 10.694 Aux train loss: 6.569 Val loss: 4.902 Aux val loss: 2.986 Train MAE: 24.985 Val MAE: 26.215 Epoch time: 203.734 seconds \n",
      "Epoch: 175 Train loss: 10.643 Aux train loss: 6.599 Val loss: 4.971 Aux val loss: 3.067 Train MAE: 21.749 Val MAE: 29.736 Epoch time: 203.768 seconds \n",
      "Epoch: 176 Train loss: 10.824 Aux train loss: 6.671 Val loss: 5.026 Aux val loss: 2.983 Train MAE: 23.516 Val MAE: 42.414 Epoch time: 207.187 seconds \n",
      "Epoch: 177 Train loss: 10.388 Aux train loss: 6.420 Val loss: 4.814 Aux val loss: 2.940 Train MAE: 23.940 Val MAE: 23.956 Epoch time: 203.862 seconds \n",
      "Epoch: 178 Train loss: 11.088 Aux train loss: 6.852 Val loss: 4.796 Aux val loss: 2.919 Train MAE: 24.308 Val MAE: 27.619 Epoch time: 204.511 seconds \n",
      "Epoch: 179 Train loss: 10.986 Aux train loss: 6.799 Val loss: 4.814 Aux val loss: 2.952 Train MAE: 23.326 Val MAE: 22.399 Epoch time: 210.231 seconds \n",
      "Epoch: 180 Train loss: 10.202 Aux train loss: 6.316 Val loss: 4.882 Aux val loss: 3.003 Train MAE: 25.235 Val MAE: 26.832 Epoch time: 207.738 seconds \n",
      "Epoch: 181 Train loss: 10.365 Aux train loss: 6.382 Val loss: 4.793 Aux val loss: 2.922 Train MAE: 23.581 Val MAE: 23.328 Epoch time: 208.611 seconds \n",
      "Epoch: 182 Train loss: 10.542 Aux train loss: 6.494 Val loss: 4.989 Aux val loss: 2.989 Train MAE: 23.951 Val MAE: 23.814 Epoch time: 205.503 seconds \n",
      "Epoch: 183 Train loss: 10.911 Aux train loss: 6.755 Val loss: 4.941 Aux val loss: 3.001 Train MAE: 21.672 Val MAE: 35.868 Epoch time: 204.784 seconds \n",
      "Epoch: 184 Train loss: 10.683 Aux train loss: 6.579 Val loss: 4.892 Aux val loss: 2.998 Train MAE: 25.237 Val MAE: 22.325 Epoch time: 208.678 seconds \n",
      "Epoch: 185 Train loss: 10.606 Aux train loss: 6.558 Val loss: 4.902 Aux val loss: 2.963 Train MAE: 24.255 Val MAE: 25.449 Epoch time: 204.601 seconds \n",
      "Epoch: 186 Train loss: 10.598 Aux train loss: 6.533 Val loss: 4.877 Aux val loss: 2.981 Train MAE: 22.368 Val MAE: 27.627 Epoch time: 207.662 seconds \n",
      "Epoch: 187 Train loss: 11.519 Aux train loss: 7.045 Val loss: 4.909 Aux val loss: 3.011 Train MAE: 24.945 Val MAE: 23.558 Epoch time: 208.920 seconds \n",
      "Epoch: 188 Train loss: 10.726 Aux train loss: 6.623 Val loss: 4.836 Aux val loss: 2.949 Train MAE: 23.817 Val MAE: 21.741 Epoch time: 205.991 seconds \n",
      "Epoch: 189 Train loss: 11.174 Aux train loss: 6.919 Val loss: 4.906 Aux val loss: 2.977 Train MAE: 24.231 Val MAE: 31.485 Epoch time: 206.270 seconds \n",
      "Epoch: 190 Train loss: 10.781 Aux train loss: 6.689 Val loss: 4.825 Aux val loss: 2.933 Train MAE: 22.712 Val MAE: 23.984 Epoch time: 205.252 seconds \n",
      "Epoch: 191 Train loss: 10.279 Aux train loss: 6.370 Val loss: 4.844 Aux val loss: 2.956 Train MAE: 23.276 Val MAE: 24.482 Epoch time: 206.846 seconds \n",
      "Epoch: 192 Train loss: 10.652 Aux train loss: 6.589 Val loss: 4.846 Aux val loss: 2.954 Train MAE: 21.880 Val MAE: 28.391 Epoch time: 207.800 seconds \n",
      "Epoch: 193 Train loss: 11.318 Aux train loss: 6.992 Val loss: 4.892 Aux val loss: 3.001 Train MAE: 25.011 Val MAE: 28.890 Epoch time: 207.570 seconds \n",
      "Epoch: 194 Train loss: 11.111 Aux train loss: 6.858 Val loss: 4.808 Aux val loss: 2.957 Train MAE: 23.210 Val MAE: 23.351 Epoch time: 210.239 seconds \n",
      "Epoch: 195 Train loss: 10.451 Aux train loss: 6.465 Val loss: 4.779 Aux val loss: 2.913 Train MAE: 21.683 Val MAE: 28.663 Epoch time: 213.193 seconds \n",
      "Epoch: 196 Train loss: 11.283 Aux train loss: 6.711 Val loss: 4.893 Aux val loss: 2.984 Train MAE: 23.289 Val MAE: 27.286 Epoch time: 204.509 seconds \n",
      "Epoch: 197 Train loss: 10.267 Aux train loss: 6.320 Val loss: 4.834 Aux val loss: 2.929 Train MAE: 23.891 Val MAE: 26.107 Epoch time: 210.527 seconds \n",
      "Epoch: 198 Train loss: 11.325 Aux train loss: 7.012 Val loss: 4.870 Aux val loss: 2.991 Train MAE: 23.780 Val MAE: 30.239 Epoch time: 207.461 seconds \n",
      "Epoch: 199 Train loss: 11.067 Aux train loss: 6.806 Val loss: 4.872 Aux val loss: 3.008 Train MAE: 25.302 Val MAE: 25.944 Epoch time: 207.386 seconds \n",
      "Epoch: 200 Train loss: 11.356 Aux train loss: 6.355 Val loss: 4.961 Aux val loss: 3.026 Train MAE: 23.714 Val MAE: 23.281 Epoch time: 207.105 seconds \n",
      "Epoch: 201 Train loss: 28.295 Aux train loss: 6.046 Val loss: 4.732 Aux val loss: 2.871 Train MAE: 18.468 Val MAE: 23.137 Epoch time: 208.117 seconds \n",
      "Epoch: 202 Train loss: 9.483 Aux train loss: 5.856 Val loss: 4.726 Aux val loss: 2.880 Train MAE: 16.829 Val MAE: 24.349 Epoch time: 206.058 seconds \n",
      "Epoch: 203 Train loss: 9.757 Aux train loss: 6.035 Val loss: 4.740 Aux val loss: 2.853 Train MAE: 16.474 Val MAE: 20.895 Epoch time: 207.091 seconds best\n",
      "Epoch: 204 Train loss: 9.698 Aux train loss: 6.020 Val loss: 4.719 Aux val loss: 2.848 Train MAE: 15.817 Val MAE: 19.808 Epoch time: 207.980 seconds best\n",
      "Epoch: 205 Train loss: 9.674 Aux train loss: 5.972 Val loss: 4.734 Aux val loss: 2.869 Train MAE: 15.939 Val MAE: 21.937 Epoch time: 202.819 seconds \n",
      "Epoch: 206 Train loss: 9.338 Aux train loss: 5.780 Val loss: 4.716 Aux val loss: 2.851 Train MAE: 15.389 Val MAE: 21.993 Epoch time: 199.406 seconds \n",
      "Epoch: 207 Train loss: 9.947 Aux train loss: 6.175 Val loss: 4.715 Aux val loss: 2.855 Train MAE: 15.575 Val MAE: 24.258 Epoch time: 198.958 seconds \n",
      "Epoch: 208 Train loss: 9.449 Aux train loss: 5.871 Val loss: 4.734 Aux val loss: 2.859 Train MAE: 14.696 Val MAE: 22.641 Epoch time: 199.065 seconds \n",
      "Epoch: 209 Train loss: 9.806 Aux train loss: 6.078 Val loss: 4.704 Aux val loss: 2.847 Train MAE: 15.543 Val MAE: 23.928 Epoch time: 198.801 seconds \n",
      "Epoch: 210 Train loss: 9.669 Aux train loss: 5.989 Val loss: 4.707 Aux val loss: 2.858 Train MAE: 15.475 Val MAE: 22.945 Epoch time: 201.591 seconds \n",
      "Epoch: 211 Train loss: 9.812 Aux train loss: 6.078 Val loss: 4.684 Aux val loss: 2.846 Train MAE: 15.464 Val MAE: 20.074 Epoch time: 200.757 seconds \n",
      "Epoch: 212 Train loss: 9.500 Aux train loss: 5.885 Val loss: 4.688 Aux val loss: 2.847 Train MAE: 14.826 Val MAE: 21.564 Epoch time: 199.429 seconds \n",
      "Epoch: 213 Train loss: 9.357 Aux train loss: 5.805 Val loss: 4.718 Aux val loss: 2.842 Train MAE: 14.661 Val MAE: 19.390 Epoch time: 199.183 seconds best\n",
      "Epoch: 214 Train loss: 9.539 Aux train loss: 5.895 Val loss: 4.679 Aux val loss: 2.829 Train MAE: 15.238 Val MAE: 21.155 Epoch time: 199.686 seconds \n",
      "Epoch: 215 Train loss: 9.375 Aux train loss: 5.809 Val loss: 4.706 Aux val loss: 2.847 Train MAE: 14.806 Val MAE: 23.831 Epoch time: 198.897 seconds \n",
      "Epoch: 216 Train loss: 9.220 Aux train loss: 5.727 Val loss: 4.688 Aux val loss: 2.846 Train MAE: 15.372 Val MAE: 23.548 Epoch time: 198.088 seconds \n",
      "Epoch: 217 Train loss: 9.588 Aux train loss: 5.921 Val loss: 4.698 Aux val loss: 2.836 Train MAE: 15.791 Val MAE: 20.671 Epoch time: 198.078 seconds \n",
      "Epoch: 218 Train loss: 9.095 Aux train loss: 5.643 Val loss: 4.698 Aux val loss: 2.848 Train MAE: 14.334 Val MAE: 21.627 Epoch time: 199.261 seconds \n",
      "Epoch: 219 Train loss: 9.509 Aux train loss: 5.914 Val loss: 4.692 Aux val loss: 2.843 Train MAE: 15.289 Val MAE: 20.433 Epoch time: 199.205 seconds \n",
      "Epoch: 220 Train loss: 9.735 Aux train loss: 6.029 Val loss: 4.664 Aux val loss: 2.831 Train MAE: 14.739 Val MAE: 20.586 Epoch time: 197.195 seconds \n",
      "Epoch: 221 Train loss: 8.958 Aux train loss: 5.567 Val loss: 4.699 Aux val loss: 2.843 Train MAE: 13.644 Val MAE: 24.095 Epoch time: 198.355 seconds \n",
      "Epoch: 222 Train loss: 9.496 Aux train loss: 5.887 Val loss: 4.710 Aux val loss: 2.836 Train MAE: 14.642 Val MAE: 21.261 Epoch time: 197.965 seconds \n",
      "Epoch: 223 Train loss: 10.413 Aux train loss: 6.434 Val loss: 4.715 Aux val loss: 2.834 Train MAE: 15.518 Val MAE: 19.817 Epoch time: 197.742 seconds \n",
      "Epoch: 224 Train loss: 9.999 Aux train loss: 6.193 Val loss: 4.683 Aux val loss: 2.816 Train MAE: 14.769 Val MAE: 20.918 Epoch time: 198.027 seconds \n",
      "Epoch: 225 Train loss: 9.528 Aux train loss: 5.908 Val loss: 4.696 Aux val loss: 2.835 Train MAE: 14.860 Val MAE: 19.140 Epoch time: 197.562 seconds best\n",
      "Epoch: 226 Train loss: 9.900 Aux train loss: 6.126 Val loss: 4.666 Aux val loss: 2.819 Train MAE: 14.987 Val MAE: 23.447 Epoch time: 198.790 seconds \n",
      "Epoch: 227 Train loss: 9.583 Aux train loss: 5.935 Val loss: 4.650 Aux val loss: 2.827 Train MAE: 14.530 Val MAE: 19.848 Epoch time: 198.072 seconds \n",
      "Epoch: 228 Train loss: 9.667 Aux train loss: 6.002 Val loss: 4.694 Aux val loss: 2.845 Train MAE: 14.109 Val MAE: 24.262 Epoch time: 198.036 seconds \n",
      "Epoch: 229 Train loss: 10.298 Aux train loss: 6.361 Val loss: 4.680 Aux val loss: 2.841 Train MAE: 14.742 Val MAE: 21.058 Epoch time: 197.045 seconds \n",
      "Epoch: 230 Train loss: 9.602 Aux train loss: 5.955 Val loss: 4.679 Aux val loss: 2.840 Train MAE: 14.833 Val MAE: 22.253 Epoch time: 197.973 seconds \n",
      "Epoch: 231 Train loss: 9.494 Aux train loss: 5.891 Val loss: 4.643 Aux val loss: 2.813 Train MAE: 14.258 Val MAE: 20.463 Epoch time: 197.673 seconds \n",
      "Epoch: 232 Train loss: 9.029 Aux train loss: 5.594 Val loss: 4.662 Aux val loss: 2.836 Train MAE: 14.638 Val MAE: 19.717 Epoch time: 197.964 seconds \n",
      "Epoch: 233 Train loss: 9.292 Aux train loss: 5.769 Val loss: 4.654 Aux val loss: 2.833 Train MAE: 13.357 Val MAE: 19.445 Epoch time: 197.279 seconds \n",
      "Epoch: 234 Train loss: 9.484 Aux train loss: 5.883 Val loss: 4.639 Aux val loss: 2.833 Train MAE: 14.777 Val MAE: 19.860 Epoch time: 197.132 seconds \n",
      "Epoch: 235 Train loss: 9.352 Aux train loss: 5.795 Val loss: 4.650 Aux val loss: 2.837 Train MAE: 14.210 Val MAE: 21.318 Epoch time: 197.199 seconds \n",
      "Epoch: 236 Train loss: 10.320 Aux train loss: 6.391 Val loss: 4.648 Aux val loss: 2.832 Train MAE: 14.631 Val MAE: 21.888 Epoch time: 197.750 seconds \n",
      "Epoch: 237 Train loss: 9.442 Aux train loss: 5.881 Val loss: 4.658 Aux val loss: 2.831 Train MAE: 14.126 Val MAE: 20.387 Epoch time: 198.219 seconds \n",
      "Epoch: 238 Train loss: 9.401 Aux train loss: 5.834 Val loss: 4.662 Aux val loss: 2.825 Train MAE: 14.961 Val MAE: 21.125 Epoch time: 197.361 seconds \n",
      "Epoch: 239 Train loss: 9.860 Aux train loss: 6.124 Val loss: 4.715 Aux val loss: 2.858 Train MAE: 13.928 Val MAE: 22.097 Epoch time: 197.678 seconds \n",
      "Epoch: 240 Train loss: 9.916 Aux train loss: 6.157 Val loss: 4.663 Aux val loss: 2.843 Train MAE: 14.756 Val MAE: 22.310 Epoch time: 197.514 seconds \n",
      "Epoch: 241 Train loss: 9.500 Aux train loss: 5.884 Val loss: 4.661 Aux val loss: 2.834 Train MAE: 13.845 Val MAE: 19.734 Epoch time: 197.035 seconds \n",
      "Epoch: 242 Train loss: 8.958 Aux train loss: 5.577 Val loss: 4.687 Aux val loss: 2.845 Train MAE: 13.738 Val MAE: 25.778 Epoch time: 197.649 seconds \n",
      "Epoch: 243 Train loss: 9.757 Aux train loss: 6.044 Val loss: 4.670 Aux val loss: 2.845 Train MAE: 14.386 Val MAE: 19.597 Epoch time: 197.502 seconds \n",
      "Epoch: 244 Train loss: 9.594 Aux train loss: 5.950 Val loss: 4.650 Aux val loss: 2.818 Train MAE: 14.482 Val MAE: 22.344 Epoch time: 197.596 seconds \n",
      "Epoch: 245 Train loss: 9.966 Aux train loss: 6.175 Val loss: 4.647 Aux val loss: 2.822 Train MAE: 14.140 Val MAE: 20.299 Epoch time: 197.730 seconds \n",
      "Epoch: 246 Train loss: 9.088 Aux train loss: 5.635 Val loss: 4.627 Aux val loss: 2.816 Train MAE: 14.518 Val MAE: 20.343 Epoch time: 197.618 seconds \n",
      "Epoch: 247 Train loss: 10.077 Aux train loss: 6.242 Val loss: 4.687 Aux val loss: 2.835 Train MAE: 14.771 Val MAE: 24.087 Epoch time: 197.492 seconds \n",
      "Epoch: 248 Train loss: 9.356 Aux train loss: 5.816 Val loss: 4.646 Aux val loss: 2.824 Train MAE: 14.000 Val MAE: 20.993 Epoch time: 197.516 seconds \n",
      "Epoch: 249 Train loss: 9.283 Aux train loss: 5.760 Val loss: 4.627 Aux val loss: 2.804 Train MAE: 14.607 Val MAE: 20.381 Epoch time: 197.005 seconds \n",
      "Epoch: 250 Train loss: 9.227 Aux train loss: 5.722 Val loss: 4.677 Aux val loss: 2.830 Train MAE: 14.381 Val MAE: 25.258 Epoch time: 197.428 seconds \n",
      "Epoch: 251 Train loss: 9.392 Aux train loss: 5.831 Val loss: 4.654 Aux val loss: 2.828 Train MAE: 14.435 Val MAE: 20.608 Epoch time: 197.305 seconds \n",
      "Epoch: 252 Train loss: 8.852 Aux train loss: 5.498 Val loss: 4.652 Aux val loss: 2.832 Train MAE: 13.750 Val MAE: 21.393 Epoch time: 197.710 seconds \n",
      "Epoch: 253 Train loss: 9.451 Aux train loss: 5.866 Val loss: 4.621 Aux val loss: 2.814 Train MAE: 13.605 Val MAE: 20.833 Epoch time: 197.486 seconds \n",
      "Epoch: 254 Train loss: 9.534 Aux train loss: 5.925 Val loss: 4.646 Aux val loss: 2.823 Train MAE: 14.257 Val MAE: 23.058 Epoch time: 197.507 seconds \n",
      "Epoch: 255 Train loss: 9.625 Aux train loss: 5.970 Val loss: 4.657 Aux val loss: 2.827 Train MAE: 13.410 Val MAE: 21.531 Epoch time: 197.610 seconds \n",
      "Epoch: 256 Train loss: 8.987 Aux train loss: 5.593 Val loss: 4.647 Aux val loss: 2.828 Train MAE: 13.573 Val MAE: 20.373 Epoch time: 197.381 seconds \n",
      "Epoch: 257 Train loss: 8.991 Aux train loss: 5.567 Val loss: 4.670 Aux val loss: 2.850 Train MAE: 13.455 Val MAE: 21.305 Epoch time: 197.826 seconds \n",
      "Epoch: 258 Train loss: 8.735 Aux train loss: 5.434 Val loss: 4.640 Aux val loss: 2.828 Train MAE: 13.410 Val MAE: 22.208 Epoch time: 197.225 seconds \n",
      "Epoch: 259 Train loss: 9.276 Aux train loss: 5.768 Val loss: 4.660 Aux val loss: 2.826 Train MAE: 13.789 Val MAE: 20.911 Epoch time: 198.061 seconds \n",
      "Epoch: 260 Train loss: 9.361 Aux train loss: 5.811 Val loss: 4.665 Aux val loss: 2.842 Train MAE: 14.224 Val MAE: 19.556 Epoch time: 197.068 seconds \n",
      "Epoch: 261 Train loss: 9.139 Aux train loss: 5.686 Val loss: 4.672 Aux val loss: 2.833 Train MAE: 13.930 Val MAE: 24.072 Epoch time: 197.371 seconds \n",
      "Epoch: 262 Train loss: 8.826 Aux train loss: 5.496 Val loss: 4.654 Aux val loss: 2.847 Train MAE: 13.344 Val MAE: 19.496 Epoch time: 197.197 seconds \n",
      "Epoch: 263 Train loss: 9.020 Aux train loss: 5.603 Val loss: 4.640 Aux val loss: 2.823 Train MAE: 13.605 Val MAE: 20.978 Epoch time: 197.194 seconds \n",
      "Epoch: 264 Train loss: 8.631 Aux train loss: 5.339 Val loss: 4.631 Aux val loss: 2.822 Train MAE: 13.962 Val MAE: 21.181 Epoch time: 197.837 seconds \n",
      "Epoch: 265 Train loss: 9.332 Aux train loss: 5.806 Val loss: 4.622 Aux val loss: 2.811 Train MAE: 13.663 Val MAE: 20.023 Epoch time: 200.591 seconds \n",
      "Epoch: 266 Train loss: 8.936 Aux train loss: 5.566 Val loss: 4.632 Aux val loss: 2.808 Train MAE: 13.396 Val MAE: 19.386 Epoch time: 204.605 seconds \n",
      "Epoch: 267 Train loss: 8.932 Aux train loss: 5.571 Val loss: 4.618 Aux val loss: 2.813 Train MAE: 13.797 Val MAE: 20.079 Epoch time: 204.388 seconds \n",
      "Epoch: 268 Train loss: 8.562 Aux train loss: 5.324 Val loss: 4.631 Aux val loss: 2.808 Train MAE: 13.324 Val MAE: 21.412 Epoch time: 204.429 seconds \n",
      "Epoch: 269 Train loss: 8.812 Aux train loss: 5.477 Val loss: 4.620 Aux val loss: 2.818 Train MAE: 13.688 Val MAE: 22.329 Epoch time: 204.858 seconds \n",
      "Epoch: 270 Train loss: 9.193 Aux train loss: 5.699 Val loss: 4.621 Aux val loss: 2.814 Train MAE: 13.772 Val MAE: 22.403 Epoch time: 204.271 seconds \n",
      "Epoch: 271 Train loss: 8.850 Aux train loss: 5.510 Val loss: 4.589 Aux val loss: 2.807 Train MAE: 13.343 Val MAE: 19.582 Epoch time: 209.316 seconds \n",
      "Epoch: 272 Train loss: 9.669 Aux train loss: 6.007 Val loss: 4.622 Aux val loss: 2.807 Train MAE: 14.792 Val MAE: 21.088 Epoch time: 209.084 seconds \n",
      "Epoch: 273 Train loss: 9.279 Aux train loss: 5.764 Val loss: 4.626 Aux val loss: 2.805 Train MAE: 14.056 Val MAE: 21.285 Epoch time: 207.574 seconds \n",
      "Epoch: 274 Train loss: 9.199 Aux train loss: 5.705 Val loss: 4.607 Aux val loss: 2.803 Train MAE: 13.784 Val MAE: 20.595 Epoch time: 204.990 seconds \n",
      "Epoch: 275 Train loss: 8.925 Aux train loss: 5.510 Val loss: 4.658 Aux val loss: 2.828 Train MAE: 13.423 Val MAE: 21.933 Epoch time: 205.071 seconds \n",
      "Epoch: 276 Train loss: 9.443 Aux train loss: 5.874 Val loss: 4.611 Aux val loss: 2.803 Train MAE: 13.507 Val MAE: 20.442 Epoch time: 204.715 seconds \n",
      "Epoch: 277 Train loss: 8.932 Aux train loss: 5.541 Val loss: 4.648 Aux val loss: 2.816 Train MAE: 13.163 Val MAE: 22.443 Epoch time: 204.850 seconds \n",
      "Epoch: 278 Train loss: 9.339 Aux train loss: 5.800 Val loss: 4.644 Aux val loss: 2.827 Train MAE: 13.627 Val MAE: 19.218 Epoch time: 204.636 seconds \n",
      "Epoch: 279 Train loss: 9.321 Aux train loss: 5.800 Val loss: 4.650 Aux val loss: 2.830 Train MAE: 13.633 Val MAE: 20.925 Epoch time: 204.686 seconds \n",
      "Epoch: 280 Train loss: 8.872 Aux train loss: 5.517 Val loss: 4.614 Aux val loss: 2.797 Train MAE: 13.257 Val MAE: 19.769 Epoch time: 203.770 seconds \n",
      "Epoch: 281 Train loss: 9.149 Aux train loss: 5.692 Val loss: 4.629 Aux val loss: 2.813 Train MAE: 13.129 Val MAE: 21.159 Epoch time: 205.069 seconds \n",
      "Epoch: 282 Train loss: 8.808 Aux train loss: 5.497 Val loss: 4.620 Aux val loss: 2.802 Train MAE: 13.425 Val MAE: 20.533 Epoch time: 206.104 seconds \n",
      "Epoch: 283 Train loss: 9.537 Aux train loss: 5.919 Val loss: 4.601 Aux val loss: 2.804 Train MAE: 13.367 Val MAE: 19.811 Epoch time: 206.068 seconds \n",
      "Epoch: 284 Train loss: 9.253 Aux train loss: 5.741 Val loss: 4.645 Aux val loss: 2.827 Train MAE: 13.277 Val MAE: 20.342 Epoch time: 204.667 seconds \n",
      "Epoch: 285 Train loss: 9.231 Aux train loss: 5.728 Val loss: 4.662 Aux val loss: 2.832 Train MAE: 13.841 Val MAE: 20.707 Epoch time: 206.121 seconds \n",
      "Epoch: 286 Train loss: 9.319 Aux train loss: 5.793 Val loss: 4.623 Aux val loss: 2.800 Train MAE: 13.830 Val MAE: 19.927 Epoch time: 205.206 seconds \n",
      "Epoch: 287 Train loss: 8.336 Aux train loss: 5.174 Val loss: 4.616 Aux val loss: 2.806 Train MAE: 12.920 Val MAE: 21.351 Epoch time: 205.083 seconds \n",
      "Epoch: 288 Train loss: 9.021 Aux train loss: 5.609 Val loss: 4.627 Aux val loss: 2.802 Train MAE: 13.162 Val MAE: 19.997 Epoch time: 204.933 seconds \n",
      "Epoch: 289 Train loss: 8.883 Aux train loss: 5.543 Val loss: 4.680 Aux val loss: 2.837 Train MAE: 13.116 Val MAE: 24.344 Epoch time: 204.455 seconds \n",
      "Epoch: 290 Train loss: 9.297 Aux train loss: 5.772 Val loss: 4.656 Aux val loss: 2.833 Train MAE: 13.426 Val MAE: 20.444 Epoch time: 206.269 seconds \n",
      "Epoch: 291 Train loss: 8.345 Aux train loss: 5.215 Val loss: 4.657 Aux val loss: 2.810 Train MAE: 13.162 Val MAE: 21.618 Epoch time: 204.426 seconds \n",
      "Epoch: 292 Train loss: 8.874 Aux train loss: 5.516 Val loss: 4.660 Aux val loss: 2.816 Train MAE: 13.717 Val MAE: 22.778 Epoch time: 204.798 seconds \n",
      "Epoch: 293 Train loss: 9.370 Aux train loss: 5.813 Val loss: 4.615 Aux val loss: 2.800 Train MAE: 13.471 Val MAE: 20.534 Epoch time: 205.827 seconds \n",
      "Epoch: 294 Train loss: 8.936 Aux train loss: 5.555 Val loss: 4.598 Aux val loss: 2.799 Train MAE: 13.270 Val MAE: 20.901 Epoch time: 205.251 seconds \n",
      "Epoch: 295 Train loss: 8.834 Aux train loss: 5.497 Val loss: 4.617 Aux val loss: 2.800 Train MAE: 12.900 Val MAE: 19.684 Epoch time: 204.657 seconds \n",
      "Epoch: 296 Train loss: 9.142 Aux train loss: 5.684 Val loss: 4.657 Aux val loss: 2.829 Train MAE: 13.237 Val MAE: 21.403 Epoch time: 207.764 seconds \n",
      "Epoch: 297 Train loss: 8.493 Aux train loss: 5.279 Val loss: 4.662 Aux val loss: 2.821 Train MAE: 12.992 Val MAE: 21.581 Epoch time: 212.426 seconds \n",
      "Epoch: 298 Train loss: 9.587 Aux train loss: 5.970 Val loss: 4.682 Aux val loss: 2.837 Train MAE: 14.389 Val MAE: 22.330 Epoch time: 212.251 seconds \n",
      "Epoch: 299 Train loss: 9.031 Aux train loss: 5.598 Val loss: 4.653 Aux val loss: 2.816 Train MAE: 13.325 Val MAE: 22.877 Epoch time: 211.247 seconds \n",
      "Epoch: 300 Train loss: 9.291 Aux train loss: 5.794 Val loss: 4.646 Aux val loss: 2.810 Train MAE: 13.539 Val MAE: 21.470 Epoch time: 204.942 seconds \n",
      "Epoch: 301 Train loss: 9.596 Aux train loss: 5.967 Val loss: 4.657 Aux val loss: 2.820 Train MAE: 14.478 Val MAE: 19.964 Epoch time: 199.673 seconds \n",
      "Epoch: 302 Train loss: 9.052 Aux train loss: 5.630 Val loss: 4.639 Aux val loss: 2.818 Train MAE: 13.180 Val MAE: 20.493 Epoch time: 200.729 seconds \n",
      "Epoch: 303 Train loss: 9.080 Aux train loss: 5.648 Val loss: 4.678 Aux val loss: 2.834 Train MAE: 12.917 Val MAE: 19.984 Epoch time: 201.149 seconds \n",
      "Epoch: 304 Train loss: 8.288 Aux train loss: 5.162 Val loss: 4.650 Aux val loss: 2.816 Train MAE: 12.764 Val MAE: 20.760 Epoch time: 202.015 seconds \n",
      "Epoch: 305 Train loss: 9.609 Aux train loss: 5.962 Val loss: 4.670 Aux val loss: 2.824 Train MAE: 13.864 Val MAE: 20.075 Epoch time: 198.437 seconds \n",
      "Epoch: 306 Train loss: 8.755 Aux train loss: 5.459 Val loss: 4.653 Aux val loss: 2.831 Train MAE: 12.970 Val MAE: 21.698 Epoch time: 198.224 seconds \n",
      "Epoch: 307 Train loss: 9.037 Aux train loss: 5.620 Val loss: 4.616 Aux val loss: 2.801 Train MAE: 12.790 Val MAE: 22.271 Epoch time: 198.655 seconds \n",
      "Epoch: 308 Train loss: 9.159 Aux train loss: 5.695 Val loss: 4.621 Aux val loss: 2.807 Train MAE: 13.222 Val MAE: 20.841 Epoch time: 197.402 seconds \n",
      "Epoch: 309 Train loss: 8.625 Aux train loss: 5.375 Val loss: 4.672 Aux val loss: 2.816 Train MAE: 12.946 Val MAE: 23.902 Epoch time: 197.161 seconds \n",
      "Epoch: 310 Train loss: 8.774 Aux train loss: 5.455 Val loss: 4.671 Aux val loss: 2.835 Train MAE: 12.699 Val MAE: 21.909 Epoch time: 198.328 seconds \n",
      "Epoch: 311 Train loss: 8.400 Aux train loss: 5.223 Val loss: 4.681 Aux val loss: 2.836 Train MAE: 13.166 Val MAE: 20.999 Epoch time: 198.136 seconds \n",
      "Epoch: 312 Train loss: 8.891 Aux train loss: 5.532 Val loss: 4.628 Aux val loss: 2.802 Train MAE: 12.237 Val MAE: 21.167 Epoch time: 198.754 seconds \n",
      "Epoch: 313 Train loss: 9.013 Aux train loss: 5.607 Val loss: 4.689 Aux val loss: 2.827 Train MAE: 12.925 Val MAE: 20.216 Epoch time: 198.388 seconds \n",
      "Epoch: 314 Train loss: 9.234 Aux train loss: 5.742 Val loss: 4.651 Aux val loss: 2.816 Train MAE: 12.932 Val MAE: 21.353 Epoch time: 198.093 seconds \n",
      "Epoch: 315 Train loss: 9.086 Aux train loss: 5.657 Val loss: 4.636 Aux val loss: 2.805 Train MAE: 13.311 Val MAE: 20.410 Epoch time: 198.651 seconds \n",
      "Epoch: 316 Train loss: 9.084 Aux train loss: 5.654 Val loss: 4.686 Aux val loss: 2.834 Train MAE: 12.456 Val MAE: 19.027 Epoch time: 198.079 seconds best\n",
      "Epoch: 317 Train loss: 8.665 Aux train loss: 5.388 Val loss: 4.608 Aux val loss: 2.790 Train MAE: 12.962 Val MAE: 22.337 Epoch time: 198.029 seconds \n",
      "Epoch: 318 Train loss: 9.237 Aux train loss: 5.743 Val loss: 4.677 Aux val loss: 2.823 Train MAE: 12.679 Val MAE: 27.003 Epoch time: 198.114 seconds \n",
      "Epoch: 319 Train loss: 8.932 Aux train loss: 5.556 Val loss: 4.597 Aux val loss: 2.797 Train MAE: 13.418 Val MAE: 21.398 Epoch time: 198.573 seconds \n",
      "Epoch: 320 Train loss: 8.687 Aux train loss: 5.401 Val loss: 4.629 Aux val loss: 2.805 Train MAE: 12.354 Val MAE: 21.826 Epoch time: 198.241 seconds \n",
      "Epoch: 321 Train loss: 9.677 Aux train loss: 5.995 Val loss: 4.628 Aux val loss: 2.812 Train MAE: 13.159 Val MAE: 21.652 Epoch time: 198.388 seconds \n",
      "Epoch: 322 Train loss: 8.874 Aux train loss: 5.530 Val loss: 4.654 Aux val loss: 2.808 Train MAE: 12.672 Val MAE: 23.068 Epoch time: 198.281 seconds \n",
      "Epoch: 323 Train loss: 8.436 Aux train loss: 5.239 Val loss: 4.679 Aux val loss: 2.827 Train MAE: 13.300 Val MAE: 21.821 Epoch time: 198.460 seconds \n",
      "Epoch: 324 Train loss: 9.152 Aux train loss: 5.721 Val loss: 4.647 Aux val loss: 2.814 Train MAE: 12.979 Val MAE: 19.626 Epoch time: 198.485 seconds \n",
      "Epoch: 325 Train loss: 9.601 Aux train loss: 5.972 Val loss: 4.674 Aux val loss: 2.836 Train MAE: 14.448 Val MAE: 20.265 Epoch time: 198.304 seconds \n",
      "Epoch: 326 Train loss: 9.182 Aux train loss: 5.712 Val loss: 4.649 Aux val loss: 2.815 Train MAE: 12.504 Val MAE: 24.885 Epoch time: 197.819 seconds \n",
      "Epoch: 327 Train loss: 8.873 Aux train loss: 5.506 Val loss: 4.681 Aux val loss: 2.840 Train MAE: 12.258 Val MAE: 22.085 Epoch time: 197.997 seconds \n",
      "Epoch: 328 Train loss: 9.016 Aux train loss: 5.609 Val loss: 4.680 Aux val loss: 2.843 Train MAE: 13.109 Val MAE: 22.083 Epoch time: 198.591 seconds \n",
      "Epoch: 329 Train loss: 9.174 Aux train loss: 5.716 Val loss: 4.596 Aux val loss: 2.795 Train MAE: 12.757 Val MAE: 21.879 Epoch time: 198.156 seconds \n",
      "Epoch: 330 Train loss: 9.304 Aux train loss: 5.763 Val loss: 4.648 Aux val loss: 2.821 Train MAE: 13.390 Val MAE: 21.117 Epoch time: 197.992 seconds \n",
      "Epoch: 331 Train loss: 8.754 Aux train loss: 5.458 Val loss: 4.609 Aux val loss: 2.808 Train MAE: 12.664 Val MAE: 20.885 Epoch time: 197.910 seconds \n",
      "Epoch: 332 Train loss: 9.424 Aux train loss: 5.868 Val loss: 4.623 Aux val loss: 2.809 Train MAE: 12.237 Val MAE: 21.528 Epoch time: 198.481 seconds \n",
      "Epoch: 333 Train loss: 9.284 Aux train loss: 5.813 Val loss: 4.653 Aux val loss: 2.811 Train MAE: 12.953 Val MAE: 20.211 Epoch time: 198.242 seconds \n",
      "Epoch: 334 Train loss: 8.973 Aux train loss: 5.578 Val loss: 4.597 Aux val loss: 2.790 Train MAE: 12.768 Val MAE: 21.879 Epoch time: 198.653 seconds \n",
      "Epoch: 335 Train loss: 8.655 Aux train loss: 5.379 Val loss: 4.629 Aux val loss: 2.796 Train MAE: 12.770 Val MAE: 22.757 Epoch time: 198.056 seconds \n",
      "Epoch: 336 Train loss: 8.434 Aux train loss: 5.254 Val loss: 4.606 Aux val loss: 2.800 Train MAE: 12.652 Val MAE: 21.187 Epoch time: 198.730 seconds \n",
      "Epoch: 337 Train loss: 9.026 Aux train loss: 5.626 Val loss: 4.608 Aux val loss: 2.797 Train MAE: 12.496 Val MAE: 22.559 Epoch time: 198.502 seconds \n",
      "Epoch: 338 Train loss: 9.361 Aux train loss: 5.828 Val loss: 4.681 Aux val loss: 2.834 Train MAE: 13.229 Val MAE: 21.592 Epoch time: 198.813 seconds \n",
      "Epoch: 339 Train loss: 8.810 Aux train loss: 5.488 Val loss: 4.634 Aux val loss: 2.805 Train MAE: 12.366 Val MAE: 21.425 Epoch time: 197.700 seconds \n",
      "Epoch: 340 Train loss: 9.617 Aux train loss: 5.968 Val loss: 4.688 Aux val loss: 2.835 Train MAE: 12.832 Val MAE: 21.160 Epoch time: 198.570 seconds \n",
      "Epoch: 341 Train loss: 8.784 Aux train loss: 5.461 Val loss: 4.592 Aux val loss: 2.790 Train MAE: 12.419 Val MAE: 22.185 Epoch time: 197.967 seconds \n",
      "Epoch: 342 Train loss: 8.711 Aux train loss: 5.418 Val loss: 4.624 Aux val loss: 2.798 Train MAE: 12.563 Val MAE: 22.650 Epoch time: 196.987 seconds \n",
      "Epoch: 343 Train loss: 8.813 Aux train loss: 5.490 Val loss: 4.630 Aux val loss: 2.797 Train MAE: 12.530 Val MAE: 21.876 Epoch time: 197.589 seconds \n",
      "Epoch: 344 Train loss: 8.417 Aux train loss: 5.254 Val loss: 4.611 Aux val loss: 2.789 Train MAE: 12.059 Val MAE: 21.285 Epoch time: 197.041 seconds \n",
      "Epoch: 345 Train loss: 8.625 Aux train loss: 5.386 Val loss: 4.618 Aux val loss: 2.806 Train MAE: 13.071 Val MAE: 22.955 Epoch time: 197.783 seconds \n",
      "Epoch: 346 Train loss: 8.590 Aux train loss: 5.360 Val loss: 4.655 Aux val loss: 2.814 Train MAE: 12.261 Val MAE: 23.855 Epoch time: 197.162 seconds \n",
      "Epoch: 347 Train loss: 8.665 Aux train loss: 5.392 Val loss: 4.628 Aux val loss: 2.800 Train MAE: 12.082 Val MAE: 21.422 Epoch time: 197.110 seconds \n",
      "Epoch: 348 Train loss: 9.177 Aux train loss: 5.709 Val loss: 4.582 Aux val loss: 2.784 Train MAE: 12.910 Val MAE: 21.490 Epoch time: 197.387 seconds \n",
      "Epoch: 349 Train loss: 8.345 Aux train loss: 5.219 Val loss: 4.600 Aux val loss: 2.794 Train MAE: 12.106 Val MAE: 21.894 Epoch time: 197.636 seconds \n",
      "Epoch: 350 Train loss: 8.843 Aux train loss: 5.506 Val loss: 4.619 Aux val loss: 2.808 Train MAE: 12.786 Val MAE: 25.714 Epoch time: 197.104 seconds \n",
      "Epoch: 351 Train loss: 8.375 Aux train loss: 5.209 Val loss: 4.621 Aux val loss: 2.806 Train MAE: 12.856 Val MAE: 19.613 Epoch time: 196.950 seconds \n",
      "Epoch: 352 Train loss: 9.317 Aux train loss: 5.732 Val loss: 4.602 Aux val loss: 2.788 Train MAE: 13.294 Val MAE: 21.139 Epoch time: 197.629 seconds \n",
      "Epoch: 353 Train loss: 9.067 Aux train loss: 5.654 Val loss: 4.670 Aux val loss: 2.832 Train MAE: 12.890 Val MAE: 20.760 Epoch time: 197.195 seconds \n",
      "Epoch: 354 Train loss: 8.731 Aux train loss: 5.451 Val loss: 4.621 Aux val loss: 2.794 Train MAE: 12.316 Val MAE: 21.838 Epoch time: 197.153 seconds \n",
      "Epoch: 355 Train loss: 8.530 Aux train loss: 5.332 Val loss: 4.620 Aux val loss: 2.807 Train MAE: 12.973 Val MAE: 20.573 Epoch time: 197.379 seconds \n",
      "Epoch: 356 Train loss: 8.497 Aux train loss: 5.259 Val loss: 4.636 Aux val loss: 2.803 Train MAE: 12.266 Val MAE: 22.141 Epoch time: 197.958 seconds \n",
      "Epoch: 357 Train loss: 9.346 Aux train loss: 5.814 Val loss: 4.597 Aux val loss: 2.797 Train MAE: 11.867 Val MAE: 20.246 Epoch time: 197.767 seconds \n",
      "Epoch: 358 Train loss: 8.636 Aux train loss: 5.377 Val loss: 4.622 Aux val loss: 2.802 Train MAE: 12.277 Val MAE: 19.885 Epoch time: 197.773 seconds \n",
      "Epoch: 359 Train loss: 9.022 Aux train loss: 5.628 Val loss: 4.628 Aux val loss: 2.810 Train MAE: 12.786 Val MAE: 22.173 Epoch time: 197.167 seconds \n",
      "Epoch: 360 Train loss: 8.929 Aux train loss: 5.552 Val loss: 4.636 Aux val loss: 2.808 Train MAE: 12.786 Val MAE: 23.110 Epoch time: 197.522 seconds \n",
      "Epoch: 361 Train loss: 8.624 Aux train loss: 5.369 Val loss: 4.608 Aux val loss: 2.786 Train MAE: 12.210 Val MAE: 20.548 Epoch time: 196.988 seconds \n",
      "Epoch: 362 Train loss: 9.433 Aux train loss: 5.873 Val loss: 4.619 Aux val loss: 2.800 Train MAE: 12.831 Val MAE: 21.472 Epoch time: 197.529 seconds \n",
      "Epoch: 363 Train loss: 8.319 Aux train loss: 5.201 Val loss: 4.654 Aux val loss: 2.810 Train MAE: 12.357 Val MAE: 20.932 Epoch time: 197.508 seconds \n",
      "Epoch: 364 Train loss: 8.359 Aux train loss: 5.196 Val loss: 4.660 Aux val loss: 2.817 Train MAE: 12.359 Val MAE: 25.501 Epoch time: 197.576 seconds \n",
      "Epoch: 365 Train loss: 8.528 Aux train loss: 5.325 Val loss: 4.635 Aux val loss: 2.808 Train MAE: 12.002 Val MAE: 20.748 Epoch time: 197.194 seconds \n",
      "Epoch: 366 Train loss: 8.142 Aux train loss: 5.083 Val loss: 4.632 Aux val loss: 2.810 Train MAE: 12.765 Val MAE: 22.044 Epoch time: 197.407 seconds \n",
      "Epoch: 367 Train loss: 8.837 Aux train loss: 5.517 Val loss: 4.636 Aux val loss: 2.806 Train MAE: 13.239 Val MAE: 19.748 Epoch time: 197.484 seconds \n",
      "Epoch: 368 Train loss: 8.992 Aux train loss: 5.607 Val loss: 4.638 Aux val loss: 2.803 Train MAE: 13.103 Val MAE: 21.062 Epoch time: 197.215 seconds \n",
      "Epoch: 369 Train loss: 8.128 Aux train loss: 5.070 Val loss: 4.629 Aux val loss: 2.798 Train MAE: 12.165 Val MAE: 20.624 Epoch time: 197.721 seconds \n",
      "Epoch: 370 Train loss: 8.634 Aux train loss: 5.390 Val loss: 4.642 Aux val loss: 2.800 Train MAE: 12.172 Val MAE: 23.500 Epoch time: 196.445 seconds \n",
      "Epoch: 371 Train loss: 9.029 Aux train loss: 5.635 Val loss: 4.621 Aux val loss: 2.801 Train MAE: 13.458 Val MAE: 20.653 Epoch time: 196.897 seconds \n",
      "Epoch: 372 Train loss: 8.692 Aux train loss: 5.421 Val loss: 4.600 Aux val loss: 2.780 Train MAE: 11.684 Val MAE: 20.458 Epoch time: 198.065 seconds \n",
      "Epoch: 373 Train loss: 8.949 Aux train loss: 5.592 Val loss: 4.679 Aux val loss: 2.802 Train MAE: 13.332 Val MAE: 21.592 Epoch time: 197.287 seconds \n",
      "Epoch: 374 Train loss: 8.550 Aux train loss: 5.323 Val loss: 4.591 Aux val loss: 2.786 Train MAE: 12.311 Val MAE: 20.560 Epoch time: 197.475 seconds \n",
      "Epoch: 375 Train loss: 9.072 Aux train loss: 5.667 Val loss: 4.620 Aux val loss: 2.797 Train MAE: 12.440 Val MAE: 21.074 Epoch time: 196.569 seconds \n",
      "Epoch: 376 Train loss: 8.582 Aux train loss: 5.357 Val loss: 4.567 Aux val loss: 2.778 Train MAE: 11.691 Val MAE: 21.034 Epoch time: 196.699 seconds \n",
      "Epoch: 377 Train loss: 9.137 Aux train loss: 5.710 Val loss: 4.605 Aux val loss: 2.795 Train MAE: 12.586 Val MAE: 19.898 Epoch time: 197.604 seconds \n",
      "Epoch: 378 Train loss: 8.690 Aux train loss: 5.428 Val loss: 4.609 Aux val loss: 2.788 Train MAE: 12.083 Val MAE: 20.898 Epoch time: 196.955 seconds \n",
      "Epoch: 379 Train loss: 9.012 Aux train loss: 5.617 Val loss: 4.613 Aux val loss: 2.789 Train MAE: 12.496 Val MAE: 21.404 Epoch time: 197.613 seconds \n",
      "Epoch: 380 Train loss: 8.581 Aux train loss: 5.371 Val loss: 4.675 Aux val loss: 2.809 Train MAE: 11.521 Val MAE: 20.900 Epoch time: 197.604 seconds \n",
      "Epoch: 381 Train loss: 8.337 Aux train loss: 5.188 Val loss: 4.662 Aux val loss: 2.822 Train MAE: 12.376 Val MAE: 21.943 Epoch time: 197.903 seconds \n",
      "Epoch: 382 Train loss: 8.865 Aux train loss: 5.524 Val loss: 4.674 Aux val loss: 2.812 Train MAE: 12.008 Val MAE: 21.478 Epoch time: 197.219 seconds \n",
      "Epoch: 383 Train loss: 9.191 Aux train loss: 5.739 Val loss: 4.625 Aux val loss: 2.804 Train MAE: 12.588 Val MAE: 21.612 Epoch time: 198.291 seconds \n",
      "Epoch: 384 Train loss: 9.067 Aux train loss: 5.657 Val loss: 4.627 Aux val loss: 2.788 Train MAE: 12.048 Val MAE: 22.661 Epoch time: 197.430 seconds \n",
      "Epoch: 385 Train loss: 8.667 Aux train loss: 5.421 Val loss: 4.579 Aux val loss: 2.778 Train MAE: 11.639 Val MAE: 21.137 Epoch time: 196.592 seconds \n",
      "Epoch: 386 Train loss: 8.944 Aux train loss: 5.565 Val loss: 4.585 Aux val loss: 2.779 Train MAE: 12.410 Val MAE: 20.377 Epoch time: 197.027 seconds \n",
      "Epoch: 387 Train loss: 8.161 Aux train loss: 5.083 Val loss: 4.582 Aux val loss: 2.771 Train MAE: 11.682 Val MAE: 21.951 Epoch time: 197.152 seconds \n",
      "Epoch: 388 Train loss: 8.803 Aux train loss: 5.484 Val loss: 4.633 Aux val loss: 2.795 Train MAE: 12.287 Val MAE: 20.562 Epoch time: 196.893 seconds \n",
      "Epoch: 389 Train loss: 9.157 Aux train loss: 5.687 Val loss: 4.609 Aux val loss: 2.790 Train MAE: 12.359 Val MAE: 21.550 Epoch time: 197.653 seconds \n",
      "Epoch: 390 Train loss: 8.863 Aux train loss: 5.538 Val loss: 4.603 Aux val loss: 2.791 Train MAE: 12.369 Val MAE: 23.398 Epoch time: 197.047 seconds \n",
      "Epoch: 391 Train loss: 8.317 Aux train loss: 5.185 Val loss: 4.625 Aux val loss: 2.797 Train MAE: 12.268 Val MAE: 20.186 Epoch time: 198.157 seconds \n",
      "Epoch: 392 Train loss: 8.870 Aux train loss: 5.521 Val loss: 4.598 Aux val loss: 2.789 Train MAE: 12.397 Val MAE: 21.544 Epoch time: 197.758 seconds \n",
      "Epoch: 393 Train loss: 7.943 Aux train loss: 4.969 Val loss: 4.629 Aux val loss: 2.782 Train MAE: 11.406 Val MAE: 21.878 Epoch time: 198.420 seconds \n",
      "Epoch: 394 Train loss: 8.578 Aux train loss: 5.352 Val loss: 4.597 Aux val loss: 2.793 Train MAE: 12.336 Val MAE: 21.260 Epoch time: 198.137 seconds \n",
      "Epoch: 395 Train loss: 8.563 Aux train loss: 5.350 Val loss: 4.641 Aux val loss: 2.795 Train MAE: 11.634 Val MAE: 21.381 Epoch time: 198.231 seconds \n",
      "Epoch: 396 Train loss: 9.162 Aux train loss: 5.709 Val loss: 4.654 Aux val loss: 2.813 Train MAE: 12.071 Val MAE: 23.298 Epoch time: 198.081 seconds \n",
      "Epoch: 397 Train loss: 8.751 Aux train loss: 5.459 Val loss: 4.611 Aux val loss: 2.778 Train MAE: 12.346 Val MAE: 22.313 Epoch time: 198.552 seconds \n",
      "Epoch: 398 Train loss: 8.779 Aux train loss: 5.469 Val loss: 4.613 Aux val loss: 2.785 Train MAE: 12.094 Val MAE: 19.390 Epoch time: 198.872 seconds \n",
      "Epoch: 399 Train loss: 8.661 Aux train loss: 5.357 Val loss: 4.640 Aux val loss: 2.804 Train MAE: 12.198 Val MAE: 21.612 Epoch time: 198.602 seconds \n",
      "Epoch: 400 Train loss: 8.522 Aux train loss: 5.339 Val loss: 4.651 Aux val loss: 2.807 Train MAE: 11.735 Val MAE: 24.680 Epoch time: 197.747 seconds \n",
      "Epoch: 401 Train loss: 8.728 Aux train loss: 5.438 Val loss: 4.621 Aux val loss: 2.788 Train MAE: 10.918 Val MAE: 22.172 Epoch time: 197.849 seconds \n",
      "Epoch: 402 Train loss: 8.799 Aux train loss: 5.480 Val loss: 4.607 Aux val loss: 2.784 Train MAE: 10.039 Val MAE: 21.192 Epoch time: 198.324 seconds \n",
      "Epoch: 403 Train loss: 8.304 Aux train loss: 5.195 Val loss: 4.578 Aux val loss: 2.767 Train MAE: 10.352 Val MAE: 20.917 Epoch time: 198.238 seconds \n",
      "Epoch: 404 Train loss: 8.793 Aux train loss: 5.489 Val loss: 4.584 Aux val loss: 2.771 Train MAE: 10.546 Val MAE: 20.136 Epoch time: 198.738 seconds \n",
      "Epoch: 405 Train loss: 8.677 Aux train loss: 5.425 Val loss: 4.597 Aux val loss: 2.778 Train MAE: 10.424 Val MAE: 20.779 Epoch time: 198.131 seconds \n",
      "Epoch: 406 Train loss: 9.292 Aux train loss: 5.800 Val loss: 4.593 Aux val loss: 2.777 Train MAE: 10.732 Val MAE: 20.858 Epoch time: 197.732 seconds \n",
      "Epoch: 407 Train loss: 8.468 Aux train loss: 5.302 Val loss: 4.595 Aux val loss: 2.776 Train MAE: 9.992 Val MAE: 21.129 Epoch time: 198.466 seconds \n",
      "Epoch: 408 Train loss: 8.929 Aux train loss: 5.582 Val loss: 4.578 Aux val loss: 2.764 Train MAE: 10.630 Val MAE: 20.572 Epoch time: 198.725 seconds \n",
      "Epoch: 409 Train loss: 8.673 Aux train loss: 5.429 Val loss: 4.579 Aux val loss: 2.770 Train MAE: 10.251 Val MAE: 20.405 Epoch time: 198.135 seconds \n",
      "Epoch: 410 Train loss: 8.293 Aux train loss: 5.183 Val loss: 4.585 Aux val loss: 2.772 Train MAE: 10.079 Val MAE: 19.978 Epoch time: 198.131 seconds \n",
      "Epoch: 411 Train loss: 8.218 Aux train loss: 5.151 Val loss: 4.578 Aux val loss: 2.763 Train MAE: 9.926 Val MAE: 20.893 Epoch time: 197.981 seconds \n",
      "Epoch: 412 Train loss: 8.570 Aux train loss: 5.353 Val loss: 4.608 Aux val loss: 2.779 Train MAE: 9.918 Val MAE: 20.294 Epoch time: 198.402 seconds \n",
      "Epoch: 413 Train loss: 8.192 Aux train loss: 5.127 Val loss: 4.575 Aux val loss: 2.764 Train MAE: 9.888 Val MAE: 20.995 Epoch time: 199.158 seconds \n",
      "Epoch: 414 Train loss: 8.114 Aux train loss: 5.088 Val loss: 4.582 Aux val loss: 2.769 Train MAE: 10.508 Val MAE: 20.686 Epoch time: 197.635 seconds \n",
      "Epoch: 415 Train loss: 8.456 Aux train loss: 5.281 Val loss: 4.570 Aux val loss: 2.760 Train MAE: 10.024 Val MAE: 20.188 Epoch time: 198.495 seconds \n",
      "Epoch: 416 Train loss: 7.890 Aux train loss: 4.944 Val loss: 4.586 Aux val loss: 2.765 Train MAE: 9.870 Val MAE: 21.326 Epoch time: 198.927 seconds \n",
      "Epoch: 417 Train loss: 9.144 Aux train loss: 5.723 Val loss: 4.599 Aux val loss: 2.781 Train MAE: 10.129 Val MAE: 20.414 Epoch time: 198.321 seconds \n",
      "Epoch: 418 Train loss: 8.719 Aux train loss: 5.449 Val loss: 4.586 Aux val loss: 2.774 Train MAE: 10.221 Val MAE: 19.475 Epoch time: 198.468 seconds \n",
      "Epoch: 419 Train loss: 8.781 Aux train loss: 5.498 Val loss: 4.586 Aux val loss: 2.769 Train MAE: 9.785 Val MAE: 20.176 Epoch time: 197.334 seconds \n",
      "Epoch: 420 Train loss: 8.474 Aux train loss: 5.293 Val loss: 4.583 Aux val loss: 2.769 Train MAE: 9.805 Val MAE: 20.110 Epoch time: 198.252 seconds \n",
      "Epoch: 421 Train loss: 8.206 Aux train loss: 5.127 Val loss: 4.600 Aux val loss: 2.775 Train MAE: 9.674 Val MAE: 20.267 Epoch time: 198.023 seconds \n",
      "Epoch: 422 Train loss: 8.414 Aux train loss: 5.265 Val loss: 4.599 Aux val loss: 2.771 Train MAE: 9.896 Val MAE: 21.240 Epoch time: 198.398 seconds \n",
      "Epoch: 423 Train loss: 8.693 Aux train loss: 5.441 Val loss: 4.602 Aux val loss: 2.774 Train MAE: 10.422 Val MAE: 23.050 Epoch time: 197.847 seconds \n",
      "Epoch: 424 Train loss: 8.355 Aux train loss: 5.227 Val loss: 4.579 Aux val loss: 2.764 Train MAE: 10.259 Val MAE: 20.467 Epoch time: 197.769 seconds \n",
      "Epoch: 425 Train loss: 9.032 Aux train loss: 5.649 Val loss: 4.575 Aux val loss: 2.764 Train MAE: 10.385 Val MAE: 20.196 Epoch time: 198.030 seconds \n",
      "Epoch: 426 Train loss: 8.236 Aux train loss: 5.144 Val loss: 4.593 Aux val loss: 2.765 Train MAE: 9.652 Val MAE: 20.533 Epoch time: 198.733 seconds \n",
      "Epoch: 427 Train loss: 8.544 Aux train loss: 5.342 Val loss: 4.590 Aux val loss: 2.775 Train MAE: 10.518 Val MAE: 20.465 Epoch time: 198.137 seconds \n",
      "Epoch: 428 Train loss: 8.208 Aux train loss: 5.156 Val loss: 4.580 Aux val loss: 2.768 Train MAE: 10.178 Val MAE: 21.202 Epoch time: 197.757 seconds \n",
      "Epoch: 429 Train loss: 8.556 Aux train loss: 5.340 Val loss: 4.585 Aux val loss: 2.771 Train MAE: 9.981 Val MAE: 20.381 Epoch time: 197.201 seconds \n",
      "Epoch: 430 Train loss: 8.377 Aux train loss: 5.240 Val loss: 4.610 Aux val loss: 2.779 Train MAE: 10.159 Val MAE: 20.914 Epoch time: 198.242 seconds \n",
      "Epoch: 431 Train loss: 8.630 Aux train loss: 5.389 Val loss: 4.591 Aux val loss: 2.769 Train MAE: 9.600 Val MAE: 20.179 Epoch time: 198.081 seconds \n",
      "Epoch: 432 Train loss: 8.174 Aux train loss: 5.120 Val loss: 4.600 Aux val loss: 2.772 Train MAE: 10.200 Val MAE: 20.085 Epoch time: 198.018 seconds \n",
      "Epoch: 433 Train loss: 8.313 Aux train loss: 5.207 Val loss: 4.599 Aux val loss: 2.776 Train MAE: 9.867 Val MAE: 21.478 Epoch time: 198.159 seconds \n",
      "Epoch: 434 Train loss: 8.762 Aux train loss: 5.466 Val loss: 4.578 Aux val loss: 2.772 Train MAE: 9.807 Val MAE: 20.032 Epoch time: 198.066 seconds \n",
      "Epoch: 435 Train loss: 8.584 Aux train loss: 5.371 Val loss: 4.571 Aux val loss: 2.761 Train MAE: 9.820 Val MAE: 20.718 Epoch time: 198.857 seconds \n",
      "Epoch: 436 Train loss: 8.398 Aux train loss: 5.252 Val loss: 4.590 Aux val loss: 2.769 Train MAE: 9.972 Val MAE: 21.327 Epoch time: 197.633 seconds \n",
      "Epoch: 437 Train loss: 8.318 Aux train loss: 5.198 Val loss: 4.590 Aux val loss: 2.778 Train MAE: 9.654 Val MAE: 20.089 Epoch time: 197.506 seconds \n",
      "Epoch: 438 Train loss: 8.698 Aux train loss: 5.442 Val loss: 4.579 Aux val loss: 2.769 Train MAE: 10.473 Val MAE: 20.390 Epoch time: 198.006 seconds \n",
      "Epoch: 439 Train loss: 7.914 Aux train loss: 4.948 Val loss: 4.588 Aux val loss: 2.768 Train MAE: 10.285 Val MAE: 21.088 Epoch time: 198.472 seconds \n",
      "Epoch: 440 Train loss: 8.242 Aux train loss: 5.145 Val loss: 4.579 Aux val loss: 2.770 Train MAE: 10.086 Val MAE: 19.573 Epoch time: 198.543 seconds \n",
      "Epoch: 441 Train loss: 8.558 Aux train loss: 5.361 Val loss: 4.586 Aux val loss: 2.770 Train MAE: 9.376 Val MAE: 20.698 Epoch time: 197.894 seconds \n",
      "Epoch: 442 Train loss: 8.728 Aux train loss: 5.470 Val loss: 4.605 Aux val loss: 2.778 Train MAE: 9.678 Val MAE: 20.313 Epoch time: 198.004 seconds \n",
      "Epoch: 443 Train loss: 8.998 Aux train loss: 5.622 Val loss: 4.590 Aux val loss: 2.771 Train MAE: 9.959 Val MAE: 19.767 Epoch time: 199.051 seconds \n",
      "Epoch: 444 Train loss: 8.844 Aux train loss: 5.524 Val loss: 4.588 Aux val loss: 2.772 Train MAE: 10.026 Val MAE: 20.547 Epoch time: 197.798 seconds \n",
      "Epoch: 445 Train loss: 8.380 Aux train loss: 5.234 Val loss: 4.599 Aux val loss: 2.776 Train MAE: 9.890 Val MAE: 21.125 Epoch time: 198.643 seconds \n",
      "Epoch: 446 Train loss: 8.799 Aux train loss: 5.510 Val loss: 4.566 Aux val loss: 2.756 Train MAE: 9.677 Val MAE: 20.439 Epoch time: 198.585 seconds \n",
      "Epoch: 447 Train loss: 8.912 Aux train loss: 5.568 Val loss: 4.588 Aux val loss: 2.769 Train MAE: 10.273 Val MAE: 20.879 Epoch time: 197.769 seconds \n",
      "Epoch: 448 Train loss: 8.278 Aux train loss: 5.187 Val loss: 4.591 Aux val loss: 2.768 Train MAE: 10.005 Val MAE: 20.814 Epoch time: 197.867 seconds \n",
      "Epoch: 449 Train loss: 8.704 Aux train loss: 5.453 Val loss: 4.581 Aux val loss: 2.766 Train MAE: 10.032 Val MAE: 20.582 Epoch time: 197.551 seconds \n",
      "Epoch: 450 Train loss: 8.650 Aux train loss: 5.404 Val loss: 4.565 Aux val loss: 2.754 Train MAE: 10.062 Val MAE: 20.681 Epoch time: 198.234 seconds \n",
      "Epoch: 451 Train loss: 8.856 Aux train loss: 5.543 Val loss: 4.578 Aux val loss: 2.763 Train MAE: 9.636 Val MAE: 20.054 Epoch time: 197.202 seconds \n",
      "Epoch: 452 Train loss: 8.231 Aux train loss: 5.150 Val loss: 4.574 Aux val loss: 2.761 Train MAE: 9.685 Val MAE: 20.646 Epoch time: 198.070 seconds \n",
      "Epoch: 453 Train loss: 8.365 Aux train loss: 5.229 Val loss: 4.568 Aux val loss: 2.763 Train MAE: 10.267 Val MAE: 20.485 Epoch time: 197.811 seconds \n",
      "Epoch: 454 Train loss: 8.336 Aux train loss: 5.209 Val loss: 4.586 Aux val loss: 2.766 Train MAE: 9.886 Val MAE: 20.342 Epoch time: 197.945 seconds \n",
      "Epoch: 455 Train loss: 8.801 Aux train loss: 5.509 Val loss: 4.601 Aux val loss: 2.773 Train MAE: 9.733 Val MAE: 21.723 Epoch time: 197.303 seconds \n",
      "Epoch: 456 Train loss: 8.964 Aux train loss: 5.631 Val loss: 4.585 Aux val loss: 2.768 Train MAE: 10.185 Val MAE: 21.729 Epoch time: 198.302 seconds \n",
      "Epoch: 457 Train loss: 8.710 Aux train loss: 5.440 Val loss: 4.581 Aux val loss: 2.764 Train MAE: 10.299 Val MAE: 21.317 Epoch time: 198.022 seconds \n",
      "Epoch: 458 Train loss: 8.437 Aux train loss: 5.295 Val loss: 4.593 Aux val loss: 2.775 Train MAE: 9.757 Val MAE: 20.975 Epoch time: 197.796 seconds \n",
      "Epoch: 459 Train loss: 7.830 Aux train loss: 4.905 Val loss: 4.581 Aux val loss: 2.767 Train MAE: 9.483 Val MAE: 20.229 Epoch time: 197.523 seconds \n",
      "Epoch: 460 Train loss: 8.644 Aux train loss: 5.410 Val loss: 4.595 Aux val loss: 2.772 Train MAE: 9.906 Val MAE: 21.607 Epoch time: 197.475 seconds \n",
      "Epoch: 461 Train loss: 8.491 Aux train loss: 5.296 Val loss: 4.569 Aux val loss: 2.758 Train MAE: 10.237 Val MAE: 21.170 Epoch time: 197.892 seconds \n",
      "Epoch: 462 Train loss: 8.451 Aux train loss: 5.291 Val loss: 4.576 Aux val loss: 2.760 Train MAE: 9.977 Val MAE: 20.619 Epoch time: 197.432 seconds \n",
      "Epoch: 463 Train loss: 7.964 Aux train loss: 4.991 Val loss: 4.586 Aux val loss: 2.772 Train MAE: 9.813 Val MAE: 20.136 Epoch time: 197.817 seconds \n",
      "Epoch: 464 Train loss: 8.706 Aux train loss: 5.464 Val loss: 4.578 Aux val loss: 2.768 Train MAE: 10.078 Val MAE: 21.074 Epoch time: 197.655 seconds \n",
      "Epoch: 465 Train loss: 8.851 Aux train loss: 5.543 Val loss: 4.571 Aux val loss: 2.764 Train MAE: 10.311 Val MAE: 19.775 Epoch time: 197.458 seconds \n",
      "Epoch: 466 Train loss: 8.414 Aux train loss: 5.264 Val loss: 4.574 Aux val loss: 2.768 Train MAE: 10.357 Val MAE: 21.038 Epoch time: 197.997 seconds \n",
      "Epoch: 467 Train loss: 8.634 Aux train loss: 5.406 Val loss: 4.584 Aux val loss: 2.768 Train MAE: 10.119 Val MAE: 20.332 Epoch time: 198.620 seconds \n",
      "Epoch: 468 Train loss: 8.028 Aux train loss: 5.035 Val loss: 4.570 Aux val loss: 2.760 Train MAE: 9.607 Val MAE: 19.546 Epoch time: 196.828 seconds \n",
      "Epoch: 469 Train loss: 8.453 Aux train loss: 5.298 Val loss: 4.603 Aux val loss: 2.776 Train MAE: 9.598 Val MAE: 20.627 Epoch time: 197.682 seconds \n",
      "Epoch: 470 Train loss: 8.183 Aux train loss: 5.117 Val loss: 4.588 Aux val loss: 2.768 Train MAE: 9.730 Val MAE: 19.941 Epoch time: 197.550 seconds \n",
      "Epoch: 471 Train loss: 8.172 Aux train loss: 5.122 Val loss: 4.587 Aux val loss: 2.776 Train MAE: 10.056 Val MAE: 21.263 Epoch time: 198.256 seconds \n",
      "Epoch: 472 Train loss: 8.283 Aux train loss: 5.182 Val loss: 4.593 Aux val loss: 2.777 Train MAE: 9.500 Val MAE: 20.217 Epoch time: 197.168 seconds \n",
      "Epoch: 473 Train loss: 8.190 Aux train loss: 5.116 Val loss: 4.576 Aux val loss: 2.762 Train MAE: 9.165 Val MAE: 20.324 Epoch time: 198.572 seconds \n",
      "Epoch: 474 Train loss: 7.969 Aux train loss: 4.997 Val loss: 4.568 Aux val loss: 2.761 Train MAE: 9.484 Val MAE: 19.904 Epoch time: 198.142 seconds \n",
      "Epoch: 475 Train loss: 8.041 Aux train loss: 5.048 Val loss: 4.577 Aux val loss: 2.766 Train MAE: 9.488 Val MAE: 20.747 Epoch time: 197.731 seconds \n",
      "Epoch: 476 Train loss: 8.261 Aux train loss: 5.170 Val loss: 4.588 Aux val loss: 2.768 Train MAE: 9.758 Val MAE: 21.101 Epoch time: 197.573 seconds \n",
      "Epoch: 477 Train loss: 8.226 Aux train loss: 5.162 Val loss: 4.592 Aux val loss: 2.766 Train MAE: 9.455 Val MAE: 20.632 Epoch time: 197.755 seconds \n",
      "Epoch: 478 Train loss: 8.669 Aux train loss: 5.445 Val loss: 4.591 Aux val loss: 2.770 Train MAE: 10.389 Val MAE: 19.749 Epoch time: 196.987 seconds \n",
      "Epoch: 479 Train loss: 8.711 Aux train loss: 5.452 Val loss: 4.582 Aux val loss: 2.766 Train MAE: 9.752 Val MAE: 19.852 Epoch time: 197.491 seconds \n",
      "Epoch: 480 Train loss: 8.688 Aux train loss: 5.429 Val loss: 4.585 Aux val loss: 2.768 Train MAE: 10.022 Val MAE: 20.496 Epoch time: 197.074 seconds \n",
      "Epoch: 481 Train loss: 8.193 Aux train loss: 5.132 Val loss: 4.598 Aux val loss: 2.771 Train MAE: 9.930 Val MAE: 23.392 Epoch time: 197.604 seconds \n",
      "Epoch: 482 Train loss: 8.474 Aux train loss: 5.311 Val loss: 4.589 Aux val loss: 2.768 Train MAE: 9.675 Val MAE: 20.751 Epoch time: 198.055 seconds \n",
      "Epoch: 483 Train loss: 7.974 Aux train loss: 5.005 Val loss: 4.580 Aux val loss: 2.772 Train MAE: 9.554 Val MAE: 20.247 Epoch time: 198.217 seconds \n",
      "Epoch: 484 Train loss: 8.343 Aux train loss: 5.229 Val loss: 4.578 Aux val loss: 2.763 Train MAE: 9.483 Val MAE: 21.213 Epoch time: 198.371 seconds \n",
      "Epoch: 485 Train loss: 8.162 Aux train loss: 5.111 Val loss: 4.576 Aux val loss: 2.759 Train MAE: 10.003 Val MAE: 20.029 Epoch time: 196.870 seconds \n",
      "Epoch: 486 Train loss: 8.629 Aux train loss: 5.428 Val loss: 4.586 Aux val loss: 2.769 Train MAE: 9.823 Val MAE: 21.334 Epoch time: 198.074 seconds \n",
      "Epoch: 487 Train loss: 8.372 Aux train loss: 5.226 Val loss: 4.557 Aux val loss: 2.756 Train MAE: 10.154 Val MAE: 20.332 Epoch time: 197.282 seconds \n",
      "Epoch: 488 Train loss: 8.723 Aux train loss: 5.447 Val loss: 4.566 Aux val loss: 2.762 Train MAE: 10.356 Val MAE: 19.694 Epoch time: 197.602 seconds \n",
      "Epoch: 489 Train loss: 8.035 Aux train loss: 5.046 Val loss: 4.581 Aux val loss: 2.766 Train MAE: 9.688 Val MAE: 21.974 Epoch time: 197.135 seconds \n",
      "Epoch: 490 Train loss: 8.534 Aux train loss: 5.339 Val loss: 4.590 Aux val loss: 2.771 Train MAE: 9.389 Val MAE: 22.264 Epoch time: 196.920 seconds \n",
      "Epoch: 491 Train loss: 8.095 Aux train loss: 5.090 Val loss: 4.573 Aux val loss: 2.761 Train MAE: 9.381 Val MAE: 19.935 Epoch time: 197.446 seconds \n",
      "Epoch: 492 Train loss: 8.528 Aux train loss: 5.345 Val loss: 4.561 Aux val loss: 2.753 Train MAE: 10.068 Val MAE: 19.701 Epoch time: 197.874 seconds \n",
      "Epoch: 493 Train loss: 8.121 Aux train loss: 5.079 Val loss: 4.589 Aux val loss: 2.770 Train MAE: 9.817 Val MAE: 20.349 Epoch time: 197.641 seconds \n",
      "Epoch: 494 Train loss: 9.045 Aux train loss: 5.662 Val loss: 4.576 Aux val loss: 2.763 Train MAE: 10.103 Val MAE: 20.652 Epoch time: 197.598 seconds \n",
      "Epoch: 495 Train loss: 7.730 Aux train loss: 4.859 Val loss: 4.600 Aux val loss: 2.772 Train MAE: 9.083 Val MAE: 21.284 Epoch time: 197.125 seconds \n",
      "Epoch: 496 Train loss: 8.901 Aux train loss: 5.588 Val loss: 4.577 Aux val loss: 2.769 Train MAE: 9.655 Val MAE: 20.184 Epoch time: 198.007 seconds \n",
      "Epoch: 497 Train loss: 8.859 Aux train loss: 5.553 Val loss: 4.587 Aux val loss: 2.764 Train MAE: 10.092 Val MAE: 20.108 Epoch time: 197.618 seconds \n",
      "Epoch: 498 Train loss: 8.408 Aux train loss: 5.259 Val loss: 4.556 Aux val loss: 2.751 Train MAE: 9.905 Val MAE: 20.752 Epoch time: 197.510 seconds \n",
      "Epoch: 499 Train loss: 8.392 Aux train loss: 5.250 Val loss: 4.552 Aux val loss: 2.748 Train MAE: 9.989 Val MAE: 19.995 Epoch time: 197.964 seconds \n",
      "Epoch: 500 Train loss: 7.807 Aux train loss: 4.900 Val loss: 4.571 Aux val loss: 2.759 Train MAE: 9.622 Val MAE: 21.340 Epoch time: 198.121 seconds \n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc_per_node=2 module11/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient17_freeze_gd --epochs=500 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01605799-a43c-4481-aae8-f932a3e414f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Val set MAE: 19.03 RMSE: 55.95\n",
      "GT count: tensor([30.,  8.], device='cuda:0')\n",
      "Predicted count: tensor([35.3365,  3.8726], device='cuda:0')\n",
      "Test set MAE: 20.03 RMSE: 89.55\n",
      "GT count: tensor([17., 34.], device='cuda:0')\n",
      "Predicted count: tensor([ 9.1079, 31.9821], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 torchrun --nproc_per_node=1 module11/evaluate.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient17_freeze_gd --epochs=500 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2b698-73dd-4a3a-bc49-d829f48df28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09136a-df0c-4c7a-868e-848ecc0b7ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0862c9a-29fc-425f-86b4-76b041fbeec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new logic to fix backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffac17f7-a69d-44ba-998b-06330c64a24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING BACKBONE\n",
      "==================================================\n",
      "Using device: cuda\n",
      "Backbone structure:\n",
      "  patch_embed.proj: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  patch_embed.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.downsample: Identity()\n",
      "  layers_0.blocks.0.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.0.drop_path1: Identity()\n",
      "  layers_0.blocks.0.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.mlp.norm: Identity()\n",
      "  layers_0.blocks.0.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.drop_path2: Identity()\n",
      "  layers_0.blocks.1.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.1.drop_path1: DropPath(drop_prob=0.004)\n",
      "  layers_0.blocks.1.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.mlp.norm: Identity()\n",
      "  layers_0.blocks.1.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.drop_path2: DropPath(drop_prob=0.004)\n",
      "  layers_1.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  layers_1.blocks.0.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.0.drop_path1: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.0.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.mlp.norm: Identity()\n",
      "  layers_1.blocks.0.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.drop_path2: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.1.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.1.drop_path1: DropPath(drop_prob=0.013)\n",
      "  layers_1.blocks.1.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.mlp.norm: Identity()\n",
      "  layers_1.blocks.1.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.drop_path2: DropPath(drop_prob=0.013)\n",
      "  layers_2.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  layers_2.blocks.0.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.0.drop_path1: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.0.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.mlp.norm: Identity()\n",
      "  layers_2.blocks.0.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.drop_path2: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.1.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.1.drop_path1: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.1.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.mlp.norm: Identity()\n",
      "  layers_2.blocks.1.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.drop_path2: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.2.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.2.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.2.drop_path1: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.2.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.2.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.2.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.mlp.norm: Identity()\n",
      "  layers_2.blocks.2.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.drop_path2: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.3.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.3.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.3.drop_path1: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.3.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.3.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.3.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.mlp.norm: Identity()\n",
      "  layers_2.blocks.3.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.drop_path2: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.4.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.4.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.4.drop_path1: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.4.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.4.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.4.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.mlp.norm: Identity()\n",
      "  layers_2.blocks.4.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.drop_path2: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.5.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.5.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.5.drop_path1: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.5.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.5.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.5.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.mlp.norm: Identity()\n",
      "  layers_2.blocks.5.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.drop_path2: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.6.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.6.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.6.drop_path1: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.6.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.6.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.6.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.mlp.norm: Identity()\n",
      "  layers_2.blocks.6.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.drop_path2: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.7.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.7.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.7.drop_path1: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.7.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.7.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.7.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.mlp.norm: Identity()\n",
      "  layers_2.blocks.7.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.drop_path2: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.8.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.8.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.8.drop_path1: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.8.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.8.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.8.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.mlp.norm: Identity()\n",
      "  layers_2.blocks.8.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.drop_path2: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.9.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.9.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.9.drop_path1: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.9.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.9.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.9.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.mlp.norm: Identity()\n",
      "  layers_2.blocks.9.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.drop_path2: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.10.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.10.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.10.drop_path1: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.10.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.10.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.10.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.mlp.norm: Identity()\n",
      "  layers_2.blocks.10.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.drop_path2: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.11.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.11.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.11.drop_path1: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.11.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.11.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.11.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.mlp.norm: Identity()\n",
      "  layers_2.blocks.11.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.drop_path2: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.12.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.12.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.12.drop_path1: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.12.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.12.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.12.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.mlp.norm: Identity()\n",
      "  layers_2.blocks.12.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.drop_path2: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.13.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.13.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.13.drop_path1: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.13.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.13.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.13.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.mlp.norm: Identity()\n",
      "  layers_2.blocks.13.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.drop_path2: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.14.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.14.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.14.drop_path1: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.14.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.14.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.14.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.mlp.norm: Identity()\n",
      "  layers_2.blocks.14.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.drop_path2: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.15.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.15.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.15.drop_path1: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.15.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.15.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.15.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.mlp.norm: Identity()\n",
      "  layers_2.blocks.15.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.drop_path2: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.16.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.16.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.16.drop_path1: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.16.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.16.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.16.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.mlp.norm: Identity()\n",
      "  layers_2.blocks.16.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.drop_path2: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.17.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.17.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.17.drop_path1: DropPath(drop_prob=0.091)\n",
      "  layers_2.blocks.17.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.17.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.17.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.mlp.norm: Identity()\n",
      "  layers_2.blocks.17.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.drop_path2: DropPath(drop_prob=0.091)\n",
      "  layers_3.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  layers_3.blocks.0.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.0.drop_path1: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.0.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.mlp.norm: Identity()\n",
      "  layers_3.blocks.0.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.drop_path2: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.1.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.1.drop_path1: DropPath(drop_prob=0.100)\n",
      "  layers_3.blocks.1.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.mlp.norm: Identity()\n",
      "  layers_3.blocks.1.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.drop_path2: DropPath(drop_prob=0.100)\n",
      "\n",
      "Detected layer name pattern: layers_0, layers_1, layers_2, layers_3\n",
      "Creating new parameter mapping from GroundingDINO...\n",
      "/opt/miniconda/envs/Rey2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "\n",
      "GroundingDINO Backbone Structure:\n",
      "  embeddings.patch_embeddings.projection: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  embeddings.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  embeddings.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.layernorm_before: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.0.drop_path: Identity()\n",
      "  encoder.layers.0.blocks.0.layernorm_after: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.0.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  encoder.layers.0.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.0.blocks.0.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.layernorm_before: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.query: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.key: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.value: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.attention.output.dense: Linear(in_features=128, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.blocks.1.drop_path: SwinDropPath(p=0.004347826354205608)\n",
      "  encoder.layers.0.blocks.1.layernorm_after: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.0.blocks.1.intermediate.dense: Linear(in_features=128, out_features=512, bias=True)\n",
      "  encoder.layers.0.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.0.blocks.1.output.dense: Linear(in_features=512, out_features=128, bias=True)\n",
      "  encoder.layers.0.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.0.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  encoder.layers.0.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.layernorm_before: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.query: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.key: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.value: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.0.attention.output.dense: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.0.drop_path: SwinDropPath(p=0.008695652708411217)\n",
      "  encoder.layers.1.blocks.0.layernorm_after: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.0.intermediate.dense: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  encoder.layers.1.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.1.blocks.0.output.dense: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.layernorm_before: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.query: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.key: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.value: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.attention.output.dense: Linear(in_features=256, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.blocks.1.drop_path: SwinDropPath(p=0.013043479062616825)\n",
      "  encoder.layers.1.blocks.1.layernorm_after: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.1.blocks.1.intermediate.dense: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  encoder.layers.1.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.1.blocks.1.output.dense: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  encoder.layers.1.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.1.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  encoder.layers.1.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.0.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.0.drop_path: SwinDropPath(p=0.017391305416822433)\n",
      "  encoder.layers.2.blocks.0.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.0.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.0.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.1.drop_path: SwinDropPath(p=0.021739132702350616)\n",
      "  encoder.layers.2.blocks.1.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.1.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.1.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.2.drop_path: SwinDropPath(p=0.02608695812523365)\n",
      "  encoder.layers.2.blocks.2.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.2.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.2.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.2.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.2.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.3.drop_path: SwinDropPath(p=0.030434783548116684)\n",
      "  encoder.layers.2.blocks.3.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.3.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.3.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.3.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.3.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.4.drop_path: SwinDropPath(p=0.03478261083364487)\n",
      "  encoder.layers.2.blocks.4.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.4.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.4.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.4.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.4.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.5.drop_path: SwinDropPath(p=0.03913043811917305)\n",
      "  encoder.layers.2.blocks.5.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.5.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.5.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.5.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.5.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.6.drop_path: SwinDropPath(p=0.04347826540470123)\n",
      "  encoder.layers.2.blocks.6.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.6.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.6.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.6.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.6.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.7.drop_path: SwinDropPath(p=0.04782608896493912)\n",
      "  encoder.layers.2.blocks.7.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.7.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.7.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.7.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.7.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.8.drop_path: SwinDropPath(p=0.052173912525177)\n",
      "  encoder.layers.2.blocks.8.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.8.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.8.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.8.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.8.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.9.drop_path: SwinDropPath(p=0.056521736085414886)\n",
      "  encoder.layers.2.blocks.9.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.9.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.9.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.9.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.9.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.10.drop_path: SwinDropPath(p=0.06086956337094307)\n",
      "  encoder.layers.2.blocks.10.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.10.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.10.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.10.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.10.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.11.drop_path: SwinDropPath(p=0.06521739065647125)\n",
      "  encoder.layers.2.blocks.11.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.11.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.11.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.11.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.11.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.12.drop_path: SwinDropPath(p=0.06956521421670914)\n",
      "  encoder.layers.2.blocks.12.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.12.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.12.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.12.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.12.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.13.drop_path: SwinDropPath(p=0.07391304522752762)\n",
      "  encoder.layers.2.blocks.13.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.13.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.13.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.13.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.13.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.14.drop_path: SwinDropPath(p=0.0782608687877655)\n",
      "  encoder.layers.2.blocks.14.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.14.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.14.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.14.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.14.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.15.drop_path: SwinDropPath(p=0.08260869979858398)\n",
      "  encoder.layers.2.blocks.15.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.15.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.15.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.15.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.15.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.16.drop_path: SwinDropPath(p=0.08695652335882187)\n",
      "  encoder.layers.2.blocks.16.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.16.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.16.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.16.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.16.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.layernorm_before: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.query: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.key: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.value: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.attention.output.dense: Linear(in_features=512, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.blocks.17.drop_path: SwinDropPath(p=0.09130434691905975)\n",
      "  encoder.layers.2.blocks.17.layernorm_after: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.2.blocks.17.intermediate.dense: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  encoder.layers.2.blocks.17.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.2.blocks.17.output.dense: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  encoder.layers.2.blocks.17.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.2.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  encoder.layers.2.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.layernorm_before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.query: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.key: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.value: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.0.attention.output.dense: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.0.drop_path: SwinDropPath(p=0.09565217792987823)\n",
      "  encoder.layers.3.blocks.0.layernorm_after: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.0.intermediate.dense: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  encoder.layers.3.blocks.0.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.3.blocks.0.output.dense: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.0.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.layernorm_before: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.query: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.key: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.value: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.self.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.attention.output.dense: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.attention.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  encoder.layers.3.blocks.1.drop_path: SwinDropPath(p=0.10000000149011612)\n",
      "  encoder.layers.3.blocks.1.layernorm_after: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  encoder.layers.3.blocks.1.intermediate.dense: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  encoder.layers.3.blocks.1.intermediate.intermediate_act_fn: GELUActivation()\n",
      "  encoder.layers.3.blocks.1.output.dense: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  encoder.layers.3.blocks.1.output.dropout: Dropout(p=0.0, inplace=False)\n",
      "  hidden_states_norms.stage2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  hidden_states_norms.stage3: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  hidden_states_norms.stage4: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "\n",
      "Parameter stats sebelum mapping:\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "\n",
      "Sample TIMM keys:\n",
      "  patch_embed.proj.weight: torch.Size([128, 3, 4, 4])\n",
      "  patch_embed.proj.bias: torch.Size([128])\n",
      "  patch_embed.norm.weight: torch.Size([128])\n",
      "  patch_embed.norm.bias: torch.Size([128])\n",
      "  layers_0.blocks.0.norm1.weight: torch.Size([128])\n",
      "  layers_0.blocks.0.norm1.bias: torch.Size([128])\n",
      "  layers_0.blocks.0.attn.relative_position_bias_table: torch.Size([169, 4])\n",
      "  layers_0.blocks.0.attn.qkv.weight: torch.Size([384, 128])\n",
      "  layers_0.blocks.0.attn.qkv.bias: torch.Size([384])\n",
      "  layers_0.blocks.0.attn.proj.weight: torch.Size([128, 128])\n",
      "\n",
      "Sample GroundingDINO keys:\n",
      "  embeddings.patch_embeddings.projection.weight: torch.Size([128, 3, 4, 4])\n",
      "  embeddings.patch_embeddings.projection.bias: torch.Size([128])\n",
      "  embeddings.norm.weight: torch.Size([128])\n",
      "  embeddings.norm.bias: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.layernorm_before.weight: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.layernorm_before.bias: torch.Size([128])\n",
      "  encoder.layers.0.blocks.0.attention.self.relative_position_bias_table: torch.Size([529, 4])\n",
      "  encoder.layers.0.blocks.0.attention.self.relative_position_index: torch.Size([144, 144])\n",
      "  encoder.layers.0.blocks.0.attention.self.query.weight: torch.Size([128, 128])\n",
      "  encoder.layers.0.blocks.0.attention.self.query.bias: torch.Size([128])\n",
      "Mapped 301/325 parameters successfully\n",
      "QKV parameters: 24/24 potential matches found\n",
      "Saved mapped parameters to ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "\n",
      "Parameter stats setelah mapping:\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "Backbone parameters frozen (requires_grad=False)\n",
      "Backbone in EVAL mode\n",
      "Input tensor shape: torch.Size([2, 3, 512, 512])\n",
      "\n",
      "Testing forward_multiscale...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -4.6752, max: 4.6311, mean: 0.0011, std: 0.9995\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -9.5395, max: 15.8090, mean: 0.0544, std: 1.2382\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -71.2870, max: 386.0356, mean: 0.2688, std: 6.7572\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -123.1154, max: 56.6445, mean: -0.0206, std: 3.7044\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "\n",
      "Testing forward_concatenated...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -4.6752, max: 4.6311, mean: 0.0011, std: 0.9995\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -9.5395, max: 15.8090, mean: 0.0544, std: 1.2382\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -71.2870, max: 386.0356, mean: 0.2688, std: 6.7572\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -123.1154, max: 56.6445, mean: -0.0206, std: 3.7044\n",
      "Permuting feature dimensions from BHWC to BCHW\n",
      "Target size after reduction: (64, 64)\n",
      "Concatenated feature shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n",
      "\n",
      "==================================================\n",
      "BACKBONE TEST COMPLETE\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "TESTING BACKBONE\n",
      "==================================================\n",
      "Using device: cuda\n",
      "Backbone structure:\n",
      "  patch_embed.proj: Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "  patch_embed.norm: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.downsample: Identity()\n",
      "  layers_0.blocks.0.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.0.drop_path1: Identity()\n",
      "  layers_0.blocks.0.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.0.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.mlp.norm: Identity()\n",
      "  layers_0.blocks.0.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.0.drop_path2: Identity()\n",
      "  layers_0.blocks.1.norm1: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.attn.qkv: Linear(in_features=128, out_features=384, bias=True)\n",
      "  layers_0.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.proj: Linear(in_features=128, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_0.blocks.1.drop_path1: DropPath(drop_prob=0.004)\n",
      "  layers_0.blocks.1.norm2: LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_0.blocks.1.mlp.fc1: Linear(in_features=128, out_features=512, bias=True)\n",
      "  layers_0.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_0.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.mlp.norm: Identity()\n",
      "  layers_0.blocks.1.mlp.fc2: Linear(in_features=512, out_features=128, bias=True)\n",
      "  layers_0.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_0.blocks.1.drop_path2: DropPath(drop_prob=0.004)\n",
      "  layers_1.downsample.norm: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.downsample.reduction: Linear(in_features=512, out_features=256, bias=False)\n",
      "  layers_1.blocks.0.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.0.drop_path1: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.0.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.0.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.mlp.norm: Identity()\n",
      "  layers_1.blocks.0.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.0.drop_path2: DropPath(drop_prob=0.009)\n",
      "  layers_1.blocks.1.norm1: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.attn.qkv: Linear(in_features=256, out_features=768, bias=True)\n",
      "  layers_1.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_1.blocks.1.drop_path1: DropPath(drop_prob=0.013)\n",
      "  layers_1.blocks.1.norm2: LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_1.blocks.1.mlp.fc1: Linear(in_features=256, out_features=1024, bias=True)\n",
      "  layers_1.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_1.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.mlp.norm: Identity()\n",
      "  layers_1.blocks.1.mlp.fc2: Linear(in_features=1024, out_features=256, bias=True)\n",
      "  layers_1.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_1.blocks.1.drop_path2: DropPath(drop_prob=0.013)\n",
      "  layers_2.downsample.norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.downsample.reduction: Linear(in_features=1024, out_features=512, bias=False)\n",
      "  layers_2.blocks.0.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.0.drop_path1: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.0.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.0.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.mlp.norm: Identity()\n",
      "  layers_2.blocks.0.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.0.drop_path2: DropPath(drop_prob=0.017)\n",
      "  layers_2.blocks.1.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.1.drop_path1: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.1.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.1.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.mlp.norm: Identity()\n",
      "  layers_2.blocks.1.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.1.drop_path2: DropPath(drop_prob=0.022)\n",
      "  layers_2.blocks.2.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.2.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.2.drop_path1: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.2.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.2.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.2.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.2.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.mlp.norm: Identity()\n",
      "  layers_2.blocks.2.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.2.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.2.drop_path2: DropPath(drop_prob=0.026)\n",
      "  layers_2.blocks.3.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.3.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.3.drop_path1: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.3.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.3.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.3.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.3.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.mlp.norm: Identity()\n",
      "  layers_2.blocks.3.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.3.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.3.drop_path2: DropPath(drop_prob=0.030)\n",
      "  layers_2.blocks.4.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.4.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.4.drop_path1: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.4.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.4.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.4.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.4.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.mlp.norm: Identity()\n",
      "  layers_2.blocks.4.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.4.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.4.drop_path2: DropPath(drop_prob=0.035)\n",
      "  layers_2.blocks.5.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.5.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.5.drop_path1: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.5.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.5.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.5.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.5.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.mlp.norm: Identity()\n",
      "  layers_2.blocks.5.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.5.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.5.drop_path2: DropPath(drop_prob=0.039)\n",
      "  layers_2.blocks.6.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.6.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.6.drop_path1: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.6.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.6.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.6.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.6.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.mlp.norm: Identity()\n",
      "  layers_2.blocks.6.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.6.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.6.drop_path2: DropPath(drop_prob=0.043)\n",
      "  layers_2.blocks.7.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.7.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.7.drop_path1: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.7.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.7.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.7.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.7.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.mlp.norm: Identity()\n",
      "  layers_2.blocks.7.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.7.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.7.drop_path2: DropPath(drop_prob=0.048)\n",
      "  layers_2.blocks.8.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.8.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.8.drop_path1: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.8.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.8.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.8.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.8.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.mlp.norm: Identity()\n",
      "  layers_2.blocks.8.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.8.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.8.drop_path2: DropPath(drop_prob=0.052)\n",
      "  layers_2.blocks.9.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.9.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.9.drop_path1: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.9.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.9.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.9.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.9.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.mlp.norm: Identity()\n",
      "  layers_2.blocks.9.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.9.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.9.drop_path2: DropPath(drop_prob=0.057)\n",
      "  layers_2.blocks.10.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.10.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.10.drop_path1: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.10.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.10.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.10.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.10.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.mlp.norm: Identity()\n",
      "  layers_2.blocks.10.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.10.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.10.drop_path2: DropPath(drop_prob=0.061)\n",
      "  layers_2.blocks.11.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.11.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.11.drop_path1: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.11.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.11.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.11.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.11.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.mlp.norm: Identity()\n",
      "  layers_2.blocks.11.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.11.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.11.drop_path2: DropPath(drop_prob=0.065)\n",
      "  layers_2.blocks.12.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.12.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.12.drop_path1: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.12.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.12.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.12.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.12.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.mlp.norm: Identity()\n",
      "  layers_2.blocks.12.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.12.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.12.drop_path2: DropPath(drop_prob=0.070)\n",
      "  layers_2.blocks.13.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.13.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.13.drop_path1: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.13.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.13.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.13.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.13.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.mlp.norm: Identity()\n",
      "  layers_2.blocks.13.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.13.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.13.drop_path2: DropPath(drop_prob=0.074)\n",
      "  layers_2.blocks.14.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.14.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.14.drop_path1: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.14.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.14.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.14.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.14.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.mlp.norm: Identity()\n",
      "  layers_2.blocks.14.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.14.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.14.drop_path2: DropPath(drop_prob=0.078)\n",
      "  layers_2.blocks.15.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.15.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.15.drop_path1: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.15.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.15.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.15.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.15.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.mlp.norm: Identity()\n",
      "  layers_2.blocks.15.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.15.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.15.drop_path2: DropPath(drop_prob=0.083)\n",
      "  layers_2.blocks.16.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.16.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.16.drop_path1: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.16.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.16.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.16.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.16.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.mlp.norm: Identity()\n",
      "  layers_2.blocks.16.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.16.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.16.drop_path2: DropPath(drop_prob=0.087)\n",
      "  layers_2.blocks.17.norm1: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.attn.qkv: Linear(in_features=512, out_features=1536, bias=True)\n",
      "  layers_2.blocks.17.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.proj: Linear(in_features=512, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.attn.softmax: Softmax(dim=-1)\n",
      "  layers_2.blocks.17.drop_path1: DropPath(drop_prob=0.091)\n",
      "  layers_2.blocks.17.norm2: LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_2.blocks.17.mlp.fc1: Linear(in_features=512, out_features=2048, bias=True)\n",
      "  layers_2.blocks.17.mlp.act: GELU(approximate='none')\n",
      "  layers_2.blocks.17.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.mlp.norm: Identity()\n",
      "  layers_2.blocks.17.mlp.fc2: Linear(in_features=2048, out_features=512, bias=True)\n",
      "  layers_2.blocks.17.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_2.blocks.17.drop_path2: DropPath(drop_prob=0.091)\n",
      "  layers_3.downsample.norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.downsample.reduction: Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  layers_3.blocks.0.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.0.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.0.drop_path1: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.0.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.0.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.0.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.0.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.mlp.norm: Identity()\n",
      "  layers_3.blocks.0.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.0.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.0.drop_path2: DropPath(drop_prob=0.096)\n",
      "  layers_3.blocks.1.norm1: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.attn.qkv: Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  layers_3.blocks.1.attn.attn_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.proj: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.attn.softmax: Softmax(dim=-1)\n",
      "  layers_3.blocks.1.drop_path1: DropPath(drop_prob=0.100)\n",
      "  layers_3.blocks.1.norm2: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  layers_3.blocks.1.mlp.fc1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  layers_3.blocks.1.mlp.act: GELU(approximate='none')\n",
      "  layers_3.blocks.1.mlp.drop1: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.mlp.norm: Identity()\n",
      "  layers_3.blocks.1.mlp.fc2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  layers_3.blocks.1.mlp.drop2: Dropout(p=0.0, inplace=False)\n",
      "  layers_3.blocks.1.drop_path2: DropPath(drop_prob=0.100)\n",
      "\n",
      "Detected layer name pattern: layers_0, layers_1, layers_2, layers_3\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Trainable params: 86,741,176\n",
      "Frozen params: 0\n",
      "Backbone parameters set to trainable (requires_grad=True)\n",
      "Backbone in TRAIN mode\n",
      "Input tensor shape: torch.Size([2, 3, 512, 512])\n",
      "\n",
      "Testing forward_multiscale...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -5.2421, max: 4.9014, mean: 0.0012, std: 0.9997\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -9.4857, max: 15.6301, mean: 0.0540, std: 1.2457\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -98.5599, max: 474.7997, mean: 0.2645, std: 6.8325\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -122.7312, max: 59.6137, mean: -0.0197, std: 3.7409\n",
      "S3 shape: torch.Size([2, 64, 64, 256])\n",
      "S4 shape: torch.Size([2, 32, 32, 512])\n",
      "S5 shape: torch.Size([2, 16, 16, 1024])\n",
      "\n",
      "Testing forward_concatenated...\n",
      "Input shape: torch.Size([2, 3, 512, 512])\n",
      "Input stats - min: -5.2421, max: 4.9014, mean: 0.0012, std: 0.9997\n",
      "S3 shape: torch.Size([2, 64, 64, 256]), min: -9.4857, max: 15.6301, mean: 0.0540, std: 1.2457\n",
      "S4 shape: torch.Size([2, 32, 32, 512]), min: -61.2455, max: 350.1863, mean: 0.2497, std: 6.5495\n",
      "S5 shape: torch.Size([2, 16, 16, 1024]), min: -132.5683, max: 61.1003, mean: -0.0217, std: 3.8831\n",
      "Permuting feature dimensions from BHWC to BCHW\n",
      "Target size after reduction: (64, 64)\n",
      "Concatenated feature shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n",
      "Concatenated shape: torch.Size([2, 1792, 64, 64])\n",
      "Expected channels: 1792, Actual: 1792\n",
      "\n",
      "Testing backward pass...\n",
      "Dummy loss: 0.06668376177549362\n",
      "Top 5 gradients by norm:\n",
      "  backbone.layers_0.blocks.1.attn.proj.weight: 0.314110\n",
      "  backbone.layers_0.blocks.0.attn.proj.weight: 0.311743\n",
      "  backbone.layers_0.blocks.0.mlp.fc2.weight: 0.268009\n",
      "  backbone.layers_0.blocks.1.mlp.fc1.weight: 0.263032\n",
      "  backbone.layers_0.blocks.1.attn.qkv.weight: 0.210039\n",
      "Optimizer step completed\n",
      "\n",
      "==================================================\n",
      "BACKBONE TEST COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!python module11/debug_backbone.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb5cbec-1189-4392-980b-cf7d3b0bb6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0320 15:17:41.202000 475015 site-packages/torch/distributed/run.py:793] \n",
      "W0320 15:17:41.202000 475015 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0320 15:17:41.202000 475015 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0320 15:17:41.202000 475015 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "0\n",
      "1\n",
      "Epoch: 1 Train loss: 23.768 Aux train loss: 10.940 Val loss: 6.478 Aux val loss: 3.846 Train MAE: 91.133 Val MAE: 55.577 Epoch time: 196.783 seconds best\n",
      "Epoch: 2 Train loss: 36.907 Aux train loss: 10.927 Val loss: 6.385 Aux val loss: 3.855 Train MAE: 61.638 Val MAE: 43.191 Epoch time: 200.824 seconds best\n",
      "Epoch: 3 Train loss: 14.906 Aux train loss: 8.986 Val loss: 6.218 Aux val loss: 3.763 Train MAE: 42.095 Val MAE: 32.505 Epoch time: 206.673 seconds best\n",
      "Epoch: 4 Train loss: 15.053 Aux train loss: 9.010 Val loss: 6.438 Aux val loss: 3.793 Train MAE: 39.850 Val MAE: 62.985 Epoch time: 207.127 seconds \n",
      "Epoch: 5 Train loss: 14.861 Aux train loss: 8.982 Val loss: 6.083 Aux val loss: 3.646 Train MAE: 37.457 Val MAE: 47.532 Epoch time: 205.535 seconds \n",
      "Epoch: 6 Train loss: 14.249 Aux train loss: 8.554 Val loss: 5.867 Aux val loss: 3.506 Train MAE: 37.769 Val MAE: 26.240 Epoch time: 206.273 seconds best\n",
      "Epoch: 7 Train loss: 13.850 Aux train loss: 8.292 Val loss: 5.651 Aux val loss: 3.466 Train MAE: 35.533 Val MAE: 22.989 Epoch time: 205.728 seconds best\n",
      "Epoch: 8 Train loss: 12.881 Aux train loss: 7.802 Val loss: 5.627 Aux val loss: 3.417 Train MAE: 30.775 Val MAE: 27.726 Epoch time: 206.079 seconds \n",
      "Epoch: 9 Train loss: 13.168 Aux train loss: 7.953 Val loss: 5.565 Aux val loss: 3.385 Train MAE: 30.705 Val MAE: 40.603 Epoch time: 206.444 seconds \n",
      "Epoch: 10 Train loss: 12.794 Aux train loss: 7.717 Val loss: 5.484 Aux val loss: 3.315 Train MAE: 30.147 Val MAE: 34.932 Epoch time: 204.257 seconds \n",
      "Epoch: 11 Train loss: 12.449 Aux train loss: 7.527 Val loss: 5.409 Aux val loss: 3.261 Train MAE: 30.761 Val MAE: 34.773 Epoch time: 205.773 seconds \n",
      "Epoch: 12 Train loss: 12.728 Aux train loss: 7.673 Val loss: 5.339 Aux val loss: 3.237 Train MAE: 31.912 Val MAE: 39.965 Epoch time: 206.467 seconds \n",
      "Epoch: 13 Train loss: 11.664 Aux train loss: 7.031 Val loss: 5.181 Aux val loss: 3.150 Train MAE: 28.958 Val MAE: 20.632 Epoch time: 205.276 seconds best\n",
      "Epoch: 14 Train loss: 12.299 Aux train loss: 7.424 Val loss: 5.449 Aux val loss: 3.243 Train MAE: 28.818 Val MAE: 45.366 Epoch time: 205.240 seconds \n",
      "Epoch: 15 Train loss: 11.232 Aux train loss: 6.798 Val loss: 5.374 Aux val loss: 3.148 Train MAE: 29.162 Val MAE: 22.872 Epoch time: 204.834 seconds \n",
      "Epoch: 16 Train loss: 11.517 Aux train loss: 6.930 Val loss: 5.087 Aux val loss: 3.063 Train MAE: 27.773 Val MAE: 25.661 Epoch time: 206.473 seconds \n",
      "Epoch: 17 Train loss: 11.405 Aux train loss: 6.884 Val loss: 5.168 Aux val loss: 3.189 Train MAE: 26.842 Val MAE: 29.096 Epoch time: 205.782 seconds \n",
      "Epoch: 18 Train loss: 11.821 Aux train loss: 7.111 Val loss: 5.074 Aux val loss: 3.063 Train MAE: 27.319 Val MAE: 21.535 Epoch time: 204.873 seconds \n",
      "Epoch: 19 Train loss: 10.305 Aux train loss: 6.229 Val loss: 5.333 Aux val loss: 3.001 Train MAE: 23.059 Val MAE: 21.940 Epoch time: 205.428 seconds \n",
      "Epoch: 20 Train loss: 11.237 Aux train loss: 6.819 Val loss: 5.174 Aux val loss: 3.084 Train MAE: 25.026 Val MAE: 32.687 Epoch time: 205.611 seconds \n",
      "Epoch: 21 Train loss: 10.461 Aux train loss: 6.364 Val loss: 5.285 Aux val loss: 3.134 Train MAE: 22.360 Val MAE: 34.007 Epoch time: 205.959 seconds \n",
      "Epoch: 22 Train loss: 10.337 Aux train loss: 6.213 Val loss: 4.760 Aux val loss: 2.893 Train MAE: 24.544 Val MAE: 23.599 Epoch time: 205.848 seconds \n",
      "Epoch: 23 Train loss: 10.630 Aux train loss: 6.437 Val loss: 4.809 Aux val loss: 2.848 Train MAE: 24.632 Val MAE: 25.151 Epoch time: 205.916 seconds \n",
      "Epoch: 24 Train loss: 10.605 Aux train loss: 6.391 Val loss: 4.811 Aux val loss: 2.917 Train MAE: 21.722 Val MAE: 21.511 Epoch time: 205.600 seconds \n",
      "Epoch: 25 Train loss: 10.707 Aux train loss: 6.138 Val loss: 4.727 Aux val loss: 2.839 Train MAE: 25.247 Val MAE: 32.896 Epoch time: 205.876 seconds \n",
      "Epoch: 26 Train loss: 10.323 Aux train loss: 6.178 Val loss: 4.742 Aux val loss: 2.826 Train MAE: 22.983 Val MAE: 32.101 Epoch time: 204.984 seconds \n",
      "Epoch: 27 Train loss: 10.788 Aux train loss: 6.485 Val loss: 4.676 Aux val loss: 2.789 Train MAE: 22.853 Val MAE: 18.245 Epoch time: 205.359 seconds best\n",
      "Epoch: 28 Train loss: 10.048 Aux train loss: 6.072 Val loss: 4.636 Aux val loss: 2.828 Train MAE: 21.132 Val MAE: 17.827 Epoch time: 205.912 seconds best\n",
      "Epoch: 29 Train loss: 10.465 Aux train loss: 6.292 Val loss: 4.728 Aux val loss: 2.837 Train MAE: 23.201 Val MAE: 34.719 Epoch time: 204.987 seconds \n",
      "Epoch: 30 Train loss: 9.344 Aux train loss: 5.600 Val loss: 4.913 Aux val loss: 2.962 Train MAE: 20.355 Val MAE: 41.306 Epoch time: 205.246 seconds \n",
      "Epoch: 31 Train loss: 9.928 Aux train loss: 6.011 Val loss: 4.579 Aux val loss: 2.777 Train MAE: 19.358 Val MAE: 20.795 Epoch time: 206.056 seconds \n",
      "Epoch: 32 Train loss: 9.820 Aux train loss: 5.952 Val loss: 4.647 Aux val loss: 2.792 Train MAE: 19.906 Val MAE: 31.066 Epoch time: 205.507 seconds \n",
      "Epoch: 33 Train loss: 11.218 Aux train loss: 5.989 Val loss: 4.651 Aux val loss: 2.780 Train MAE: 22.068 Val MAE: 20.271 Epoch time: 205.365 seconds \n",
      "Epoch: 34 Train loss: 9.474 Aux train loss: 5.688 Val loss: 4.748 Aux val loss: 2.819 Train MAE: 20.424 Val MAE: 27.575 Epoch time: 205.984 seconds \n",
      "Epoch: 35 Train loss: 10.356 Aux train loss: 7.095 Val loss: 4.576 Aux val loss: 2.801 Train MAE: 20.181 Val MAE: 20.452 Epoch time: 202.685 seconds \n",
      "Epoch: 36 Train loss: 9.793 Aux train loss: 5.926 Val loss: 4.709 Aux val loss: 2.783 Train MAE: 19.832 Val MAE: 39.194 Epoch time: 200.123 seconds \n",
      "Epoch: 37 Train loss: 9.914 Aux train loss: 5.951 Val loss: 4.691 Aux val loss: 2.768 Train MAE: 21.902 Val MAE: 24.930 Epoch time: 199.835 seconds \n",
      "Epoch: 38 Train loss: 9.859 Aux train loss: 5.937 Val loss: 4.672 Aux val loss: 2.774 Train MAE: 20.760 Val MAE: 22.372 Epoch time: 199.275 seconds \n",
      "Epoch: 39 Train loss: 10.365 Aux train loss: 6.281 Val loss: 4.465 Aux val loss: 2.709 Train MAE: 20.033 Val MAE: 16.702 Epoch time: 199.752 seconds best\n",
      "Epoch: 40 Train loss: 9.578 Aux train loss: 5.806 Val loss: 4.699 Aux val loss: 2.855 Train MAE: 18.422 Val MAE: 20.062 Epoch time: 200.299 seconds \n",
      "Epoch: 41 Train loss: 9.522 Aux train loss: 5.749 Val loss: 4.601 Aux val loss: 2.692 Train MAE: 19.202 Val MAE: 28.601 Epoch time: 200.368 seconds \n",
      "Epoch: 42 Train loss: 10.050 Aux train loss: 6.060 Val loss: 4.553 Aux val loss: 2.750 Train MAE: 21.021 Val MAE: 20.600 Epoch time: 200.495 seconds \n",
      "Epoch: 43 Train loss: 9.500 Aux train loss: 5.726 Val loss: 4.521 Aux val loss: 2.730 Train MAE: 17.856 Val MAE: 18.717 Epoch time: 199.536 seconds \n",
      "Epoch: 44 Train loss: 9.616 Aux train loss: 5.591 Val loss: 4.539 Aux val loss: 2.689 Train MAE: 21.098 Val MAE: 29.796 Epoch time: 198.592 seconds \n",
      "Epoch: 45 Train loss: 9.426 Aux train loss: 5.684 Val loss: 4.534 Aux val loss: 2.712 Train MAE: 18.616 Val MAE: 30.919 Epoch time: 198.902 seconds \n",
      "Epoch: 46 Train loss: 8.590 Aux train loss: 5.176 Val loss: 4.372 Aux val loss: 2.647 Train MAE: 17.880 Val MAE: 16.092 Epoch time: 198.848 seconds best\n",
      "Epoch: 47 Train loss: 9.545 Aux train loss: 5.805 Val loss: 4.447 Aux val loss: 2.668 Train MAE: 17.857 Val MAE: 18.202 Epoch time: 199.127 seconds \n",
      "Epoch: 48 Train loss: 9.634 Aux train loss: 5.815 Val loss: 4.458 Aux val loss: 2.686 Train MAE: 17.807 Val MAE: 21.378 Epoch time: 199.963 seconds \n",
      "Epoch: 49 Train loss: 8.764 Aux train loss: 5.293 Val loss: 4.803 Aux val loss: 2.645 Train MAE: 17.331 Val MAE: 22.388 Epoch time: 199.819 seconds \n",
      "Epoch: 50 Train loss: 8.879 Aux train loss: 5.374 Val loss: 4.399 Aux val loss: 2.696 Train MAE: 17.124 Val MAE: 18.525 Epoch time: 198.929 seconds \n",
      "Epoch: 51 Train loss: 9.069 Aux train loss: 5.465 Val loss: 4.489 Aux val loss: 2.685 Train MAE: 18.915 Val MAE: 28.065 Epoch time: 199.351 seconds \n",
      "Epoch: 52 Train loss: 9.789 Aux train loss: 5.913 Val loss: 4.321 Aux val loss: 2.638 Train MAE: 18.633 Val MAE: 19.136 Epoch time: 198.960 seconds \n",
      "Epoch: 53 Train loss: 9.231 Aux train loss: 5.594 Val loss: 4.322 Aux val loss: 2.605 Train MAE: 17.836 Val MAE: 18.468 Epoch time: 198.821 seconds \n",
      "Epoch: 54 Train loss: 8.808 Aux train loss: 5.243 Val loss: 6.471 Aux val loss: 2.590 Train MAE: 18.136 Val MAE: 21.787 Epoch time: 199.531 seconds \n",
      "Epoch: 55 Train loss: 9.079 Aux train loss: 5.508 Val loss: 4.481 Aux val loss: 2.737 Train MAE: 17.031 Val MAE: 27.151 Epoch time: 198.916 seconds \n",
      "Epoch: 56 Train loss: 8.991 Aux train loss: 5.451 Val loss: 4.379 Aux val loss: 2.619 Train MAE: 17.736 Val MAE: 29.240 Epoch time: 197.303 seconds \n",
      "Epoch: 57 Train loss: 8.678 Aux train loss: 5.242 Val loss: 4.424 Aux val loss: 2.660 Train MAE: 16.744 Val MAE: 27.504 Epoch time: 197.680 seconds \n",
      "Epoch: 58 Train loss: 9.392 Aux train loss: 5.647 Val loss: 4.379 Aux val loss: 2.637 Train MAE: 18.389 Val MAE: 29.856 Epoch time: 198.001 seconds \n",
      "Epoch: 59 Train loss: 8.834 Aux train loss: 5.366 Val loss: 4.287 Aux val loss: 2.594 Train MAE: 17.453 Val MAE: 18.068 Epoch time: 198.426 seconds \n",
      "Epoch: 60 Train loss: 9.214 Aux train loss: 5.610 Val loss: 4.328 Aux val loss: 2.585 Train MAE: 16.853 Val MAE: 23.145 Epoch time: 198.448 seconds \n",
      "Epoch: 61 Train loss: 8.426 Aux train loss: 5.105 Val loss: 4.235 Aux val loss: 2.548 Train MAE: 16.520 Val MAE: 23.455 Epoch time: 199.347 seconds \n",
      "Epoch: 62 Train loss: 8.273 Aux train loss: 5.008 Val loss: 4.235 Aux val loss: 2.594 Train MAE: 17.143 Val MAE: 20.534 Epoch time: 199.107 seconds \n",
      "Epoch: 63 Train loss: 9.238 Aux train loss: 6.110 Val loss: 4.259 Aux val loss: 2.573 Train MAE: 15.962 Val MAE: 23.778 Epoch time: 199.311 seconds \n",
      "Epoch: 64 Train loss: 8.906 Aux train loss: 5.387 Val loss: 4.404 Aux val loss: 2.624 Train MAE: 17.919 Val MAE: 23.585 Epoch time: 199.533 seconds \n",
      "Epoch: 65 Train loss: 8.926 Aux train loss: 5.392 Val loss: 4.200 Aux val loss: 2.615 Train MAE: 16.838 Val MAE: 22.154 Epoch time: 199.362 seconds \n",
      "Epoch: 66 Train loss: 8.912 Aux train loss: 5.406 Val loss: 4.288 Aux val loss: 2.593 Train MAE: 18.633 Val MAE: 19.192 Epoch time: 200.142 seconds \n",
      "Epoch: 67 Train loss: 8.774 Aux train loss: 5.318 Val loss: 4.224 Aux val loss: 2.603 Train MAE: 16.978 Val MAE: 18.825 Epoch time: 202.808 seconds \n",
      "Epoch: 68 Train loss: 8.673 Aux train loss: 5.263 Val loss: 4.285 Aux val loss: 2.650 Train MAE: 17.239 Val MAE: 17.511 Epoch time: 206.861 seconds \n",
      "Epoch: 69 Train loss: 9.044 Aux train loss: 5.513 Val loss: 4.247 Aux val loss: 2.576 Train MAE: 16.417 Val MAE: 23.503 Epoch time: 206.008 seconds \n",
      "Epoch: 70 Train loss: 8.737 Aux train loss: 5.299 Val loss: 4.215 Aux val loss: 2.563 Train MAE: 15.582 Val MAE: 16.843 Epoch time: 208.023 seconds \n",
      "Epoch: 71 Train loss: 8.413 Aux train loss: 5.136 Val loss: 4.317 Aux val loss: 2.520 Train MAE: 15.389 Val MAE: 22.357 Epoch time: 205.172 seconds \n",
      "Epoch: 72 Train loss: 8.658 Aux train loss: 5.226 Val loss: 4.232 Aux val loss: 2.592 Train MAE: 17.772 Val MAE: 16.726 Epoch time: 205.563 seconds \n",
      "Epoch: 73 Train loss: 8.781 Aux train loss: 5.305 Val loss: 4.305 Aux val loss: 2.565 Train MAE: 18.290 Val MAE: 22.289 Epoch time: 205.533 seconds \n",
      "Epoch: 74 Train loss: 8.971 Aux train loss: 5.424 Val loss: 4.464 Aux val loss: 2.714 Train MAE: 17.158 Val MAE: 20.669 Epoch time: 207.180 seconds \n",
      "Epoch: 75 Train loss: 8.515 Aux train loss: 5.170 Val loss: 4.238 Aux val loss: 2.590 Train MAE: 16.616 Val MAE: 17.876 Epoch time: 205.759 seconds \n",
      "Epoch: 76 Train loss: 8.251 Aux train loss: 5.019 Val loss: 4.199 Aux val loss: 2.542 Train MAE: 16.065 Val MAE: 15.669 Epoch time: 204.714 seconds best\n",
      "Epoch: 77 Train loss: 8.449 Aux train loss: 5.091 Val loss: 4.291 Aux val loss: 2.570 Train MAE: 15.286 Val MAE: 23.078 Epoch time: 205.600 seconds \n",
      "Epoch: 78 Train loss: 8.022 Aux train loss: 4.861 Val loss: 4.185 Aux val loss: 2.558 Train MAE: 16.628 Val MAE: 15.718 Epoch time: 205.653 seconds \n",
      "Epoch: 79 Train loss: 8.980 Aux train loss: 5.470 Val loss: 4.266 Aux val loss: 2.568 Train MAE: 17.956 Val MAE: 19.488 Epoch time: 206.942 seconds \n",
      "Epoch: 80 Train loss: 8.138 Aux train loss: 4.957 Val loss: 4.281 Aux val loss: 2.616 Train MAE: 15.484 Val MAE: 22.348 Epoch time: 206.810 seconds \n",
      "Epoch: 81 Train loss: 9.228 Aux train loss: 5.595 Val loss: 4.156 Aux val loss: 2.520 Train MAE: 16.006 Val MAE: 16.834 Epoch time: 204.876 seconds \n",
      "Epoch: 82 Train loss: 8.321 Aux train loss: 5.035 Val loss: 4.236 Aux val loss: 2.648 Train MAE: 15.229 Val MAE: 19.855 Epoch time: 205.357 seconds \n",
      "Epoch: 83 Train loss: 8.350 Aux train loss: 5.055 Val loss: 4.224 Aux val loss: 2.568 Train MAE: 15.602 Val MAE: 27.431 Epoch time: 207.533 seconds \n",
      "Epoch: 84 Train loss: 8.296 Aux train loss: 5.042 Val loss: 4.363 Aux val loss: 2.645 Train MAE: 14.889 Val MAE: 23.522 Epoch time: 208.699 seconds \n",
      "Epoch: 85 Train loss: 8.471 Aux train loss: 5.126 Val loss: 4.308 Aux val loss: 2.626 Train MAE: 14.722 Val MAE: 27.343 Epoch time: 207.283 seconds \n",
      "Epoch: 86 Train loss: 8.214 Aux train loss: 4.997 Val loss: 4.340 Aux val loss: 2.615 Train MAE: 15.109 Val MAE: 28.105 Epoch time: 204.733 seconds \n",
      "Epoch: 87 Train loss: 8.303 Aux train loss: 5.059 Val loss: 4.280 Aux val loss: 2.581 Train MAE: 15.734 Val MAE: 19.540 Epoch time: 206.745 seconds \n",
      "Epoch: 88 Train loss: 8.700 Aux train loss: 5.283 Val loss: 4.332 Aux val loss: 2.603 Train MAE: 15.790 Val MAE: 18.630 Epoch time: 205.155 seconds \n",
      "Epoch: 89 Train loss: 8.275 Aux train loss: 5.009 Val loss: 4.184 Aux val loss: 2.576 Train MAE: 15.223 Val MAE: 16.519 Epoch time: 205.942 seconds \n",
      "Epoch: 90 Train loss: 8.463 Aux train loss: 5.154 Val loss: 4.226 Aux val loss: 2.551 Train MAE: 15.879 Val MAE: 26.423 Epoch time: 206.307 seconds \n",
      "Epoch: 91 Train loss: 8.395 Aux train loss: 5.075 Val loss: 4.305 Aux val loss: 2.550 Train MAE: 16.297 Val MAE: 20.796 Epoch time: 206.222 seconds \n",
      "Epoch: 92 Train loss: 8.673 Aux train loss: 5.271 Val loss: 4.198 Aux val loss: 2.535 Train MAE: 15.865 Val MAE: 19.896 Epoch time: 207.580 seconds \n",
      "Epoch: 93 Train loss: 8.240 Aux train loss: 5.002 Val loss: 4.136 Aux val loss: 2.503 Train MAE: 14.365 Val MAE: 21.555 Epoch time: 207.195 seconds \n",
      "Epoch: 94 Train loss: 8.566 Aux train loss: 5.157 Val loss: 4.153 Aux val loss: 2.503 Train MAE: 16.051 Val MAE: 17.312 Epoch time: 206.028 seconds \n",
      "Epoch: 95 Train loss: 8.481 Aux train loss: 5.172 Val loss: 4.241 Aux val loss: 2.539 Train MAE: 15.125 Val MAE: 20.727 Epoch time: 205.334 seconds \n",
      "Epoch: 96 Train loss: 8.882 Aux train loss: 5.404 Val loss: 4.086 Aux val loss: 2.493 Train MAE: 15.614 Val MAE: 16.357 Epoch time: 206.604 seconds \n",
      "Epoch: 97 Train loss: 8.000 Aux train loss: 4.851 Val loss: 4.172 Aux val loss: 2.527 Train MAE: 15.483 Val MAE: 21.728 Epoch time: 205.196 seconds \n",
      "Epoch: 98 Train loss: 8.626 Aux train loss: 5.243 Val loss: 4.361 Aux val loss: 2.557 Train MAE: 15.643 Val MAE: 16.261 Epoch time: 205.571 seconds \n",
      "Epoch: 99 Train loss: 8.329 Aux train loss: 5.042 Val loss: 4.152 Aux val loss: 2.538 Train MAE: 15.324 Val MAE: 18.916 Epoch time: 206.637 seconds \n",
      "Epoch: 100 Train loss: 8.487 Aux train loss: 5.148 Val loss: 4.283 Aux val loss: 2.604 Train MAE: 15.538 Val MAE: 18.661 Epoch time: 204.843 seconds \n",
      "Epoch: 101 Train loss: 8.059 Aux train loss: 4.913 Val loss: 4.134 Aux val loss: 2.529 Train MAE: 15.182 Val MAE: 18.845 Epoch time: 201.943 seconds \n",
      "Epoch: 102 Train loss: 8.328 Aux train loss: 5.038 Val loss: 4.304 Aux val loss: 2.531 Train MAE: 14.845 Val MAE: 24.292 Epoch time: 199.361 seconds \n",
      "Epoch: 103 Train loss: 8.427 Aux train loss: 5.148 Val loss: 4.185 Aux val loss: 2.570 Train MAE: 14.854 Val MAE: 21.315 Epoch time: 200.956 seconds \n",
      "Epoch: 104 Train loss: 8.135 Aux train loss: 4.957 Val loss: 4.366 Aux val loss: 2.578 Train MAE: 15.154 Val MAE: 23.400 Epoch time: 201.322 seconds \n",
      "Epoch: 105 Train loss: 7.963 Aux train loss: 4.798 Val loss: 4.158 Aux val loss: 2.523 Train MAE: 15.428 Val MAE: 18.769 Epoch time: 200.767 seconds \n",
      "Epoch: 106 Train loss: 8.421 Aux train loss: 5.089 Val loss: 4.169 Aux val loss: 2.506 Train MAE: 15.210 Val MAE: 24.808 Epoch time: 200.176 seconds \n",
      "Epoch: 107 Train loss: 7.463 Aux train loss: 4.505 Val loss: 4.165 Aux val loss: 2.528 Train MAE: 14.126 Val MAE: 17.325 Epoch time: 200.070 seconds \n",
      "Epoch: 108 Train loss: 7.939 Aux train loss: 4.821 Val loss: 4.086 Aux val loss: 2.490 Train MAE: 15.277 Val MAE: 18.811 Epoch time: 200.557 seconds \n",
      "Epoch: 109 Train loss: 7.762 Aux train loss: 4.729 Val loss: 4.155 Aux val loss: 2.503 Train MAE: 13.695 Val MAE: 18.823 Epoch time: 200.151 seconds \n",
      "Epoch: 110 Train loss: 8.071 Aux train loss: 4.895 Val loss: 4.215 Aux val loss: 2.506 Train MAE: 15.598 Val MAE: 25.274 Epoch time: 199.338 seconds \n",
      "Epoch: 111 Train loss: 7.999 Aux train loss: 4.854 Val loss: 4.268 Aux val loss: 2.702 Train MAE: 14.438 Val MAE: 18.794 Epoch time: 199.474 seconds \n",
      "Epoch: 112 Train loss: 7.832 Aux train loss: 4.744 Val loss: 4.213 Aux val loss: 2.512 Train MAE: 13.817 Val MAE: 24.021 Epoch time: 200.314 seconds \n",
      "Epoch: 113 Train loss: 8.142 Aux train loss: 4.933 Val loss: 4.152 Aux val loss: 2.662 Train MAE: 15.345 Val MAE: 23.121 Epoch time: 199.661 seconds \n",
      "Epoch: 114 Train loss: 7.978 Aux train loss: 4.845 Val loss: 4.085 Aux val loss: 2.479 Train MAE: 15.760 Val MAE: 15.553 Epoch time: 199.725 seconds best\n",
      "Epoch: 115 Train loss: 7.799 Aux train loss: 4.723 Val loss: 4.491 Aux val loss: 2.495 Train MAE: 13.862 Val MAE: 22.125 Epoch time: 198.753 seconds \n",
      "Epoch: 116 Train loss: 7.668 Aux train loss: 4.647 Val loss: 4.072 Aux val loss: 2.499 Train MAE: 13.465 Val MAE: 15.116 Epoch time: 198.964 seconds best\n",
      "Epoch: 117 Train loss: 8.205 Aux train loss: 4.955 Val loss: 4.103 Aux val loss: 2.482 Train MAE: 15.392 Val MAE: 23.237 Epoch time: 199.758 seconds \n",
      "Epoch: 118 Train loss: 8.094 Aux train loss: 4.922 Val loss: 4.246 Aux val loss: 2.582 Train MAE: 15.113 Val MAE: 22.648 Epoch time: 199.807 seconds \n",
      "Epoch: 119 Train loss: 7.691 Aux train loss: 4.674 Val loss: 4.173 Aux val loss: 2.564 Train MAE: 14.805 Val MAE: 22.443 Epoch time: 199.520 seconds \n",
      "Epoch: 120 Train loss: 8.606 Aux train loss: 5.239 Val loss: 4.252 Aux val loss: 2.531 Train MAE: 15.117 Val MAE: 26.134 Epoch time: 198.559 seconds \n",
      "Epoch: 121 Train loss: 8.037 Aux train loss: 4.879 Val loss: 4.152 Aux val loss: 2.537 Train MAE: 15.413 Val MAE: 22.924 Epoch time: 198.702 seconds \n",
      "Epoch: 122 Train loss: 8.250 Aux train loss: 5.016 Val loss: 4.356 Aux val loss: 2.606 Train MAE: 14.186 Val MAE: 21.386 Epoch time: 197.645 seconds \n",
      "Epoch: 123 Train loss: 8.255 Aux train loss: 5.002 Val loss: 4.205 Aux val loss: 2.510 Train MAE: 16.099 Val MAE: 18.447 Epoch time: 198.492 seconds \n",
      "Epoch: 124 Train loss: 8.236 Aux train loss: 5.008 Val loss: 4.119 Aux val loss: 2.473 Train MAE: 14.806 Val MAE: 18.949 Epoch time: 197.632 seconds \n",
      "Epoch: 125 Train loss: 8.390 Aux train loss: 5.122 Val loss: 4.143 Aux val loss: 2.593 Train MAE: 15.044 Val MAE: 20.554 Epoch time: 197.758 seconds \n",
      "Epoch: 126 Train loss: 7.814 Aux train loss: 4.756 Val loss: 4.064 Aux val loss: 2.487 Train MAE: 15.109 Val MAE: 14.812 Epoch time: 197.927 seconds best\n",
      "Epoch: 127 Train loss: 7.516 Aux train loss: 4.572 Val loss: 4.304 Aux val loss: 2.476 Train MAE: 14.533 Val MAE: 23.581 Epoch time: 197.516 seconds \n",
      "Epoch: 128 Train loss: 7.911 Aux train loss: 4.826 Val loss: 4.096 Aux val loss: 2.478 Train MAE: 14.275 Val MAE: 16.388 Epoch time: 197.881 seconds \n",
      "Epoch: 129 Train loss: 7.608 Aux train loss: 4.614 Val loss: 4.064 Aux val loss: 2.477 Train MAE: 13.712 Val MAE: 17.352 Epoch time: 197.908 seconds \n",
      "Epoch: 130 Train loss: 8.355 Aux train loss: 5.073 Val loss: 4.231 Aux val loss: 2.593 Train MAE: 13.565 Val MAE: 23.972 Epoch time: 199.642 seconds \n",
      "Epoch: 131 Train loss: 7.942 Aux train loss: 4.836 Val loss: 8.458 Aux val loss: 2.470 Train MAE: 14.909 Val MAE: 20.916 Epoch time: 198.644 seconds \n",
      "Epoch: 132 Train loss: 8.172 Aux train loss: 4.947 Val loss: 4.092 Aux val loss: 2.489 Train MAE: 13.955 Val MAE: 14.743 Epoch time: 198.716 seconds best\n",
      "Epoch: 133 Train loss: 8.145 Aux train loss: 4.933 Val loss: 4.328 Aux val loss: 2.486 Train MAE: 13.863 Val MAE: 14.000 Epoch time: 198.820 seconds best\n",
      "Epoch: 134 Train loss: 8.152 Aux train loss: 4.964 Val loss: 4.201 Aux val loss: 2.515 Train MAE: 15.336 Val MAE: 20.377 Epoch time: 198.320 seconds \n",
      "Epoch: 135 Train loss: 7.835 Aux train loss: 4.751 Val loss: 4.077 Aux val loss: 2.431 Train MAE: 13.928 Val MAE: 20.848 Epoch time: 199.022 seconds \n",
      "Epoch: 136 Train loss: 7.595 Aux train loss: 4.607 Val loss: 4.155 Aux val loss: 2.497 Train MAE: 13.563 Val MAE: 18.630 Epoch time: 199.587 seconds \n",
      "Epoch: 137 Train loss: 7.547 Aux train loss: 4.569 Val loss: 4.093 Aux val loss: 2.464 Train MAE: 14.294 Val MAE: 26.134 Epoch time: 199.252 seconds \n",
      "Epoch: 138 Train loss: 7.803 Aux train loss: 4.740 Val loss: 4.079 Aux val loss: 2.455 Train MAE: 15.061 Val MAE: 25.202 Epoch time: 198.677 seconds \n",
      "Epoch: 139 Train loss: 7.516 Aux train loss: 4.594 Val loss: 4.058 Aux val loss: 2.447 Train MAE: 13.577 Val MAE: 21.386 Epoch time: 199.800 seconds \n",
      "Epoch: 140 Train loss: 7.613 Aux train loss: 4.618 Val loss: 4.191 Aux val loss: 2.493 Train MAE: 13.825 Val MAE: 23.505 Epoch time: 198.470 seconds \n",
      "Epoch: 141 Train loss: 8.015 Aux train loss: 4.884 Val loss: 4.094 Aux val loss: 2.536 Train MAE: 15.069 Val MAE: 22.338 Epoch time: 198.555 seconds \n",
      "Epoch: 142 Train loss: 7.747 Aux train loss: 4.714 Val loss: 4.086 Aux val loss: 2.475 Train MAE: 13.516 Val MAE: 16.890 Epoch time: 198.607 seconds \n",
      "Epoch: 143 Train loss: 7.974 Aux train loss: 4.871 Val loss: 4.176 Aux val loss: 2.534 Train MAE: 14.018 Val MAE: 23.669 Epoch time: 204.577 seconds \n",
      "Epoch: 144 Train loss: 7.356 Aux train loss: 4.475 Val loss: 4.366 Aux val loss: 2.641 Train MAE: 12.822 Val MAE: 30.356 Epoch time: 205.180 seconds \n",
      "Epoch: 145 Train loss: 7.807 Aux train loss: 4.731 Val loss: 4.052 Aux val loss: 2.467 Train MAE: 15.227 Val MAE: 21.400 Epoch time: 205.246 seconds \n",
      "Epoch: 146 Train loss: 7.835 Aux train loss: 4.776 Val loss: 4.045 Aux val loss: 2.458 Train MAE: 12.673 Val MAE: 16.019 Epoch time: 204.375 seconds \n",
      "Epoch: 147 Train loss: 8.008 Aux train loss: 4.839 Val loss: 4.180 Aux val loss: 2.467 Train MAE: 13.558 Val MAE: 21.449 Epoch time: 207.053 seconds \n",
      "Epoch: 148 Train loss: 7.686 Aux train loss: 4.670 Val loss: 4.054 Aux val loss: 2.462 Train MAE: 13.460 Val MAE: 18.815 Epoch time: 204.292 seconds \n",
      "Epoch: 149 Train loss: 8.007 Aux train loss: 4.842 Val loss: 4.045 Aux val loss: 2.493 Train MAE: 12.521 Val MAE: 13.770 Epoch time: 204.780 seconds best\n",
      "Epoch: 150 Train loss: 7.692 Aux train loss: 4.680 Val loss: 4.036 Aux val loss: 2.465 Train MAE: 13.138 Val MAE: 21.499 Epoch time: 204.661 seconds \n",
      "Epoch: 151 Train loss: 7.886 Aux train loss: 4.778 Val loss: 4.095 Aux val loss: 2.508 Train MAE: 12.114 Val MAE: 27.450 Epoch time: 205.048 seconds \n",
      "Epoch: 152 Train loss: 7.916 Aux train loss: 4.820 Val loss: 4.066 Aux val loss: 2.451 Train MAE: 12.865 Val MAE: 25.683 Epoch time: 205.175 seconds \n",
      "Epoch: 153 Train loss: 7.841 Aux train loss: 4.786 Val loss: 4.138 Aux val loss: 2.475 Train MAE: 13.758 Val MAE: 19.512 Epoch time: 204.837 seconds \n",
      "Epoch: 154 Train loss: 8.373 Aux train loss: 5.194 Val loss: 4.209 Aux val loss: 2.591 Train MAE: 13.482 Val MAE: 25.614 Epoch time: 203.697 seconds \n",
      "Epoch: 155 Train loss: 7.501 Aux train loss: 4.585 Val loss: 4.058 Aux val loss: 2.491 Train MAE: 12.288 Val MAE: 18.266 Epoch time: 205.100 seconds \n",
      "Epoch: 156 Train loss: 8.016 Aux train loss: 4.846 Val loss: 4.026 Aux val loss: 2.459 Train MAE: 14.511 Val MAE: 15.601 Epoch time: 206.347 seconds \n",
      "Epoch: 157 Train loss: 7.918 Aux train loss: 4.769 Val loss: 4.120 Aux val loss: 2.490 Train MAE: 14.685 Val MAE: 17.663 Epoch time: 204.966 seconds \n",
      "Epoch: 158 Train loss: 7.261 Aux train loss: 4.432 Val loss: 4.210 Aux val loss: 2.571 Train MAE: 13.091 Val MAE: 24.892 Epoch time: 209.267 seconds \n",
      "Epoch: 159 Train loss: 7.577 Aux train loss: 4.617 Val loss: 4.007 Aux val loss: 2.429 Train MAE: 12.981 Val MAE: 17.759 Epoch time: 205.671 seconds \n",
      "Epoch: 160 Train loss: 7.737 Aux train loss: 4.689 Val loss: 4.189 Aux val loss: 2.494 Train MAE: 12.751 Val MAE: 20.244 Epoch time: 205.472 seconds \n",
      "Epoch: 161 Train loss: 7.326 Aux train loss: 4.467 Val loss: 4.061 Aux val loss: 2.426 Train MAE: 12.896 Val MAE: 15.672 Epoch time: 206.840 seconds \n",
      "Epoch: 162 Train loss: 7.769 Aux train loss: 4.725 Val loss: 4.164 Aux val loss: 2.527 Train MAE: 13.443 Val MAE: 16.452 Epoch time: 204.699 seconds \n",
      "Epoch: 163 Train loss: 7.385 Aux train loss: 4.496 Val loss: 4.106 Aux val loss: 2.509 Train MAE: 15.379 Val MAE: 18.777 Epoch time: 205.222 seconds \n",
      "Epoch: 164 Train loss: 7.372 Aux train loss: 4.481 Val loss: 4.089 Aux val loss: 2.475 Train MAE: 12.960 Val MAE: 28.643 Epoch time: 204.620 seconds \n",
      "Epoch: 165 Train loss: 8.053 Aux train loss: 4.885 Val loss: 4.074 Aux val loss: 2.470 Train MAE: 13.396 Val MAE: 21.851 Epoch time: 204.094 seconds \n",
      "Epoch: 166 Train loss: 7.076 Aux train loss: 4.336 Val loss: 4.047 Aux val loss: 2.440 Train MAE: 12.506 Val MAE: 19.567 Epoch time: 205.181 seconds \n",
      "Epoch: 167 Train loss: 7.572 Aux train loss: 4.973 Val loss: 4.070 Aux val loss: 2.509 Train MAE: 12.013 Val MAE: 17.045 Epoch time: 208.158 seconds \n",
      "Epoch: 168 Train loss: 7.900 Aux train loss: 4.799 Val loss: 4.166 Aux val loss: 2.460 Train MAE: 13.556 Val MAE: 20.058 Epoch time: 206.242 seconds \n",
      "Epoch: 169 Train loss: 7.175 Aux train loss: 4.366 Val loss: 4.042 Aux val loss: 2.459 Train MAE: 12.604 Val MAE: 18.560 Epoch time: 205.860 seconds \n",
      "Epoch: 170 Train loss: 7.181 Aux train loss: 4.405 Val loss: 4.070 Aux val loss: 2.518 Train MAE: 13.145 Val MAE: 20.927 Epoch time: 205.999 seconds \n",
      "Epoch: 171 Train loss: 7.798 Aux train loss: 4.733 Val loss: 4.024 Aux val loss: 2.450 Train MAE: 13.654 Val MAE: 24.921 Epoch time: 205.100 seconds \n",
      "Epoch: 172 Train loss: 8.042 Aux train loss: 4.898 Val loss: 4.182 Aux val loss: 2.506 Train MAE: 14.032 Val MAE: 24.408 Epoch time: 207.362 seconds \n",
      "Epoch: 173 Train loss: 7.158 Aux train loss: 4.341 Val loss: 3.944 Aux val loss: 2.385 Train MAE: 13.544 Val MAE: 13.894 Epoch time: 206.174 seconds \n",
      "Epoch: 174 Train loss: 61.352 Aux train loss: 4.565 Val loss: 4.162 Aux val loss: 2.527 Train MAE: 15.940 Val MAE: 19.884 Epoch time: 205.392 seconds \n",
      "Epoch: 175 Train loss: 7.617 Aux train loss: 4.650 Val loss: 4.076 Aux val loss: 2.505 Train MAE: 12.060 Val MAE: 14.832 Epoch time: 204.778 seconds \n",
      "Epoch: 176 Train loss: 7.660 Aux train loss: 4.668 Val loss: 3.979 Aux val loss: 2.419 Train MAE: 12.043 Val MAE: 14.635 Epoch time: 204.911 seconds \n",
      "Epoch: 177 Train loss: 7.349 Aux train loss: 4.448 Val loss: 3.980 Aux val loss: 2.427 Train MAE: 12.655 Val MAE: 15.021 Epoch time: 203.200 seconds \n",
      "Epoch: 178 Train loss: 9.139 Aux train loss: 4.903 Val loss: 4.116 Aux val loss: 2.468 Train MAE: 12.857 Val MAE: 17.397 Epoch time: 199.997 seconds \n",
      "Epoch: 179 Train loss: 8.044 Aux train loss: 4.886 Val loss: 4.103 Aux val loss: 2.473 Train MAE: 13.652 Val MAE: 25.173 Epoch time: 199.502 seconds \n",
      "Epoch: 180 Train loss: 7.188 Aux train loss: 4.358 Val loss: 3.971 Aux val loss: 2.422 Train MAE: 13.475 Val MAE: 13.400 Epoch time: 199.396 seconds best\n",
      "Epoch: 181 Train loss: 7.352 Aux train loss: 4.486 Val loss: 3.991 Aux val loss: 2.454 Train MAE: 12.687 Val MAE: 14.652 Epoch time: 199.953 seconds \n",
      "Epoch: 182 Train loss: 7.499 Aux train loss: 4.558 Val loss: 3.974 Aux val loss: 2.427 Train MAE: 13.479 Val MAE: 21.224 Epoch time: 200.121 seconds \n",
      "Epoch: 183 Train loss: 7.902 Aux train loss: 4.844 Val loss: 3.983 Aux val loss: 2.422 Train MAE: 12.639 Val MAE: 16.429 Epoch time: 200.759 seconds \n",
      "Epoch: 184 Train loss: 7.616 Aux train loss: 4.630 Val loss: 4.091 Aux val loss: 2.401 Train MAE: 12.808 Val MAE: 21.852 Epoch time: 200.349 seconds \n",
      "Epoch: 185 Train loss: 7.514 Aux train loss: 4.581 Val loss: 3.978 Aux val loss: 2.407 Train MAE: 13.759 Val MAE: 16.089 Epoch time: 199.472 seconds \n",
      "Epoch: 186 Train loss: 7.608 Aux train loss: 4.639 Val loss: 3.977 Aux val loss: 2.436 Train MAE: 12.445 Val MAE: 17.180 Epoch time: 198.930 seconds \n",
      "Epoch: 187 Train loss: 8.145 Aux train loss: 4.946 Val loss: 4.090 Aux val loss: 2.438 Train MAE: 13.681 Val MAE: 25.042 Epoch time: 198.832 seconds \n",
      "Epoch: 188 Train loss: 7.687 Aux train loss: 4.686 Val loss: 4.085 Aux val loss: 2.474 Train MAE: 13.152 Val MAE: 19.600 Epoch time: 199.135 seconds \n",
      "Epoch: 189 Train loss: 7.915 Aux train loss: 4.800 Val loss: 4.068 Aux val loss: 2.445 Train MAE: 12.927 Val MAE: 16.024 Epoch time: 199.838 seconds \n",
      "Epoch: 190 Train loss: 7.571 Aux train loss: 4.636 Val loss: 4.070 Aux val loss: 2.457 Train MAE: 12.473 Val MAE: 16.003 Epoch time: 200.431 seconds \n",
      "Epoch: 191 Train loss: 7.302 Aux train loss: 4.459 Val loss: 4.108 Aux val loss: 2.448 Train MAE: 12.233 Val MAE: 18.826 Epoch time: 199.952 seconds \n",
      "Epoch: 192 Train loss: 7.594 Aux train loss: 4.642 Val loss: 4.055 Aux val loss: 2.518 Train MAE: 12.983 Val MAE: 16.885 Epoch time: 199.313 seconds \n",
      "Epoch: 193 Train loss: 8.058 Aux train loss: 4.886 Val loss: 4.097 Aux val loss: 2.479 Train MAE: 13.815 Val MAE: 24.030 Epoch time: 199.540 seconds \n",
      "Epoch: 194 Train loss: 8.011 Aux train loss: 4.862 Val loss: 4.056 Aux val loss: 2.426 Train MAE: 13.162 Val MAE: 17.436 Epoch time: 199.912 seconds \n",
      "Epoch: 195 Train loss: 7.639 Aux train loss: 4.612 Val loss: 4.147 Aux val loss: 2.439 Train MAE: 13.115 Val MAE: 25.654 Epoch time: 199.643 seconds \n",
      "Epoch: 196 Train loss: 7.826 Aux train loss: 4.794 Val loss: 4.161 Aux val loss: 2.467 Train MAE: 12.794 Val MAE: 21.903 Epoch time: 199.716 seconds \n",
      "Epoch: 197 Train loss: 7.112 Aux train loss: 4.332 Val loss: 4.102 Aux val loss: 2.454 Train MAE: 12.830 Val MAE: 24.588 Epoch time: 199.698 seconds \n",
      "Epoch: 198 Train loss: 8.139 Aux train loss: 4.951 Val loss: 4.289 Aux val loss: 2.425 Train MAE: 12.537 Val MAE: 16.724 Epoch time: 198.617 seconds \n",
      "Epoch: 199 Train loss: 8.204 Aux train loss: 4.852 Val loss: 3.951 Aux val loss: 2.399 Train MAE: 14.684 Val MAE: 18.644 Epoch time: 199.223 seconds \n",
      "Epoch: 200 Train loss: 7.168 Aux train loss: 4.360 Val loss: 4.048 Aux val loss: 2.452 Train MAE: 13.870 Val MAE: 29.524 Epoch time: 198.585 seconds \n",
      "Epoch: 201 Train loss: 6.848 Aux train loss: 4.195 Val loss: 3.856 Aux val loss: 2.354 Train MAE: 8.916 Val MAE: 14.276 Epoch time: 199.358 seconds \n",
      "Epoch: 202 Train loss: 6.689 Aux train loss: 4.085 Val loss: 3.851 Aux val loss: 2.343 Train MAE: 8.082 Val MAE: 13.899 Epoch time: 199.464 seconds \n",
      "Epoch: 203 Train loss: 6.878 Aux train loss: 4.190 Val loss: 3.855 Aux val loss: 2.344 Train MAE: 7.900 Val MAE: 15.048 Epoch time: 198.385 seconds \n",
      "Epoch: 204 Train loss: 6.791 Aux train loss: 4.150 Val loss: 3.882 Aux val loss: 2.368 Train MAE: 7.834 Val MAE: 14.802 Epoch time: 198.620 seconds \n",
      "Epoch: 205 Train loss: 6.814 Aux train loss: 4.170 Val loss: 3.880 Aux val loss: 2.374 Train MAE: 7.949 Val MAE: 15.147 Epoch time: 198.542 seconds \n",
      "Epoch: 206 Train loss: 6.593 Aux train loss: 4.027 Val loss: 3.883 Aux val loss: 2.362 Train MAE: 7.545 Val MAE: 14.311 Epoch time: 198.760 seconds \n",
      "Epoch: 207 Train loss: 7.059 Aux train loss: 4.320 Val loss: 3.850 Aux val loss: 2.340 Train MAE: 8.007 Val MAE: 14.328 Epoch time: 198.070 seconds \n",
      "Epoch: 208 Train loss: 6.557 Aux train loss: 4.013 Val loss: 3.883 Aux val loss: 2.369 Train MAE: 7.409 Val MAE: 16.060 Epoch time: 198.034 seconds \n",
      "Epoch: 209 Train loss: 6.968 Aux train loss: 4.262 Val loss: 3.934 Aux val loss: 2.376 Train MAE: 7.346 Val MAE: 16.785 Epoch time: 198.350 seconds \n",
      "Epoch: 210 Train loss: 6.801 Aux train loss: 4.155 Val loss: 3.920 Aux val loss: 2.374 Train MAE: 7.485 Val MAE: 17.831 Epoch time: 198.803 seconds \n",
      "Epoch: 211 Train loss: 6.890 Aux train loss: 4.213 Val loss: 3.883 Aux val loss: 2.361 Train MAE: 7.497 Val MAE: 16.302 Epoch time: 198.719 seconds \n",
      "Epoch: 212 Train loss: 6.643 Aux train loss: 4.061 Val loss: 3.895 Aux val loss: 2.352 Train MAE: 7.155 Val MAE: 15.338 Epoch time: 198.401 seconds \n",
      "Epoch: 213 Train loss: 6.580 Aux train loss: 4.031 Val loss: 3.943 Aux val loss: 2.371 Train MAE: 6.969 Val MAE: 16.073 Epoch time: 198.379 seconds \n",
      "Epoch: 214 Train loss: 6.710 Aux train loss: 4.094 Val loss: 3.883 Aux val loss: 2.351 Train MAE: 7.160 Val MAE: 15.073 Epoch time: 198.850 seconds \n",
      "Epoch: 215 Train loss: 6.726 Aux train loss: 4.104 Val loss: 3.880 Aux val loss: 2.345 Train MAE: 7.076 Val MAE: 17.910 Epoch time: 198.569 seconds \n",
      "Epoch: 216 Train loss: 6.351 Aux train loss: 3.898 Val loss: 3.956 Aux val loss: 2.363 Train MAE: 7.082 Val MAE: 17.517 Epoch time: 198.092 seconds \n",
      "Epoch: 217 Train loss: 6.705 Aux train loss: 4.103 Val loss: 3.873 Aux val loss: 2.348 Train MAE: 7.145 Val MAE: 15.313 Epoch time: 198.217 seconds \n",
      "Epoch: 218 Train loss: 6.359 Aux train loss: 3.900 Val loss: 3.897 Aux val loss: 2.353 Train MAE: 7.009 Val MAE: 14.642 Epoch time: 198.910 seconds \n",
      "Epoch: 219 Train loss: 6.681 Aux train loss: 4.083 Val loss: 3.854 Aux val loss: 2.335 Train MAE: 7.288 Val MAE: 14.930 Epoch time: 199.278 seconds \n",
      "Epoch: 220 Train loss: 6.871 Aux train loss: 4.195 Val loss: 3.882 Aux val loss: 2.356 Train MAE: 7.287 Val MAE: 16.204 Epoch time: 197.911 seconds \n",
      "Epoch: 221 Train loss: 6.298 Aux train loss: 3.847 Val loss: 3.888 Aux val loss: 2.354 Train MAE: 7.033 Val MAE: 14.922 Epoch time: 198.002 seconds \n",
      "Epoch: 222 Train loss: 6.774 Aux train loss: 4.137 Val loss: 3.891 Aux val loss: 2.363 Train MAE: 7.019 Val MAE: 14.970 Epoch time: 198.178 seconds \n",
      "Epoch: 223 Train loss: 7.505 Aux train loss: 4.573 Val loss: 3.898 Aux val loss: 2.349 Train MAE: 7.398 Val MAE: 15.419 Epoch time: 197.796 seconds \n",
      "Epoch: 224 Train loss: 7.086 Aux train loss: 4.327 Val loss: 3.888 Aux val loss: 2.348 Train MAE: 6.700 Val MAE: 15.641 Epoch time: 198.489 seconds \n",
      "Epoch: 225 Train loss: 6.692 Aux train loss: 4.089 Val loss: 3.884 Aux val loss: 2.349 Train MAE: 7.092 Val MAE: 15.148 Epoch time: 198.192 seconds \n",
      "Epoch: 226 Train loss: 6.985 Aux train loss: 4.270 Val loss: 3.892 Aux val loss: 2.363 Train MAE: 7.232 Val MAE: 15.379 Epoch time: 197.892 seconds \n",
      "Epoch: 227 Train loss: 6.734 Aux train loss: 4.109 Val loss: 3.872 Aux val loss: 2.341 Train MAE: 6.987 Val MAE: 16.499 Epoch time: 197.746 seconds \n",
      "Epoch: 228 Train loss: 6.805 Aux train loss: 4.160 Val loss: 3.889 Aux val loss: 2.338 Train MAE: 7.069 Val MAE: 16.112 Epoch time: 197.629 seconds \n",
      "Epoch: 229 Train loss: 7.377 Aux train loss: 4.497 Val loss: 3.869 Aux val loss: 2.338 Train MAE: 6.954 Val MAE: 15.404 Epoch time: 197.522 seconds \n",
      "Epoch: 230 Train loss: 6.815 Aux train loss: 4.159 Val loss: 3.885 Aux val loss: 2.352 Train MAE: 6.960 Val MAE: 15.297 Epoch time: 198.251 seconds \n",
      "Epoch: 231 Train loss: 6.630 Aux train loss: 4.061 Val loss: 3.855 Aux val loss: 2.336 Train MAE: 6.868 Val MAE: 14.687 Epoch time: 197.935 seconds \n",
      "Epoch: 232 Train loss: 6.205 Aux train loss: 3.796 Val loss: 3.880 Aux val loss: 2.352 Train MAE: 7.053 Val MAE: 16.132 Epoch time: 198.069 seconds \n",
      "Epoch: 233 Train loss: 6.608 Aux train loss: 4.048 Val loss: 3.910 Aux val loss: 2.349 Train MAE: 6.999 Val MAE: 16.213 Epoch time: 197.929 seconds \n",
      "Epoch: 234 Train loss: 6.675 Aux train loss: 4.084 Val loss: 3.942 Aux val loss: 2.369 Train MAE: 7.043 Val MAE: 18.518 Epoch time: 197.854 seconds \n",
      "Epoch: 235 Train loss: 6.576 Aux train loss: 4.018 Val loss: 3.908 Aux val loss: 2.341 Train MAE: 7.064 Val MAE: 17.848 Epoch time: 197.939 seconds \n",
      "Epoch: 236 Train loss: 7.465 Aux train loss: 4.550 Val loss: 3.915 Aux val loss: 2.345 Train MAE: 7.167 Val MAE: 16.428 Epoch time: 197.633 seconds \n",
      "Epoch: 237 Train loss: 6.583 Aux train loss: 4.029 Val loss: 3.907 Aux val loss: 2.340 Train MAE: 7.009 Val MAE: 16.643 Epoch time: 198.906 seconds \n",
      "Epoch: 238 Train loss: 6.526 Aux train loss: 3.996 Val loss: 3.873 Aux val loss: 2.333 Train MAE: 7.133 Val MAE: 14.868 Epoch time: 197.979 seconds \n",
      "Epoch: 239 Train loss: 7.049 Aux train loss: 4.306 Val loss: 3.863 Aux val loss: 2.334 Train MAE: 6.986 Val MAE: 15.632 Epoch time: 197.804 seconds \n",
      "Epoch: 240 Train loss: 6.901 Aux train loss: 4.226 Val loss: 3.887 Aux val loss: 2.351 Train MAE: 6.966 Val MAE: 14.820 Epoch time: 197.991 seconds \n",
      "Epoch: 241 Train loss: 6.637 Aux train loss: 4.057 Val loss: 3.892 Aux val loss: 2.360 Train MAE: 6.711 Val MAE: 14.510 Epoch time: 197.487 seconds \n",
      "Epoch: 242 Train loss: 6.318 Aux train loss: 3.858 Val loss: 3.917 Aux val loss: 2.357 Train MAE: 6.721 Val MAE: 18.419 Epoch time: 197.898 seconds \n",
      "Epoch: 243 Train loss: 6.979 Aux train loss: 4.255 Val loss: 3.865 Aux val loss: 2.344 Train MAE: 6.761 Val MAE: 13.529 Epoch time: 197.681 seconds \n",
      "Epoch: 244 Train loss: 6.703 Aux train loss: 4.106 Val loss: 3.865 Aux val loss: 2.340 Train MAE: 6.935 Val MAE: 14.376 Epoch time: 198.057 seconds \n",
      "Epoch: 245 Train loss: 7.110 Aux train loss: 4.348 Val loss: 3.942 Aux val loss: 2.400 Train MAE: 6.695 Val MAE: 18.087 Epoch time: 197.706 seconds \n",
      "Epoch: 246 Train loss: 6.280 Aux train loss: 3.839 Val loss: 3.888 Aux val loss: 2.341 Train MAE: 6.644 Val MAE: 13.996 Epoch time: 198.279 seconds \n",
      "Epoch: 247 Train loss: 7.159 Aux train loss: 4.381 Val loss: 3.934 Aux val loss: 2.371 Train MAE: 6.943 Val MAE: 17.270 Epoch time: 197.656 seconds \n",
      "Epoch: 248 Train loss: 6.572 Aux train loss: 4.011 Val loss: 3.951 Aux val loss: 2.373 Train MAE: 6.814 Val MAE: 15.951 Epoch time: 197.830 seconds \n",
      "Epoch: 249 Train loss: 6.553 Aux train loss: 4.011 Val loss: 3.919 Aux val loss: 2.360 Train MAE: 6.813 Val MAE: 16.806 Epoch time: 197.402 seconds \n",
      "Epoch: 250 Train loss: 6.543 Aux train loss: 3.999 Val loss: 3.940 Aux val loss: 2.366 Train MAE: 6.824 Val MAE: 16.094 Epoch time: 197.842 seconds \n",
      "Epoch: 251 Train loss: 6.597 Aux train loss: 4.045 Val loss: 3.926 Aux val loss: 2.376 Train MAE: 6.497 Val MAE: 18.369 Epoch time: 197.960 seconds \n",
      "Epoch: 252 Train loss: 6.079 Aux train loss: 3.732 Val loss: 3.888 Aux val loss: 2.330 Train MAE: 6.782 Val MAE: 16.285 Epoch time: 198.063 seconds \n",
      "Epoch: 253 Train loss: 6.703 Aux train loss: 4.104 Val loss: 3.906 Aux val loss: 2.354 Train MAE: 6.794 Val MAE: 15.738 Epoch time: 197.353 seconds \n",
      "Epoch: 254 Train loss: 6.665 Aux train loss: 4.068 Val loss: 3.900 Aux val loss: 2.344 Train MAE: 6.736 Val MAE: 16.374 Epoch time: 197.824 seconds \n",
      "Epoch: 255 Train loss: 6.852 Aux train loss: 4.179 Val loss: 3.872 Aux val loss: 2.344 Train MAE: 6.512 Val MAE: 16.998 Epoch time: 198.269 seconds \n",
      "Epoch: 256 Train loss: 6.241 Aux train loss: 3.819 Val loss: 3.886 Aux val loss: 2.336 Train MAE: 6.344 Val MAE: 15.846 Epoch time: 197.681 seconds \n",
      "Epoch: 257 Train loss: 6.177 Aux train loss: 3.780 Val loss: 3.878 Aux val loss: 2.340 Train MAE: 6.564 Val MAE: 17.137 Epoch time: 197.718 seconds \n",
      "Epoch: 258 Train loss: 6.055 Aux train loss: 3.707 Val loss: 3.870 Aux val loss: 2.327 Train MAE: 6.331 Val MAE: 16.723 Epoch time: 197.987 seconds \n",
      "Epoch: 259 Train loss: 6.601 Aux train loss: 4.031 Val loss: 3.881 Aux val loss: 2.327 Train MAE: 6.584 Val MAE: 15.731 Epoch time: 198.655 seconds \n",
      "Epoch: 260 Train loss: 6.535 Aux train loss: 3.995 Val loss: 3.874 Aux val loss: 2.348 Train MAE: 6.993 Val MAE: 15.799 Epoch time: 197.953 seconds \n",
      "Epoch: 261 Train loss: 6.360 Aux train loss: 3.888 Val loss: 3.899 Aux val loss: 2.349 Train MAE: 6.819 Val MAE: 16.161 Epoch time: 197.722 seconds \n",
      "Epoch: 262 Train loss: 6.125 Aux train loss: 3.750 Val loss: 3.870 Aux val loss: 2.336 Train MAE: 6.662 Val MAE: 14.515 Epoch time: 197.794 seconds \n",
      "Epoch: 263 Train loss: 6.395 Aux train loss: 3.910 Val loss: 3.858 Aux val loss: 2.326 Train MAE: 6.428 Val MAE: 14.307 Epoch time: 198.068 seconds \n",
      "Epoch: 264 Train loss: 5.962 Aux train loss: 3.639 Val loss: 3.935 Aux val loss: 2.364 Train MAE: 6.627 Val MAE: 16.977 Epoch time: 198.725 seconds \n",
      "Epoch: 265 Train loss: 6.547 Aux train loss: 3.996 Val loss: 3.951 Aux val loss: 2.373 Train MAE: 6.417 Val MAE: 18.018 Epoch time: 197.883 seconds \n",
      "Epoch: 266 Train loss: 6.324 Aux train loss: 3.836 Val loss: 3.895 Aux val loss: 2.355 Train MAE: 6.541 Val MAE: 15.812 Epoch time: 197.851 seconds \n",
      "Epoch: 267 Train loss: 6.249 Aux train loss: 3.817 Val loss: 3.862 Aux val loss: 2.341 Train MAE: 6.817 Val MAE: 14.076 Epoch time: 198.851 seconds \n",
      "Epoch: 268 Train loss: 6.006 Aux train loss: 3.665 Val loss: 3.867 Aux val loss: 2.329 Train MAE: 6.683 Val MAE: 14.243 Epoch time: 197.926 seconds \n",
      "Epoch: 269 Train loss: 6.100 Aux train loss: 3.738 Val loss: 3.889 Aux val loss: 2.339 Train MAE: 6.221 Val MAE: 15.910 Epoch time: 198.345 seconds \n",
      "Epoch: 270 Train loss: 6.474 Aux train loss: 3.957 Val loss: 3.861 Aux val loss: 2.323 Train MAE: 6.419 Val MAE: 14.373 Epoch time: 198.548 seconds \n",
      "Epoch: 271 Train loss: 6.214 Aux train loss: 3.792 Val loss: 3.860 Aux val loss: 2.342 Train MAE: 6.593 Val MAE: 14.022 Epoch time: 197.852 seconds \n",
      "Epoch: 272 Train loss: 6.868 Aux train loss: 4.190 Val loss: 3.916 Aux val loss: 2.346 Train MAE: 6.969 Val MAE: 17.394 Epoch time: 197.681 seconds \n",
      "Epoch: 273 Train loss: 6.496 Aux train loss: 3.982 Val loss: 3.866 Aux val loss: 2.343 Train MAE: 6.456 Val MAE: 15.141 Epoch time: 198.312 seconds \n",
      "Epoch: 274 Train loss: 6.562 Aux train loss: 4.010 Val loss: 3.876 Aux val loss: 2.351 Train MAE: 7.024 Val MAE: 14.772 Epoch time: 198.470 seconds \n",
      "Epoch: 275 Train loss: 6.156 Aux train loss: 3.772 Val loss: 3.868 Aux val loss: 2.318 Train MAE: 6.876 Val MAE: 18.250 Epoch time: 198.153 seconds \n",
      "Epoch: 276 Train loss: 6.559 Aux train loss: 4.009 Val loss: 3.854 Aux val loss: 2.321 Train MAE: 6.963 Val MAE: 15.765 Epoch time: 198.341 seconds \n",
      "Epoch: 277 Train loss: 6.212 Aux train loss: 3.787 Val loss: 3.902 Aux val loss: 2.332 Train MAE: 6.736 Val MAE: 15.766 Epoch time: 199.173 seconds \n",
      "Epoch: 278 Train loss: 6.727 Aux train loss: 4.116 Val loss: 3.887 Aux val loss: 2.343 Train MAE: 6.565 Val MAE: 15.543 Epoch time: 198.621 seconds \n",
      "Epoch: 279 Train loss: 6.604 Aux train loss: 4.028 Val loss: 3.907 Aux val loss: 2.362 Train MAE: 6.734 Val MAE: 15.996 Epoch time: 198.398 seconds \n",
      "Epoch: 280 Train loss: 6.162 Aux train loss: 3.771 Val loss: 3.880 Aux val loss: 2.347 Train MAE: 6.397 Val MAE: 15.663 Epoch time: 198.679 seconds \n",
      "Epoch: 281 Train loss: 6.418 Aux train loss: 3.916 Val loss: 3.861 Aux val loss: 2.340 Train MAE: 6.710 Val MAE: 14.092 Epoch time: 199.256 seconds \n",
      "Epoch: 282 Train loss: 6.121 Aux train loss: 3.737 Val loss: 3.827 Aux val loss: 2.323 Train MAE: 6.618 Val MAE: 14.819 Epoch time: 199.157 seconds \n",
      "Epoch: 283 Train loss: 6.804 Aux train loss: 4.148 Val loss: 3.890 Aux val loss: 2.339 Train MAE: 6.741 Val MAE: 14.289 Epoch time: 198.554 seconds \n",
      "Epoch: 284 Train loss: 6.524 Aux train loss: 3.986 Val loss: 3.876 Aux val loss: 2.323 Train MAE: 6.802 Val MAE: 14.377 Epoch time: 198.446 seconds \n",
      "Epoch: 285 Train loss: 6.473 Aux train loss: 3.960 Val loss: 3.889 Aux val loss: 2.357 Train MAE: 6.968 Val MAE: 14.973 Epoch time: 199.210 seconds \n",
      "Epoch: 286 Train loss: 6.553 Aux train loss: 4.003 Val loss: 3.950 Aux val loss: 2.368 Train MAE: 6.639 Val MAE: 17.404 Epoch time: 198.865 seconds \n",
      "Epoch: 287 Train loss: 5.789 Aux train loss: 3.545 Val loss: 3.882 Aux val loss: 2.334 Train MAE: 6.298 Val MAE: 14.620 Epoch time: 198.975 seconds \n",
      "Epoch: 288 Train loss: 6.387 Aux train loss: 3.903 Val loss: 3.935 Aux val loss: 2.354 Train MAE: 6.492 Val MAE: 18.064 Epoch time: 198.092 seconds \n",
      "Epoch: 289 Train loss: 6.156 Aux train loss: 3.766 Val loss: 3.902 Aux val loss: 2.335 Train MAE: 6.467 Val MAE: 15.331 Epoch time: 198.419 seconds \n",
      "Epoch: 290 Train loss: 6.576 Aux train loss: 4.025 Val loss: 3.904 Aux val loss: 2.358 Train MAE: 6.757 Val MAE: 15.726 Epoch time: 199.404 seconds \n",
      "Epoch: 291 Train loss: 5.844 Aux train loss: 3.576 Val loss: 3.945 Aux val loss: 2.386 Train MAE: 6.819 Val MAE: 18.468 Epoch time: 198.793 seconds \n",
      "Epoch: 292 Train loss: 6.257 Aux train loss: 3.832 Val loss: 3.907 Aux val loss: 2.350 Train MAE: 6.322 Val MAE: 17.450 Epoch time: 198.022 seconds \n",
      "Epoch: 293 Train loss: 6.585 Aux train loss: 4.032 Val loss: 3.895 Aux val loss: 2.339 Train MAE: 6.513 Val MAE: 14.663 Epoch time: 198.671 seconds \n",
      "Epoch: 294 Train loss: 6.188 Aux train loss: 3.789 Val loss: 3.874 Aux val loss: 2.324 Train MAE: 6.345 Val MAE: 15.698 Epoch time: 199.066 seconds \n",
      "Epoch: 295 Train loss: 6.104 Aux train loss: 3.736 Val loss: 3.915 Aux val loss: 2.350 Train MAE: 6.464 Val MAE: 15.593 Epoch time: 198.573 seconds \n",
      "Epoch: 296 Train loss: 6.362 Aux train loss: 3.890 Val loss: 3.877 Aux val loss: 2.324 Train MAE: 6.597 Val MAE: 15.705 Epoch time: 199.364 seconds \n",
      "Epoch: 297 Train loss: 5.884 Aux train loss: 3.603 Val loss: 3.904 Aux val loss: 2.348 Train MAE: 6.475 Val MAE: 16.958 Epoch time: 198.589 seconds \n",
      "Epoch: 298 Train loss: 6.836 Aux train loss: 4.181 Val loss: 3.883 Aux val loss: 2.336 Train MAE: 6.195 Val MAE: 15.575 Epoch time: 199.085 seconds \n",
      "Epoch: 299 Train loss: 6.373 Aux train loss: 3.888 Val loss: 3.899 Aux val loss: 2.340 Train MAE: 6.273 Val MAE: 15.927 Epoch time: 198.906 seconds \n",
      "Epoch: 300 Train loss: 6.372 Aux train loss: 3.912 Val loss: 3.872 Aux val loss: 2.315 Train MAE: 6.522 Val MAE: 15.002 Epoch time: 197.910 seconds \n",
      "Epoch: 301 Train loss: 6.754 Aux train loss: 4.131 Val loss: 3.837 Aux val loss: 2.313 Train MAE: 6.413 Val MAE: 14.262 Epoch time: 198.361 seconds \n",
      "Epoch: 302 Train loss: 6.328 Aux train loss: 3.876 Val loss: 3.881 Aux val loss: 2.337 Train MAE: 6.454 Val MAE: 17.899 Epoch time: 198.839 seconds \n",
      "Epoch: 303 Train loss: 6.372 Aux train loss: 3.895 Val loss: 3.888 Aux val loss: 2.321 Train MAE: 6.195 Val MAE: 14.050 Epoch time: 199.018 seconds \n",
      "Epoch: 304 Train loss: 5.780 Aux train loss: 3.538 Val loss: 3.901 Aux val loss: 2.334 Train MAE: 6.215 Val MAE: 16.381 Epoch time: 198.502 seconds \n",
      "Epoch: 305 Train loss: 6.874 Aux train loss: 4.192 Val loss: 3.946 Aux val loss: 2.369 Train MAE: 6.625 Val MAE: 16.878 Epoch time: 198.794 seconds \n",
      "Epoch: 306 Train loss: 6.123 Aux train loss: 3.748 Val loss: 3.897 Aux val loss: 2.339 Train MAE: 6.817 Val MAE: 17.023 Epoch time: 198.639 seconds \n",
      "Epoch: 307 Train loss: 6.320 Aux train loss: 3.862 Val loss: 3.909 Aux val loss: 2.351 Train MAE: 6.398 Val MAE: 18.246 Epoch time: 198.836 seconds \n",
      "Epoch: 308 Train loss: 6.531 Aux train loss: 3.989 Val loss: 3.894 Aux val loss: 2.341 Train MAE: 6.745 Val MAE: 17.479 Epoch time: 198.776 seconds \n",
      "Epoch: 309 Train loss: 6.031 Aux train loss: 3.693 Val loss: 3.894 Aux val loss: 2.341 Train MAE: 6.390 Val MAE: 16.023 Epoch time: 198.112 seconds \n",
      "Epoch: 310 Train loss: 6.099 Aux train loss: 3.732 Val loss: 3.908 Aux val loss: 2.345 Train MAE: 6.189 Val MAE: 16.099 Epoch time: 198.446 seconds \n",
      "Epoch: 311 Train loss: 5.804 Aux train loss: 3.556 Val loss: 3.900 Aux val loss: 2.340 Train MAE: 6.342 Val MAE: 16.708 Epoch time: 198.842 seconds \n",
      "Epoch: 312 Train loss: 6.317 Aux train loss: 3.863 Val loss: 3.875 Aux val loss: 2.339 Train MAE: 6.208 Val MAE: 13.972 Epoch time: 198.940 seconds \n",
      "Epoch: 313 Train loss: 6.398 Aux train loss: 3.902 Val loss: 3.916 Aux val loss: 2.342 Train MAE: 6.639 Val MAE: 13.772 Epoch time: 198.854 seconds \n",
      "Epoch: 314 Train loss: 6.460 Aux train loss: 3.954 Val loss: 3.921 Aux val loss: 2.333 Train MAE: 6.669 Val MAE: 16.850 Epoch time: 198.307 seconds \n",
      "Epoch: 315 Train loss: 6.328 Aux train loss: 3.880 Val loss: 3.937 Aux val loss: 2.355 Train MAE: 6.413 Val MAE: 15.784 Epoch time: 198.657 seconds \n",
      "Epoch: 316 Train loss: 6.420 Aux train loss: 3.926 Val loss: 3.910 Aux val loss: 2.335 Train MAE: 6.602 Val MAE: 16.494 Epoch time: 198.440 seconds \n",
      "Epoch: 317 Train loss: 6.068 Aux train loss: 3.718 Val loss: 3.918 Aux val loss: 2.377 Train MAE: 6.428 Val MAE: 17.904 Epoch time: 198.912 seconds \n",
      "Epoch: 318 Train loss: 6.428 Aux train loss: 3.924 Val loss: 3.905 Aux val loss: 2.352 Train MAE: 6.460 Val MAE: 16.347 Epoch time: 198.404 seconds \n",
      "Epoch: 319 Train loss: 6.224 Aux train loss: 3.805 Val loss: 3.855 Aux val loss: 2.334 Train MAE: 6.295 Val MAE: 15.292 Epoch time: 198.443 seconds \n",
      "Epoch: 320 Train loss: 6.095 Aux train loss: 3.736 Val loss: 3.917 Aux val loss: 2.358 Train MAE: 6.196 Val MAE: 15.773 Epoch time: 198.969 seconds \n",
      "Epoch: 321 Train loss: 6.895 Aux train loss: 4.218 Val loss: 3.877 Aux val loss: 2.335 Train MAE: 6.541 Val MAE: 14.408 Epoch time: 198.844 seconds \n",
      "Epoch: 322 Train loss: 6.219 Aux train loss: 3.799 Val loss: 3.886 Aux val loss: 2.322 Train MAE: 6.113 Val MAE: 15.752 Epoch time: 198.741 seconds \n",
      "Epoch: 323 Train loss: 5.839 Aux train loss: 3.573 Val loss: 3.930 Aux val loss: 2.357 Train MAE: 6.188 Val MAE: 16.806 Epoch time: 198.730 seconds \n",
      "Epoch: 324 Train loss: 6.461 Aux train loss: 3.943 Val loss: 3.949 Aux val loss: 2.344 Train MAE: 6.214 Val MAE: 17.233 Epoch time: 198.535 seconds \n",
      "Epoch: 325 Train loss: 6.824 Aux train loss: 4.157 Val loss: 3.898 Aux val loss: 2.349 Train MAE: 6.833 Val MAE: 15.728 Epoch time: 198.462 seconds \n",
      "Epoch: 326 Train loss: 6.576 Aux train loss: 4.018 Val loss: 3.913 Aux val loss: 2.343 Train MAE: 6.270 Val MAE: 15.284 Epoch time: 198.142 seconds \n",
      "Epoch: 327 Train loss: 6.185 Aux train loss: 3.791 Val loss: 3.852 Aux val loss: 2.311 Train MAE: 5.716 Val MAE: 15.607 Epoch time: 198.306 seconds \n",
      "Epoch: 328 Train loss: 6.381 Aux train loss: 3.901 Val loss: 3.898 Aux val loss: 2.332 Train MAE: 6.449 Val MAE: 15.378 Epoch time: 198.902 seconds \n",
      "Epoch: 329 Train loss: 6.413 Aux train loss: 3.921 Val loss: 3.902 Aux val loss: 2.322 Train MAE: 6.498 Val MAE: 15.786 Epoch time: 199.348 seconds \n",
      "Epoch: 330 Train loss: 6.504 Aux train loss: 3.984 Val loss: 3.866 Aux val loss: 2.330 Train MAE: 6.406 Val MAE: 14.406 Epoch time: 198.457 seconds \n",
      "Epoch: 331 Train loss: 6.132 Aux train loss: 3.750 Val loss: 3.913 Aux val loss: 2.334 Train MAE: 6.415 Val MAE: 17.248 Epoch time: 198.074 seconds \n",
      "Epoch: 332 Train loss: 6.706 Aux train loss: 4.104 Val loss: 3.879 Aux val loss: 2.325 Train MAE: 6.928 Val MAE: 17.509 Epoch time: 198.072 seconds \n",
      "Epoch: 333 Train loss: 6.486 Aux train loss: 3.973 Val loss: 3.879 Aux val loss: 2.332 Train MAE: 6.245 Val MAE: 14.667 Epoch time: 197.781 seconds \n",
      "Epoch: 334 Train loss: 6.307 Aux train loss: 3.853 Val loss: 3.920 Aux val loss: 2.349 Train MAE: 6.172 Val MAE: 16.553 Epoch time: 198.449 seconds \n",
      "Epoch: 335 Train loss: 5.986 Aux train loss: 3.673 Val loss: 3.919 Aux val loss: 2.348 Train MAE: 6.543 Val MAE: 16.586 Epoch time: 198.017 seconds \n",
      "Epoch: 336 Train loss: 5.834 Aux train loss: 3.571 Val loss: 3.881 Aux val loss: 2.325 Train MAE: 6.265 Val MAE: 19.285 Epoch time: 198.676 seconds \n",
      "Epoch: 337 Train loss: 6.398 Aux train loss: 3.910 Val loss: 3.885 Aux val loss: 2.333 Train MAE: 7.178 Val MAE: 14.412 Epoch time: 198.069 seconds \n",
      "Epoch: 338 Train loss: 6.666 Aux train loss: 4.076 Val loss: 3.906 Aux val loss: 2.345 Train MAE: 6.346 Val MAE: 16.397 Epoch time: 198.229 seconds \n",
      "Epoch: 339 Train loss: 6.237 Aux train loss: 3.816 Val loss: 3.920 Aux val loss: 2.348 Train MAE: 6.426 Val MAE: 16.082 Epoch time: 197.698 seconds \n",
      "Epoch: 340 Train loss: 6.992 Aux train loss: 4.275 Val loss: 3.877 Aux val loss: 2.323 Train MAE: 6.651 Val MAE: 14.687 Epoch time: 198.269 seconds \n",
      "Epoch: 341 Train loss: 6.248 Aux train loss: 3.814 Val loss: 3.925 Aux val loss: 2.341 Train MAE: 6.200 Val MAE: 18.217 Epoch time: 198.956 seconds \n",
      "Epoch: 342 Train loss: 6.211 Aux train loss: 3.788 Val loss: 3.908 Aux val loss: 2.333 Train MAE: 6.401 Val MAE: 15.715 Epoch time: 198.235 seconds \n",
      "Epoch: 343 Train loss: 6.106 Aux train loss: 3.745 Val loss: 3.910 Aux val loss: 2.340 Train MAE: 6.436 Val MAE: 18.231 Epoch time: 197.842 seconds \n",
      "Epoch: 344 Train loss: 5.916 Aux train loss: 3.622 Val loss: 3.926 Aux val loss: 2.346 Train MAE: 5.891 Val MAE: 18.125 Epoch time: 197.866 seconds \n",
      "Epoch: 345 Train loss: 6.105 Aux train loss: 3.736 Val loss: 3.869 Aux val loss: 2.323 Train MAE: 6.704 Val MAE: 17.689 Epoch time: 197.732 seconds \n",
      "Epoch: 346 Train loss: 5.943 Aux train loss: 3.637 Val loss: 3.945 Aux val loss: 2.365 Train MAE: 6.469 Val MAE: 18.532 Epoch time: 198.092 seconds \n",
      "Epoch: 347 Train loss: 6.140 Aux train loss: 3.758 Val loss: 3.894 Aux val loss: 2.341 Train MAE: 6.391 Val MAE: 14.976 Epoch time: 197.703 seconds \n",
      "Epoch: 348 Train loss: 6.516 Aux train loss: 3.981 Val loss: 3.919 Aux val loss: 2.352 Train MAE: 6.353 Val MAE: 15.857 Epoch time: 198.077 seconds \n",
      "Epoch: 349 Train loss: 5.692 Aux train loss: 3.485 Val loss: 3.891 Aux val loss: 2.339 Train MAE: 5.985 Val MAE: 15.266 Epoch time: 197.669 seconds \n",
      "Epoch: 350 Train loss: 6.348 Aux train loss: 3.866 Val loss: 3.911 Aux val loss: 2.341 Train MAE: 6.554 Val MAE: 16.469 Epoch time: 197.707 seconds \n",
      "Epoch: 351 Train loss: 5.843 Aux train loss: 3.588 Val loss: 3.893 Aux val loss: 2.336 Train MAE: 6.276 Val MAE: 15.084 Epoch time: 197.953 seconds \n",
      "Epoch: 352 Train loss: 6.471 Aux train loss: 3.957 Val loss: 3.956 Aux val loss: 2.375 Train MAE: 6.204 Val MAE: 17.816 Epoch time: 198.064 seconds \n",
      "Epoch: 353 Train loss: 6.375 Aux train loss: 3.902 Val loss: 3.880 Aux val loss: 2.330 Train MAE: 6.492 Val MAE: 14.287 Epoch time: 197.496 seconds \n",
      "Epoch: 354 Train loss: 6.192 Aux train loss: 3.786 Val loss: 3.883 Aux val loss: 2.341 Train MAE: 6.839 Val MAE: 15.841 Epoch time: 197.661 seconds \n",
      "Epoch: 355 Train loss: 5.906 Aux train loss: 3.618 Val loss: 3.876 Aux val loss: 2.344 Train MAE: 6.315 Val MAE: 18.111 Epoch time: 197.704 seconds \n",
      "Epoch: 356 Train loss: 5.920 Aux train loss: 3.626 Val loss: 3.877 Aux val loss: 2.337 Train MAE: 6.346 Val MAE: 16.221 Epoch time: 198.375 seconds \n",
      "Epoch: 357 Train loss: 6.624 Aux train loss: 4.059 Val loss: 3.857 Aux val loss: 2.316 Train MAE: 6.492 Val MAE: 14.472 Epoch time: 197.487 seconds \n",
      "Epoch: 358 Train loss: 6.001 Aux train loss: 3.669 Val loss: 3.842 Aux val loss: 2.325 Train MAE: 6.210 Val MAE: 14.707 Epoch time: 198.215 seconds \n",
      "Epoch: 359 Train loss: 6.292 Aux train loss: 3.866 Val loss: 3.898 Aux val loss: 2.387 Train MAE: 6.747 Val MAE: 16.047 Epoch time: 197.906 seconds \n",
      "Epoch: 360 Train loss: 6.328 Aux train loss: 3.874 Val loss: 3.901 Aux val loss: 2.350 Train MAE: 6.132 Val MAE: 15.607 Epoch time: 198.617 seconds \n",
      "Epoch: 361 Train loss: 6.080 Aux train loss: 3.718 Val loss: 3.859 Aux val loss: 2.332 Train MAE: 6.007 Val MAE: 14.573 Epoch time: 197.669 seconds \n",
      "Epoch: 362 Train loss: 6.742 Aux train loss: 4.116 Val loss: 3.900 Aux val loss: 2.340 Train MAE: 6.529 Val MAE: 17.058 Epoch time: 197.821 seconds \n",
      "Epoch: 363 Train loss: 5.718 Aux train loss: 3.505 Val loss: 3.869 Aux val loss: 2.324 Train MAE: 6.245 Val MAE: 18.395 Epoch time: 197.690 seconds \n",
      "Epoch: 364 Train loss: 5.875 Aux train loss: 3.594 Val loss: 3.918 Aux val loss: 2.334 Train MAE: 6.692 Val MAE: 15.131 Epoch time: 198.050 seconds \n",
      "Epoch: 365 Train loss: 6.018 Aux train loss: 3.687 Val loss: 3.892 Aux val loss: 2.340 Train MAE: 6.185 Val MAE: 15.503 Epoch time: 197.914 seconds \n",
      "Epoch: 366 Train loss: 5.659 Aux train loss: 3.469 Val loss: 3.870 Aux val loss: 2.333 Train MAE: 6.221 Val MAE: 17.252 Epoch time: 198.585 seconds \n",
      "Epoch: 367 Train loss: 6.239 Aux train loss: 3.826 Val loss: 3.880 Aux val loss: 2.337 Train MAE: 6.497 Val MAE: 14.419 Epoch time: 198.347 seconds \n",
      "Epoch: 368 Train loss: 6.386 Aux train loss: 3.906 Val loss: 3.881 Aux val loss: 2.325 Train MAE: 6.391 Val MAE: 15.311 Epoch time: 197.892 seconds \n",
      "Epoch: 369 Train loss: 5.721 Aux train loss: 3.496 Val loss: 3.851 Aux val loss: 2.311 Train MAE: 6.266 Val MAE: 14.602 Epoch time: 197.909 seconds \n",
      "Epoch: 370 Train loss: 6.026 Aux train loss: 3.692 Val loss: 3.856 Aux val loss: 2.321 Train MAE: 6.633 Val MAE: 17.167 Epoch time: 197.940 seconds \n",
      "Epoch: 371 Train loss: 6.342 Aux train loss: 3.881 Val loss: 3.851 Aux val loss: 2.318 Train MAE: 6.570 Val MAE: 14.928 Epoch time: 197.672 seconds \n",
      "Epoch: 372 Train loss: 6.057 Aux train loss: 3.713 Val loss: 3.859 Aux val loss: 2.327 Train MAE: 6.510 Val MAE: 16.564 Epoch time: 198.136 seconds \n",
      "Epoch: 373 Train loss: 6.252 Aux train loss: 3.825 Val loss: 3.899 Aux val loss: 2.349 Train MAE: 6.075 Val MAE: 17.305 Epoch time: 198.202 seconds \n",
      "Epoch: 374 Train loss: 6.002 Aux train loss: 3.670 Val loss: 3.939 Aux val loss: 2.344 Train MAE: 6.403 Val MAE: 16.266 Epoch time: 197.908 seconds \n",
      "Epoch: 375 Train loss: 6.431 Aux train loss: 3.926 Val loss: 3.895 Aux val loss: 2.339 Train MAE: 6.249 Val MAE: 18.275 Epoch time: 197.850 seconds \n",
      "Epoch: 376 Train loss: 6.074 Aux train loss: 3.716 Val loss: 3.910 Aux val loss: 2.341 Train MAE: 5.732 Val MAE: 16.157 Epoch time: 197.824 seconds \n",
      "Epoch: 377 Train loss: 6.511 Aux train loss: 3.986 Val loss: 3.838 Aux val loss: 2.309 Train MAE: 6.226 Val MAE: 13.940 Epoch time: 198.100 seconds \n",
      "Epoch: 378 Train loss: 6.106 Aux train loss: 3.742 Val loss: 3.846 Aux val loss: 2.315 Train MAE: 5.934 Val MAE: 15.595 Epoch time: 197.925 seconds \n",
      "Epoch: 379 Train loss: 6.385 Aux train loss: 3.906 Val loss: 3.857 Aux val loss: 2.317 Train MAE: 6.393 Val MAE: 14.699 Epoch time: 198.283 seconds \n",
      "Epoch: 380 Train loss: 5.933 Aux train loss: 3.640 Val loss: 3.887 Aux val loss: 2.318 Train MAE: 6.071 Val MAE: 15.092 Epoch time: 198.273 seconds \n",
      "Epoch: 381 Train loss: 5.903 Aux train loss: 3.611 Val loss: 3.892 Aux val loss: 2.335 Train MAE: 6.322 Val MAE: 17.305 Epoch time: 197.932 seconds \n",
      "Epoch: 382 Train loss: 6.282 Aux train loss: 3.848 Val loss: 3.883 Aux val loss: 2.331 Train MAE: 6.725 Val MAE: 15.753 Epoch time: 197.698 seconds \n",
      "Epoch: 383 Train loss: 6.610 Aux train loss: 4.043 Val loss: 3.882 Aux val loss: 2.327 Train MAE: 6.214 Val MAE: 15.253 Epoch time: 198.293 seconds \n",
      "Epoch: 384 Train loss: 6.486 Aux train loss: 3.971 Val loss: 3.885 Aux val loss: 2.326 Train MAE: 6.340 Val MAE: 16.496 Epoch time: 198.866 seconds \n",
      "Epoch: 385 Train loss: 6.142 Aux train loss: 3.765 Val loss: 3.851 Aux val loss: 2.305 Train MAE: 6.250 Val MAE: 14.374 Epoch time: 198.269 seconds \n",
      "Epoch: 386 Train loss: 6.354 Aux train loss: 3.893 Val loss: 3.855 Aux val loss: 2.303 Train MAE: 6.220 Val MAE: 15.751 Epoch time: 198.595 seconds \n",
      "Epoch: 387 Train loss: 5.764 Aux train loss: 3.523 Val loss: 3.864 Aux val loss: 2.319 Train MAE: 6.095 Val MAE: 15.438 Epoch time: 199.114 seconds \n",
      "Epoch: 388 Train loss: 6.288 Aux train loss: 3.850 Val loss: 3.892 Aux val loss: 2.325 Train MAE: 6.494 Val MAE: 17.690 Epoch time: 199.296 seconds \n",
      "Epoch: 389 Train loss: 6.533 Aux train loss: 3.992 Val loss: 3.878 Aux val loss: 2.318 Train MAE: 6.291 Val MAE: 14.691 Epoch time: 199.515 seconds \n",
      "Epoch: 390 Train loss: 6.258 Aux train loss: 3.823 Val loss: 3.901 Aux val loss: 2.344 Train MAE: 6.309 Val MAE: 16.235 Epoch time: 199.122 seconds \n",
      "Epoch: 391 Train loss: 5.795 Aux train loss: 3.556 Val loss: 3.884 Aux val loss: 2.326 Train MAE: 6.064 Val MAE: 16.842 Epoch time: 199.972 seconds \n",
      "Epoch: 392 Train loss: 6.338 Aux train loss: 3.882 Val loss: 3.909 Aux val loss: 2.348 Train MAE: 6.328 Val MAE: 16.419 Epoch time: 198.232 seconds \n",
      "Epoch: 393 Train loss: 5.478 Aux train loss: 3.366 Val loss: 3.903 Aux val loss: 2.331 Train MAE: 6.122 Val MAE: 15.189 Epoch time: 198.566 seconds \n",
      "Epoch: 394 Train loss: 5.964 Aux train loss: 3.661 Val loss: 3.892 Aux val loss: 2.322 Train MAE: 6.119 Val MAE: 14.989 Epoch time: 198.702 seconds \n",
      "Epoch: 395 Train loss: 6.046 Aux train loss: 3.708 Val loss: 3.889 Aux val loss: 2.324 Train MAE: 5.882 Val MAE: 16.686 Epoch time: 197.952 seconds \n",
      "Epoch: 396 Train loss: 6.543 Aux train loss: 4.002 Val loss: 3.882 Aux val loss: 2.346 Train MAE: 5.940 Val MAE: 18.312 Epoch time: 198.314 seconds \n",
      "Epoch: 397 Train loss: 6.225 Aux train loss: 3.808 Val loss: 3.839 Aux val loss: 2.314 Train MAE: 6.567 Val MAE: 15.284 Epoch time: 198.154 seconds \n",
      "Epoch: 398 Train loss: 6.124 Aux train loss: 3.745 Val loss: 3.856 Aux val loss: 2.317 Train MAE: 6.538 Val MAE: 14.941 Epoch time: 198.570 seconds \n",
      "Epoch: 399 Train loss: 6.008 Aux train loss: 3.679 Val loss: 3.890 Aux val loss: 2.337 Train MAE: 6.029 Val MAE: 17.335 Epoch time: 197.861 seconds \n",
      "Epoch: 400 Train loss: 6.024 Aux train loss: 3.693 Val loss: 3.864 Aux val loss: 2.323 Train MAE: 6.283 Val MAE: 14.480 Epoch time: 197.716 seconds \n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc_per_node=2 module11/train.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient17_freeze_gd --epochs=400 --pre_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193df64b-94c7-415f-ae8a-3f2ccefcac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-mapped weights from ./pretrained_models/timm_swin_base_with_gdino_weights.pth\n",
      "Val set MAE: 13.40 RMSE: 38.84\n",
      "GT count: tensor([30.,  8.], device='cuda:0')\n",
      "Predicted count: tensor([30.8235, 11.1631], device='cuda:0')\n",
      "Test set MAE: 15.03 RMSE: 93.09\n",
      "GT count: tensor([17., 34.], device='cuda:0')\n",
      "Predicted count: tensor([25.6420, 40.8864], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=6 torchrun --nproc_per_node=1 module11/evaluate.py --lr=1e-4 --backbone_lr=0 \\\n",
    "--num_enc_layers=1 \\\n",
    "--model_name=efficient17_freeze_gd --epochs=400 --pre_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
